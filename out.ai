===== DESIGN_DOC.md =====
Emera v2 – Design Document
Living Fractals Travelling Through the Book-World
Version Date: February 21, 2026 (revised emphasis on emergence)
Core Metaphor
The text stream is a perfect, pre-existing multidimensional landscape (the book-world) already populated with atomic base-token fractals. The system is a traveller moving sequentially through this terrain. Survival and growth depend on minimizing energy spent per unit distance travelled — achieved by evolving super-tokens that allow increasingly accurate, low-cost anticipation of future landscape.
First Principle – Strict Emergence
No super-token is born an "orchestrator", "specialist", "generalist" or any other role.
All super-tokens start from the same minimal genome template.
Differentiation into coherent behavioral strategies (reliable frequent contributors vs rare high-impact interveners vs other possible personas) arises solely from differential survival and reproduction under the energy gradients described below.
1. Core Entities (all emergent)
Entity
Description
Origin
Evolutionary pressure gradient
Base Tokens
Static atomic fractals, one per vocabulary item, always correct.
Given by the environment (fixed)
Expensive to discover directly → strong pressure to anticipate them.
Super-Tokens
Composite fractals grown via minting / symbiogenesis.
Minted from resonant coalitions
Fitness = net energy saved per distance travelled.
Gap Field
The living multidimensional space / shared terrain itself. Circular buffer of emitted attractor points + metadata (position, energy, velocity, phase, omega, emitter ID).
The medium in which everything lives
Collective short-term memory; patterns propagate via geometric resonance.
2. Global Dynamics – Energy-Efficient Traversal
Frontier / Cursor
Single read position in the stream. Advances one token (or small window) per global timestep.
Energy Ledger (global traveller + per-super-token)
Base travel toll — fixed tiny cost per position advanced (time passes).
Discovery cost — high per base-token when directly revealed (surprise / mismatch).
Attempt cost — moderate-to-high whenever a super-token activates and proposes a prediction segment.
Silence credit — small positive rebate per timestep of inactivity. Grows slowly at first, then faster (log or exponential curve). Capped very low for strategies that activate frequently; effectively uncapped for strategies that activate very rarely.
Jackpot reward — large one-time bonus on correct prediction. Scales with:
rarity of predicted segment (inverse log-frequency or online IDF)
length / complexity of correct match
match quality
accumulated silent timesteps (very strong multiplier for long-dormant strategies)
Prediction vs Discovery
Super-token with high confidence proposes next segment → strong match → glide forward cheaply (cursor advances, energy rebate).
Mismatch → pay discovery cost × unmatched length → force-activate real base tokens → incorporate them (refine self or mint new super-token).
3. Parallel Multi-Round Chaos Game (Signalling & Turn-Taking Medium)
Every global timestep = K short synchronized rounds (K ≈ 4–12, tunable or energy-modulated).
Per round (all currently active super-tokens in parallel):
Read current gap state (includes emissions from previous round of this timestep).
Compute resonance vector (geometric + phase + velocity gating).
Perform one chaos game iteration on private IFS → new attractor point.
Emit new point immediately → gap updates for next round.
No explicit roles or classes — behavioural differentiation emerges because:
Super-tokens that emit frequently, with low amplitude, repetitive geometric motifs → tend to accumulate small steady rebates + enable other strategies → survive reliably (orchestrator-like attractor).
Super-tokens that emit very rarely, with high amplitude, highly specific geometry → accumulate large silence credit + occasional huge jackpots → survive via rare big wins (specialist-like attractor).
Super-tokens that emit too often and inaccurately → pay constant attempt costs + mismatch penalties → die out.
The parallel rounds themselves create the signalling substrate:
Early rounds dominated by frequent emitters → faint, repetitive priming waves ("structural expectation slots").
Later rounds allow rare emitters to respond only when resonance is extremely high → precise interventions.
4. Super-Token Genome (uniform starting point, evolvable traits)
All super-tokens begin from the same minimal template. Differentiation arises through mutation, selection, and symbiogenesis.
Evolvable Trait
Description
Selection gradient effect
IFS parameters
Affine maps defining private chaos game trajectory
Shapes that resonate usefully with upcoming terrain survive.
Activation threshold
Confidence needed to emit a proposal
Low → frequent small contributions; very high → rare big wins.
Emission amplitude / decay
Strength and persistence of emitted points
Low amp + long decay → steady scaffolding; high amp + short decay → rare sharp interventions.
Silence credit growth rate
How fast rebate accumulates during inactivity
Slow growth → frequent activation; fast/exponential → rare activation rewarded.
Resonance filter shape
Geometric / phase preference for incoming gap patterns
Broad → responds to many contexts; narrow → responds only to specific cues.
5. Minting / Symbiogenesis (emergent coalition binding)
Triggered when a coalition of super-tokens consistently enables cheap traversal together (high joint energy savings).
Child inherits blended traits (IFS average + interpolated emission / silence / filter parameters).
Successful fusions inherit energy windfall → larger predictive reach.
No pre-defined "types" — fusions between frequent + rare strategists naturally produce hybrid or higher-order personas.
6. Key Selection Pressures (no hand-crafted categories)
Pressure
Favors emergence of …
Prevents …
High attempt cost + mismatch penalty
Very high activation thresholds → rare activation
Constant over-prediction / delusional strategies
Exponential silence credit growth
Long-dormant strategies that wait for perfect moments
Hyperactive low-reward emitters
Rarity-scaled jackpot
Narrow specialists on Zipf tail
Generalists trying to cover everything
Low-cost frequent emission rebate
Steady scaffolding of structural patterns
Complete silence (no one coordinates)
Discovery cost when surprised
Accurate anticipation of future terrain
Ignoring upcoming base tokens
These gradients are sufficient to make Zipf-like division of labor (frequent reliable backbone vs rare high-impact interveners) an evolutionarily stable outcome — without ever naming the categories in advance.
7. Expected Emergent Personas (not pre-defined)
Steady-state frequent predictors (grammar / function-word scaffolds)
Long-dormant rare-pattern specialists (low-frequency lexical items)
Hybrid coordinator–specialist fusions
Possibly: short-burst refiners, error-correctors, meta-predictors of prediction confidence, etc.
8. Next Steps for Prototyping
Minimal toy corpus (100–500 token vocab, short sentences with clear Zipf distribution).
Uniform starting genome for all minted super-tokens.
Implement energy ledger + parallel K-round chaos game loop.
Add silence credit curve + rarity-scaled jackpot.
Run long enough to observe whether frequency-based behavioral clusters appear without any explicit type labels.
This version stays true to "let mother nature do the work" while still providing strong, coherent selection gradients that make the desired division of labor (and potentially many others) highly probable.


of course, if a super token runs out of energy, they die, perhaps energy sharing is a nice addition later on.
-e 

===== concat.sh =====
#!/bin/bash

OUTPUT="out.ai"
> "$OUTPUT"

FILES=$(git ls-files | grep -E '\.(py|md|go|sh|toml|yaml|yml|Makefile)$' | grep -v '^out/' | grep -v '^data/' | grep -v '__pycache__')

for f in $FILES; do
  echo "===== $f =====" >> "$OUTPUT"
  cat "$f" >> "$OUTPUT"
  echo -e "\n" >> "$OUTPUT"
done

echo "Written to $OUTPUT"
wc -c "$OUTPUT"
-e 

===== config.py =====
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Optional


@dataclass(frozen=True)
class EmeraConfig:
    # Reproducibility
    seed: int = 42

    # Token space
    token_space: str = "byte_parity"  # {"byte_parity", "gpt2"}
    gpt2_model_name: str = "gpt2"

    # Vocabulary/world
    base_tokens: int = 512
    world_vocab_size: int = 128
    corpus_file: Optional[str] = None
    world_len: int = 200_000
    zipf_alpha: float = 1.15

    # Latent/gap geometry
    d_latent: int = 32
    gap_dim: int = 16
    gap_len: int = 128
    gap_decay: float = 0.97
    gap_velocity_decay: float = 0.90
    gap_read_backend: str = "auto"  # {"auto", "numpy", "jax"}
    gap_read_batch_size: int = 128
    omega_base: float = 0.035
    omega_jitter: float = 0.010
    beacon_strength: float = 0.10

    # Chaos/proposal
    num_ifs: int = 4
    chaos_substeps_per_round: int = (
        1  # legacy: ignored when energy-regulated; kept for SVG compat
    )
    chaos_min_substeps: int = 1
    chaos_max_substeps: int = 24
    chaos_substep_cost: float = 0.004
    chaos_svg_default_steps: int = 2000
    chaos_svg_max_steps: int = 50000
    k_rounds: int = 6
    proposal_lmax: int = 14
    confidence_scale: float = 10.0
    proposal_drift_scale: float = 0.08
    obligatory_proposals: bool = True
    proposal_min_bet: float = 0.0005
    proposal_bet_unit_scale: float = 0.05
    proposal_bet_max_energy_frac: float = 0.12
    proposal_bet_floor_frac: float = 0.08
    proposal_bet_conf_gain: float = 8.0
    proposal_frontier_contrast: int = 16
    proposal_frontier_fallback: int = 32
    frontier_rescue_energy: float = 0.55
    frontier_rescue_noise: float = 0.04
    frontier_rescue_max_per_step: int = 1
    contrastive_enabled: bool = True
    contrastive_correct_reward: float = 0.06
    contrastive_wrong_penalty: float = 0.45
    contrastive_wrong_exp: float = 2.0

    # Initial super-token population
    initial_super_tokens: int = 100
    initial_super_energy: float = 3.00

    # Genome trait template
    activation_threshold_init: float = 0.45
    emission_amplitude_init: float = 0.40
    emission_decay_init: float = 0.15
    silence_growth_init: float = 1.00
    resonance_width_init: float = 0.45
    phase_coupling_init: float = 0.25
    velocity_coupling_init: float = 0.20
    proposal_length_bias_init: float = 0.00

    # Thermodynamic ledger
    ambient_dissipation: float = 0.003
    metabolic_tax_rate: float = 0.0015
    base_toll: float = 0.005
    attempt_cost_base: float = 0.04
    discovery_cost: float = 0.18
    token_energy_cap: float = 50.0
    min_viable_energy: float = 1e-4
    survivor_grace_steps: int = 20
    survivor_relief_active_frac: float = 0.35
    survivor_relief_reservoir_frac: float = 0.01
    strict_energy_budget: bool = True
    conserve_total_energy: bool = True
    energy_inflow_per_step: float = 0.0
    energy_reservoir_init: float = 120.0
    energy_reservoir_cap: float = 1000.0
    death_recycle_fraction: float = 1.0
    death_recycle_flat: float = 0.0

    # Silence credit
    silence_log_coeff: float = 0.006
    silence_exp_coeff: float = 0.0014
    silence_exp_rate: float = 0.015

    # Jackpot
    jackpot_base: float = 4.00
    jackpot_length_scale: float = 0.35
    jackpot_silence_scale: float = 0.40

    # Symbiogenesis
    spawn_cost: float = 0.45
    mint_interval: int = 6
    mint_delta: float = -0.20
    mint_parent_contrib_frac: float = 0.35
    capsule_frontier_window: int = 48
    capsule_mint_parent_pool: int = 10
    self_copy_enabled: bool = True
    self_copy_interval: int = 3
    self_copy_cost: float = 0.30
    self_copy_parent_contrib_frac: float = 0.35
    self_copy_min_energy: float = 0.80
    self_copy_min_match_frac: float = 0.50
    self_copy_min_score: float = 0.0
    self_copy_max_per_step: int = 2
    ema_alpha: float = 0.02
    mutation_scale: float = 0.08
    ifs_mutation_scale: float = 0.03

    # Pareto mutation dynamics
    pareto_alpha_init: float = 1.7
    pareto_alpha_min: float = 1.1
    pareto_alpha_max: float = 4.5
    pareto_mutation_scale: float = 0.22
    pareto_clip: float = 12.0

    # Reward split for child success
    child_reward_share: float = 0.72
    parent_reward_share: float = 0.10

    # Adaptive natural laws (environment-driven)
    dynamic_laws: bool = True
    law_update_interval: int = 25
    adaptation_ema_decay: float = 0.97
    adaptation_rate: float = 0.05
    adaptation_signal_decay: float = 0.70
    target_active_super: float = 100.0
    target_match_rate: float = 0.08
    target_proposal_pressure: float = 0.70
    target_birth_death_gap: float = 0.0

    # Law bounds
    attempt_cost_min: float = 0.005
    attempt_cost_max: float = 0.40
    jackpot_base_min: float = 0.20
    jackpot_base_max: float = 20.0
    silence_log_min: float = 1e-5
    silence_log_max: float = 0.04
    silence_exp_min: float = 1e-6
    silence_exp_max: float = 0.02
    ambient_dissipation_min: float = 1e-4
    ambient_dissipation_max: float = 0.08
    spawn_cost_min: float = 0.05
    spawn_cost_max: float = 8.0
    mint_delta_min: float = -0.30
    mint_delta_max: float = 0.60

    # Seasonal forcing
    seasons_enabled: bool = True
    season_period: int = 400
    season_strength: float = 0.30
    season_wave_decay: float = 0.65
    season_revival_spores: int = 24
    season_revival_energy: float = 1.1
    season_topology_jitter: float = 0.02

    # Logging
    log_every: int = 50

    def validate(self) -> None:
        if self.token_space not in {"byte_parity", "gpt2"}:
            raise ValueError("token_space must be one of: {'byte_parity', 'gpt2'}.")
        if self.token_space == "byte_parity" and self.base_tokens != 512:
            raise ValueError("byte_parity mode expects base_tokens == 512.")
        if self.base_tokens < 8:
            raise ValueError("base_tokens must be >= 8.")
        if self.world_vocab_size < 8 or self.world_vocab_size > self.base_tokens:
            raise ValueError("world_vocab_size must be in [8, base_tokens].")
        if self.d_latent < 4:
            raise ValueError("d_latent must be >= 4.")
        if self.gap_dim < 4:
            raise ValueError("gap_dim must be >= 4.")
        if self.gap_dim > self.d_latent:
            raise ValueError("gap_dim must be <= d_latent.")
        if self.gap_read_backend not in {"auto", "numpy", "jax"}:
            raise ValueError(
                "gap_read_backend must be one of {'auto', 'numpy', 'jax'}."
            )
        if self.gap_read_batch_size < 1:
            raise ValueError("gap_read_batch_size must be >= 1.")
        if self.num_ifs < 1:
            raise ValueError("num_ifs must be >= 1.")
        if self.chaos_substeps_per_round < 1:
            raise ValueError("chaos_substeps_per_round must be >= 1.")
        if self.chaos_min_substeps < 1:
            raise ValueError("chaos_min_substeps must be >= 1.")
        if self.chaos_max_substeps < self.chaos_min_substeps:
            raise ValueError("chaos_max_substeps must be >= chaos_min_substeps.")
        if self.chaos_substep_cost < 0.0:
            raise ValueError("chaos_substep_cost must be >= 0.")
        if self.chaos_svg_default_steps < 1:
            raise ValueError("chaos_svg_default_steps must be >= 1.")
        if self.chaos_svg_max_steps < self.chaos_svg_default_steps:
            raise ValueError("chaos_svg_max_steps must be >= chaos_svg_default_steps.")
        if self.k_rounds < 1:
            raise ValueError("k_rounds must be >= 1.")
        if self.proposal_lmax < 1:
            raise ValueError("proposal_lmax must be >= 1.")
        if self.proposal_min_bet < 0.0:
            raise ValueError("proposal_min_bet must be >= 0.")
        if self.proposal_bet_unit_scale <= 0.0:
            raise ValueError("proposal_bet_unit_scale must be > 0.")
        if not (0.0 < self.proposal_bet_max_energy_frac <= 1.0):
            raise ValueError("proposal_bet_max_energy_frac must be in (0, 1].")
        if not (0.0 <= self.proposal_bet_floor_frac <= 1.0):
            raise ValueError("proposal_bet_floor_frac must be in [0, 1].")
        if self.proposal_bet_conf_gain <= 0.0:
            raise ValueError("proposal_bet_conf_gain must be > 0.")
        if self.proposal_frontier_contrast < 0:
            raise ValueError("proposal_frontier_contrast must be >= 0.")
        if self.proposal_frontier_fallback < 0:
            raise ValueError("proposal_frontier_fallback must be >= 0.")
        if self.frontier_rescue_energy <= 0.0:
            raise ValueError("frontier_rescue_energy must be > 0.")
        if self.frontier_rescue_noise < 0.0:
            raise ValueError("frontier_rescue_noise must be >= 0.")
        if self.frontier_rescue_max_per_step < 0:
            raise ValueError("frontier_rescue_max_per_step must be >= 0.")
        if self.contrastive_correct_reward < 0.0:
            raise ValueError("contrastive_correct_reward must be >= 0.")
        if self.contrastive_wrong_penalty < 0.0:
            raise ValueError("contrastive_wrong_penalty must be >= 0.")
        if self.contrastive_wrong_exp < 1.0:
            raise ValueError("contrastive_wrong_exp must be >= 1.")
        if self.initial_super_tokens < 1:
            raise ValueError("initial_super_tokens must be >= 1.")
        if self.spawn_cost <= 0:
            raise ValueError("spawn_cost must be > 0.")
        if not (0.0 <= self.mint_parent_contrib_frac <= 1.0):
            raise ValueError("mint_parent_contrib_frac must be in [0, 1].")
        if self.capsule_frontier_window < 1:
            raise ValueError("capsule_frontier_window must be >= 1.")
        if self.capsule_frontier_window > self.gap_len:
            raise ValueError("capsule_frontier_window must be <= gap_len.")
        if self.capsule_mint_parent_pool < 2:
            raise ValueError("capsule_mint_parent_pool must be >= 2.")
        if self.self_copy_interval < 1:
            raise ValueError("self_copy_interval must be >= 1.")
        if self.self_copy_cost <= 0.0:
            raise ValueError("self_copy_cost must be > 0.")
        if not (0.0 <= self.self_copy_parent_contrib_frac <= 1.0):
            raise ValueError("self_copy_parent_contrib_frac must be in [0, 1].")
        if self.self_copy_min_energy < 0.0:
            raise ValueError("self_copy_min_energy must be >= 0.")
        if not (0.0 <= self.self_copy_min_match_frac <= 1.0):
            raise ValueError("self_copy_min_match_frac must be in [0, 1].")
        if self.self_copy_max_per_step < 0:
            raise ValueError("self_copy_max_per_step must be >= 0.")
        if self.token_energy_cap <= 0:
            raise ValueError("token_energy_cap must be > 0.")
        if self.min_viable_energy < 0.0:
            raise ValueError("min_viable_energy must be >= 0.")
        if self.survivor_grace_steps < 0:
            raise ValueError("survivor_grace_steps must be >= 0.")
        if not (0.0 <= self.survivor_relief_active_frac <= 1.0):
            raise ValueError("survivor_relief_active_frac must be in [0, 1].")
        if not (0.0 <= self.survivor_relief_reservoir_frac <= 1.0):
            raise ValueError("survivor_relief_reservoir_frac must be in [0, 1].")
        if self.metabolic_tax_rate < 0.0:
            raise ValueError("metabolic_tax_rate must be >= 0.")
        if self.energy_inflow_per_step < 0.0:
            raise ValueError("energy_inflow_per_step must be >= 0.")
        if self.energy_reservoir_init < 0.0:
            raise ValueError("energy_reservoir_init must be >= 0.")
        if self.energy_reservoir_cap <= 0.0:
            raise ValueError("energy_reservoir_cap must be > 0.")
        if self.conserve_total_energy and self.energy_inflow_per_step != 0.0:
            raise ValueError(
                "conserve_total_energy requires energy_inflow_per_step == 0."
            )
        if not (0.0 <= self.death_recycle_fraction <= 1.0):
            raise ValueError("death_recycle_fraction must be in [0, 1].")
        if self.death_recycle_flat < 0.0:
            raise ValueError("death_recycle_flat must be >= 0.")
        if self.mint_interval < 1:
            raise ValueError("mint_interval must be >= 1.")
        if self.law_update_interval < 1:
            raise ValueError("law_update_interval must be >= 1.")
        if self.season_period < 2:
            raise ValueError("season_period must be >= 2.")
        if self.season_strength < 0.0:
            raise ValueError("season_strength must be >= 0.")
        if not (0.0 <= self.season_wave_decay < 1.0):
            raise ValueError("season_wave_decay must be in [0, 1).")
        if self.season_revival_spores < 0:
            raise ValueError("season_revival_spores must be >= 0.")
        if self.season_revival_energy <= 0.0:
            raise ValueError("season_revival_energy must be > 0.")
        if self.season_topology_jitter < 0.0:
            raise ValueError("season_topology_jitter must be >= 0.")
        if not (0.0 <= self.adaptation_ema_decay < 1.0):
            raise ValueError("adaptation_ema_decay must be in [0, 1).")
        if self.adaptation_rate < 0.0:
            raise ValueError("adaptation_rate must be >= 0.")
        if not (0.0 <= self.adaptation_signal_decay < 1.0):
            raise ValueError("adaptation_signal_decay must be in [0, 1).")
        if self.pareto_alpha_init < 1.01:
            raise ValueError("pareto_alpha_init must be >= 1.01.")
        if (
            self.pareto_alpha_min < 1.01
            or self.pareto_alpha_max < self.pareto_alpha_min
        ):
            raise ValueError("invalid pareto alpha bounds.")
        if not (0.0 <= self.child_reward_share <= 1.0):
            raise ValueError("child_reward_share must be in [0, 1].")
        if not (0.0 <= self.parent_reward_share <= 1.0):
            raise ValueError("parent_reward_share must be in [0, 1].")
        total = self.child_reward_share + 2.0 * self.parent_reward_share
        if total > 1.0 + 1e-6:
            raise ValueError("child_reward_share + 2*parent_reward_share must be <= 1.")
        if self.corpus_file:
            path = Path(self.corpus_file)
            if not path.exists():
                raise FileNotFoundError(f"corpus_file does not exist: {path}")
-e 

===== cooperation.py =====
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, Iterable, Tuple


def _pair_key(a: int, b: int) -> tuple[int, int]:
    if a <= b:
        return (a, b)
    return (b, a)


@dataclass
class CooperationStats:
    alpha: float
    ind_ema: Dict[int, float] = field(default_factory=dict)
    pair_ema: Dict[tuple[int, int], float] = field(default_factory=dict)
    pair_hits: Dict[tuple[int, int], int] = field(default_factory=dict)

    def update(self, contributions: Dict[int, float], realized_return: float) -> None:
        if not contributions:
            return
        a = float(max(min(self.alpha, 1.0), 1e-6))
        for tid, c in contributions.items():
            prev = self.ind_ema.get(tid, 0.0)
            x = float(c) * float(realized_return)
            self.ind_ema[tid] = (1.0 - a) * prev + a * x

        ids = sorted(contributions.keys())
        for i in range(len(ids)):
            for j in range(i + 1, len(ids)):
                ti, tj = ids[i], ids[j]
                key = _pair_key(ti, tj)
                mass = min(float(contributions[ti]), float(contributions[tj]))
                prev = self.pair_ema.get(key, 0.0)
                x = mass * float(realized_return)
                self.pair_ema[key] = (1.0 - a) * prev + a * x
                self.pair_hits[key] = self.pair_hits.get(key, 0) + 1

    def synergy(self, a: int, b: int) -> float:
        key = _pair_key(a, b)
        pair = self.pair_ema.get(key, 0.0)
        ind = 0.5 * (self.ind_ema.get(a, 0.0) + self.ind_ema.get(b, 0.0))
        return float(pair - ind)

    def top_pairs(self, min_hits: int = 1) -> list[tuple[int, int, float, int]]:
        out: list[tuple[int, int, float, int]] = []
        for (a, b), p in self.pair_ema.items():
            hits = self.pair_hits.get((a, b), 0)
            if hits < min_hits:
                continue
            s = p - 0.5 * (self.ind_ema.get(a, 0.0) + self.ind_ema.get(b, 0.0))
            if s > 0.0:
                out.append((a, b, float(s), int(hits)))
        out.sort(key=lambda x: x[2], reverse=True)
        return out

    def prune_dead(self, alive_ids: Iterable[int]) -> None:
        alive = set(int(x) for x in alive_ids)
        self.ind_ema = {k: v for k, v in self.ind_ema.items() if k in alive}
        self.pair_ema = {k: v for k, v in self.pair_ema.items() if (k[0] in alive and k[1] in alive)}
        self.pair_hits = {k: v for k, v in self.pair_hits.items() if (k[0] in alive and k[1] in alive)}
-e 

===== dashboard.py =====
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import html
import json
import re
import threading
import time
from dataclasses import replace
from http.server import BaseHTTPRequestHandler, ThreadingHTTPServer
from pathlib import Path
from typing import Any
from urllib.parse import parse_qs, urlparse

import numpy as np

from config import EmeraConfig
from engine import EmeraEngine, StepResult


def _decode_identity(
    engine: EmeraEngine, identity_values: np.ndarray, max_chars: int = 48
) -> str:
    vals = np.asarray(identity_values, dtype=np.int32).tolist()
    text = engine.decode_tokens_text(vals, max_chars=max_chars)
    if len(text) > max_chars:
        text = text[:max_chars] + "..."
    return text


def _parse_bool(v: str | None, default: bool = False) -> bool:
    if v is None:
        return default
    return v.lower() in {"1", "true", "yes", "on"}


def _parse_float(v: str | None, default: float) -> float:
    if v is None:
        return default
    try:
        return float(v)
    except ValueError:
        return default


def _parse_int(
    v: str | None, default: int, lo: int | None = None, hi: int | None = None
) -> int:
    if v is None:
        out = default
    else:
        try:
            out = int(v)
        except ValueError:
            out = default
    if lo is not None:
        out = max(out, lo)
    if hi is not None:
        out = min(out, hi)
    return out


class Trainer:
    def __init__(self, cfg: EmeraConfig, steps_per_tick: int, sleep_s: float):
        self.engine = EmeraEngine(cfg)
        self.world_bytes: np.ndarray | None = None
        if self.engine.cfg.token_space == "byte_parity":
            self.world_bytes = (self.engine.world.tokens & 0xFF).astype(np.uint8)
        self.lock = threading.RLock()
        self.steps_per_tick = max(int(steps_per_tick), 1)
        self.sleep_s = max(float(sleep_s), 0.0)
        self.running = True
        self.stop_event = threading.Event()
        self.started_at = time.time()
        self.last_result: StepResult | None = None
        self.last_inference: dict[str, Any] | None = None
        self.total_steps = 0
        self.error = ""
        self.thread = threading.Thread(
            target=self._loop, daemon=True, name="emera-trainer"
        )

    def start(self) -> None:
        self.thread.start()

    def stop(self) -> None:
        self.stop_event.set()
        self.thread.join(timeout=2.0)

    def toggle(self) -> bool:
        with self.lock:
            self.running = not self.running
            return self.running

    def _loop(self) -> None:
        while not self.stop_event.is_set():
            if not self.running:
                time.sleep(0.05)
                continue
            try:
                for _ in range(self.steps_per_tick):
                    if self.stop_event.is_set():
                        break
                    with self.lock:
                        self.last_result = self.engine.step()
                        self.total_steps += 1
            except Exception as exc:  # pragma: no cover
                with self.lock:
                    self.error = f"{type(exc).__name__}: {exc}"
                    self.running = False
                time.sleep(0.2)
            if self.sleep_s > 0.0:
                time.sleep(self.sleep_s)
            else:
                time.sleep(0.0)

    def _frontier_line(self) -> tuple[str, str, str]:
        if self.world_bytes is None:
            n = int(self.engine.world.length)
            if n == 0:
                return "", "", ""
            c = int(self.engine.world.cursor) % n
            window = 10
            idx = (c + np.arange(-window, window + 1, dtype=np.int64)) % n
            vals = self.engine.world.tokens[idx].astype(np.int32).tolist()
            left = self.engine.decode_tokens_text(vals[:window], max_chars=200)
            cur = self.engine.decode_tokens_text([vals[window]], max_chars=60)
            right = self.engine.decode_tokens_text(vals[window + 1 :], max_chars=200)
            marked = f"{left}[{cur}]{right}"
            frontier_token = int(self.engine.world.peek(1)[0]) if n > 0 else -1
            frontier_tag = (
                f"{frontier_token}:{cur.encode('unicode_escape').decode('ascii')}"
            )
            return "TOKEN_STREAM [*]", marked, frontier_tag

        n = int(self.world_bytes.size)
        if n == 0:
            return "", "", ""
        c = int(self.engine.world.cursor) % n
        start = c
        while start > 0 and int(self.world_bytes[start - 1]) != 10:
            start -= 1
        end = c
        while end < (n - 1) and int(self.world_bytes[end]) != 10:
            end += 1
        b = self.world_bytes
        pre = b[start:c].tobytes().decode("utf-8", errors="replace")
        cur = bytes([int(b[c])]).decode("utf-8", errors="replace")
        post = b[c + 1 : end].tobytes().decode("utf-8", errors="replace")
        marked = f"{pre}[{cur}]{post}"
        clean = (pre + cur + post).strip()
        m = re.match(r"^\s*([1-3]?\s?[A-Za-z]+)\s+(\d+):(\d+)", clean)
        if m:
            book = m.group(1).upper()
            chapter = m.group(2)
            verse = m.group(3)
            location = f"{book} [{chapter}] :{verse}"
        else:
            location = "UNKNOWN [*]"
        frontier_token = int(self.engine.world.peek(1)[0]) if n > 0 else -1
        frontier_byte = frontier_token & 0xFF
        frontier_hex = f"0x{frontier_byte:02x}"
        return location, marked, frontier_hex

    def status_snapshot(self) -> dict[str, Any]:
        with self.lock:
            now = time.time()
            elapsed = max(now - self.started_at, 1e-9)
            steps_per_s = float(self.total_steps) / elapsed
            stats = self.last_result.stats if self.last_result is not None else {}
            snap = self.engine.metrics.snapshot()
            location, marked, frontier_hex = self._frontier_line()
            return {
                "running": bool(self.running),
                "error": self.error,
                "step": int(self.engine.step_idx),
                "active": int(len(self.engine.super_tokens)),
                "proposers": int(stats.get("proposers", 0)),
                "winner": int(stats.get("winner_id", -1)),
                "match": int(stats.get("match_len", 0)),
                "advance": int(stats.get("advance_len", 0)),
                "return": float(stats.get("realized_return", 0.0)),
                "jackpot": float(stats.get("jackpot", 0.0)),
                "reservoir": float(stats.get("reservoir", 0.0)),
                "drift": float(stats.get("energy_drift", 0.0)),
                "steps_per_s": steps_per_s,
                "location": location,
                "frontier_line": marked,
                "frontier_hex": frontier_hex,
                "glide": float(snap.get("glide_ratio", 0.0)),
                "epa": float(snap.get("energy_per_advance", 0.0)),
                "root_frac": float(snap.get("root_only_fraction", 1.0)),
                "caps_nonroot": int(snap.get("nonroot_live_capsules", 0)),
                "caps_half_life": float(snap.get("capsule_half_life", 0.0)),
                "caps_t1k": float(snap.get("lineage_persistence_1k", 0.0)),
                "chaos_avg_sub": float(snap.get("chaos_avg_substeps", 0.0)),
                "chaos_energy": float(snap.get("chaos_energy_spent", 0.0)),
            }

    def supertokens(
        self, sort_by: str, desc: bool, min_energy: float, query: str, limit: int
    ) -> list[dict[str, Any]]:
        with self.lock:
            step_idx = int(self.engine.step_idx)
            q = query.lower().strip()
            rows: list[dict[str, Any]] = []
            for token in self.engine.super_tokens.values():
                energy = float(token.energy)
                if energy < min_energy:
                    continue
                text = _decode_identity(
                    self.engine,
                    token.identity_bytes,
                    max_chars=64,
                )
                if q and q not in text.lower() and q not in str(token.token_id):
                    continue
                age = max(step_idx - int(token.birth_step), 0)
                rs = float(self.engine.last_resonance_strength.get(token.token_id, 0.0))
                _, conf, _ = self.engine._raw_decode_for_token(token, rs)
                rows.append(
                    {
                        "id": int(token.token_id),
                        "age": int(age),
                        "energy": float(energy),
                        "inactive": int(token.inactivity_steps),
                        "len": int(
                            np.asarray(token.identity_bytes, dtype=np.int32).size
                        ),
                        "conf": float(conf),
                        "text": text,
                        "contains_frontier": bool(
                            self.engine._identity_contains_frontier_byte(
                                token.identity_bytes
                            )
                        ),
                        "lineage": "root"
                        if int(token.parent_a) < 0 and int(token.parent_b) < 0
                        else f"{int(token.parent_a)},{int(token.parent_b)}",
                    }
                )

            key_map = {
                "id": lambda r: r["id"],
                "age": lambda r: r["age"],
                "energy": lambda r: r["energy"],
                "inactive": lambda r: r["inactive"],
                "len": lambda r: r["len"],
                "conf": lambda r: r["conf"],
            }
            key_fn = key_map.get(sort_by, key_map["energy"])
            rows.sort(key=key_fn, reverse=desc)
            return rows[: max(limit, 1)]

    def chaos_svg(
        self,
        token_id: int,
        steps: int | None = None,
        width: int = 440,
        height: int = 230,
    ) -> str:
        with self.lock:
            token = self.engine.super_tokens.get(int(token_id))
            if token is None:
                return "<div class='card'><strong>Chaos</strong><p>token not found.</p></div>"
            ifs = np.asarray(token.ifs, dtype=np.float32)
            p = np.asarray(token.state_vec[:2], dtype=np.float32).copy()
            ident = _decode_identity(
                self.engine,
                token.identity_bytes,
                max_chars=64,
            )
            tid = int(token.token_id)
            energy = float(token.energy)
            cfg = self.engine.cfg
            default_steps = int(getattr(cfg, "chaos_svg_default_steps", 2000))
            max_steps = int(getattr(cfg, "chaos_svg_max_steps", max(default_steps, 1)))
            s = default_steps if steps is None else int(steps)
            s = int(np.clip(s, 1, max_steps))

        rng = np.random.default_rng(tid + 1337)
        pts = np.zeros((s, 2), dtype=np.float32)
        for i in range(s):
            k = int(rng.integers(0, ifs.shape[0]))
            a = ifs[k, :, :2]
            b = ifs[k, :, 2]
            p = np.tanh(a @ p + b).astype(np.float32)
            pts[i] = p

        pad = 10.0
        xs = ((pts[:, 0] + 1.0) * 0.5) * (width - 2 * pad) + pad
        ys = (1.0 - (pts[:, 1] + 1.0) * 0.5) * (height - 2 * pad) + pad
        point_str = " ".join(
            f"{x:.2f},{y:.2f}" for x, y in zip(xs.tolist(), ys.tolist())
        )
        return (
            "<div class='card'>"
            f"<strong>Chaos Game | token {tid}</strong>"
            f"<div class='muted'>energy={energy:.3f} steps={s} identity='{html.escape(ident)}'</div>"
            f"<svg width='{width}' height='{height}' viewBox='0 0 {width} {height}' class='plot'>"
            "<rect x='0' y='0' width='100%' height='100%' fill='#0b0f17'/>"
            f"<polyline points='{point_str}' fill='none' stroke='#5dd6ff' stroke-opacity='0.52' stroke-width='1.0'/>"
            "</svg>"
            "</div>"
        )

    def gap_snapshot(self, limit: int = 32, scatter_limit: int = 220) -> dict[str, Any]:
        with self.lock:
            energy = np.asarray(self.engine.gap.energy, dtype=np.float32).copy()
            pts = np.asarray(self.engine.gap.points, dtype=np.float32).copy()
            emit = np.asarray(self.engine.gap.emitter_id, dtype=np.int64).copy()
            step_idx = np.asarray(self.engine.gap.step_idx, dtype=np.int64).copy()
            round_idx = np.asarray(self.engine.gap.round_idx, dtype=np.int32).copy()
            ptr = int(self.engine.gap.ptr)

        active = np.where(energy > 1e-7)[0]
        if active.size == 0:
            return {"rows": [], "scatter": [], "ptr": ptr}
        order = active[np.argsort(energy[active])[::-1]]
        top = order[: max(limit, 1)]
        rows = [
            {
                "slot": int(i),
                "energy": float(energy[i]),
                "emitter": int(emit[i]),
                "step": int(step_idx[i]),
                "round": int(round_idx[i]),
                "x": float(pts[i, 0]),
                "y": float(pts[i, 1]),
            }
            for i in top.tolist()
        ]
        scatter_idx = order[: max(scatter_limit, 1)]
        max_e = float(np.max(energy[scatter_idx])) if scatter_idx.size else 1.0
        scatter = []
        for i in scatter_idx.tolist():
            e = float(energy[i])
            ratio = float(np.clip(e / max(max_e, 1e-9), 0.0, 1.0))
            scatter.append(
                {
                    "x": float(pts[i, 0]),
                    "y": float(pts[i, 1]),
                    "e": e,
                    "r": 1.0 + 3.0 * np.sqrt(ratio),
                    "h": int(200 - 150 * ratio),
                }
            )
        return {"rows": rows, "scatter": scatter, "ptr": ptr}

    def run_inference(
        self,
        prompt: str,
        new_tokens: int,
        passes: int,
        window: int,
        top_k: int,
        temperature: float,
    ) -> dict[str, Any]:
        with self.lock:
            out = self.engine.infer_generate(
                prompt=prompt,
                max_new_tokens=int(new_tokens),
                diffusion_passes=int(passes),
                window_size=int(window),
                top_k=int(top_k),
                temperature=float(temperature),
            )
            self.last_inference = out
            return out


class DashboardHandler(BaseHTTPRequestHandler):
    trainer: Trainer | None = None

    def log_message(self, fmt: str, *args: Any) -> None:  # pragma: no cover
        return

    def _send_html(self, body: str, status: int = 200) -> None:
        data = body.encode("utf-8")
        self.send_response(status)
        self.send_header("Content-Type", "text/html; charset=utf-8")
        self.send_header("Content-Length", str(len(data)))
        self.end_headers()
        self.wfile.write(data)

    def _send_json(self, payload: dict[str, Any], status: int = 200) -> None:
        data = json.dumps(payload, ensure_ascii=True).encode("utf-8")
        self.send_response(status)
        self.send_header("Content-Type", "application/json; charset=utf-8")
        self.send_header("Content-Length", str(len(data)))
        self.end_headers()
        self.wfile.write(data)

    def do_GET(self) -> None:  # noqa: N802
        parsed = urlparse(self.path)
        qs = parse_qs(parsed.query)
        path = parsed.path
        if path == "/":
            self._send_html(self._render_index())
            return
        if path == "/partials/status":
            self._send_html(self._render_status())
            return
        if path == "/partials/supertokens":
            self._send_html(self._render_supertokens(qs))
            return
        if path == "/partials/chaos":
            self._send_html(self._render_chaos(qs))
            return
        if path == "/partials/gap":
            self._send_html(self._render_gap(qs))
            return
        if path == "/api/status":
            t = self._trainer()
            status = t.status_snapshot()
            snap = t.engine.metrics.snapshot()
            payload = {
                "step": int(status.get("step", 0)),
                "active_super": int(status.get("active", 0)),
                "energy_per_advance": float(snap.get("energy_per_advance", 0.0)),
                "glide_ratio": float(snap.get("glide_ratio", 0.0)),
                "gap_compression_ratio": float(snap.get("gap_compression_ratio", 1.0)),
                "best_gap_compression_ratio": float(
                    snap.get("best_gap_compression_ratio", 1.0)
                ),
                "max_symbio_depth": int(snap.get("max_symbio_depth", 0)),
                "root_only_fraction": float(snap.get("root_only_fraction", 1.0)),
                "nonroot_live_capsules": int(snap.get("nonroot_live_capsules", 0)),
                "capsule_half_life": float(snap.get("capsule_half_life", 0.0)),
                "lineage_persistence_1k": float(
                    snap.get("lineage_persistence_1k", 0.0)
                ),
                "chaos_energy_spent": float(snap.get("chaos_energy_spent", 0.0)),
                "chaos_avg_substeps": float(snap.get("chaos_avg_substeps", 0.0)),
            }
            self._send_json(payload)
            return
        self._send_html("<h1>404</h1>", status=404)

    def do_POST(self) -> None:  # noqa: N802
        parsed = urlparse(self.path)
        if parsed.path == "/action/toggle":
            trainer = self._trainer()
            trainer.toggle()
            self._send_html(self._render_status())
            return
        if parsed.path == "/action/infer":
            form = self._form_data()
            prompt = str(form.get("prompt", "") or "")
            new_tokens = _parse_int(form.get("new_tokens"), default=96, lo=1, hi=2000)
            passes = _parse_int(form.get("passes"), default=2, lo=1, hi=12)
            window = _parse_int(form.get("window"), default=16, lo=1, hi=256)
            top_k = _parse_int(form.get("top_k"), default=16, lo=1, hi=256)
            temperature = _parse_float(form.get("temperature"), default=0.0)
            out = self._trainer().run_inference(
                prompt=prompt,
                new_tokens=new_tokens,
                passes=passes,
                window=window,
                top_k=top_k,
                temperature=temperature,
            )
            self._send_html(self._render_infer_result(out))
            return
        self._send_html("<h1>404</h1>", status=404)

    def _trainer(self) -> Trainer:
        trainer = self.__class__.trainer
        if trainer is None:
            raise RuntimeError("trainer not initialized")
        return trainer

    def _form_data(self) -> dict[str, str]:
        try:
            n = int(self.headers.get("Content-Length", "0") or "0")
        except ValueError:
            n = 0
        raw = (
            self.rfile.read(max(n, 0)).decode("utf-8", errors="ignore") if n > 0 else ""
        )
        parsed = parse_qs(raw)
        out: dict[str, str] = {}
        for k, vals in parsed.items():
            out[k] = vals[0] if vals else ""
        return out

    def _render_index(self) -> str:
        return """<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Emera Live Dashboard</title>
  <script src="https://unpkg.com/htmx.org@1.9.12"></script>
  <style>
    :root { --bg:#0b0f17; --card:#121826; --muted:#7e8aa6; --text:#ecf2ff; --line:#26324a; --good:#61d9a0; --warn:#ffcc66; --bad:#ff6a7a; --accent:#5dd6ff; }
    * { box-sizing:border-box; }
    body { margin:0; font-family: ui-sans-serif, SF Pro Text, Segoe UI, sans-serif; background:linear-gradient(180deg,#0b0f17 0%,#090d15 100%); color:var(--text); }
    .wrap { max-width: 1400px; margin: 20px auto; padding: 0 14px 20px; }
    .grid { display:grid; gap:12px; grid-template-columns: 1.3fr 1fr; }
    .card { background:var(--card); border:1px solid var(--line); border-radius:12px; padding:10px 12px; }
    .row { display:flex; gap:10px; align-items:center; flex-wrap:wrap; }
    .muted { color:var(--muted); font-size: 12px; }
    .k { color: var(--muted); font-size:12px; text-transform:uppercase; letter-spacing:.08em; }
    .v { font-weight: 700; margin-right: 8px; }
    .btn { border:1px solid var(--line); border-radius:8px; background:#172037; color:var(--text); padding:6px 10px; cursor:pointer; }
    input, select { border:1px solid var(--line); border-radius:8px; background:#0f1422; color:var(--text); padding:6px 8px; }
    table { width:100%; border-collapse: collapse; font-size:12px; }
    th, td { border-bottom:1px solid var(--line); padding:6px; text-align:left; vertical-align:top; }
    th { color:var(--muted); position:sticky; top:0; background:var(--card); }
    .mono { font-family: ui-monospace, SFMono-Regular, Menlo, monospace; }
    .plot { border:1px solid var(--line); border-radius:8px; width:100%; max-width:100%; height:auto; }
    .pill { font-size:11px; border-radius:999px; padding:2px 8px; border:1px solid var(--line); }
    .ok { color:var(--good); border-color:#295940; }
    .warn { color:var(--warn); border-color:#695526; }
    .bad { color:var(--bad); border-color:#6a2b36; }
    @media (max-width: 980px) { .grid { grid-template-columns: 1fr; } }
  </style>
</head>
<body>
  <div class="wrap">
    <h2 style="margin: 0 0 12px;">Emera Live Inspection</h2>
    <div id="status" hx-get="/partials/status" hx-trigger="load, every 1s"></div>
    <div class="grid">
      <div class="card">
        <div class="row" style="justify-content:space-between;">
          <strong>SuperTokens</strong>
          <span class="muted">filter + sort + chaos-game render</span>
        </div>
        <form id="st-filter" class="row" hx-get="/partials/supertokens" hx-target="#supertokens" hx-trigger="change, keyup delay:300ms from:input">
          <label class="muted">Sort <select name="sort"><option value="energy">energy</option><option value="age">age</option><option value="conf">conf</option><option value="len">len</option><option value="inactive">inactive</option><option value="id">id</option></select></label>
          <label class="muted">Desc <input type="checkbox" name="desc" checked /></label>
          <label class="muted">Min energy <input type="number" name="min_energy" value="0" step="0.1" style="width:88px;" /></label>
          <label class="muted">Limit <input type="number" name="limit" value="120" step="1" min="1" max="1000" style="width:78px;" /></label>
          <label class="muted">Chaos steps <input type="number" name="chaos_steps" value="2000" step="100" min="1" max="50000" style="width:96px;" /></label>
          <label class="muted">Query <input type="text" name="q" placeholder="id or text" /></label>
        </form>
        <div id="supertokens" hx-get="/partials/supertokens" hx-include="#st-filter" hx-trigger="load, every 2s"></div>
      </div>
      <div style="display:grid; gap:12px;">
        <div class="card">
          <div class="row" style="justify-content:space-between;">
            <strong>Inference</strong>
            <span class="muted">sliding-window diffusion refinement</span>
          </div>
          <form hx-post="/action/infer" hx-target="#infer-result" hx-swap="innerHTML">
            <label class="muted">Prompt</label>
            <textarea name="prompt" rows="6" style="width:100%; margin:6px 0 8px; border:1px solid var(--line); border-radius:8px; background:#0f1422; color:var(--text); padding:8px;">In the beginning</textarea>
            <div class="row">
              <label class="muted">New <input type="number" name="new_tokens" value="96" min="1" max="2000" style="width:90px;" /></label>
              <label class="muted">Passes <input type="number" name="passes" value="2" min="1" max="12" style="width:80px;" /></label>
              <label class="muted">Window <input type="number" name="window" value="16" min="1" max="256" style="width:80px;" /></label>
              <label class="muted">Top-k <input type="number" name="top_k" value="16" min="1" max="256" style="width:80px;" /></label>
              <label class="muted">Temp <input type="number" name="temperature" value="0.35" step="0.05" min="0" max="2" style="width:80px;" /></label>
              <button class="btn" type="submit">Run</button>
            </div>
          </form>
          <div id="infer-result" class="mono muted" style="margin-top:8px;">run inference to see generated output.</div>
        </div>
        <div id="gap" hx-get="/partials/gap" hx-trigger="load, every 2s"></div>
        <div id="chaos" class="card"><strong>Chaos Game</strong><p class="muted">Click any token's Chaos button to render.</p></div>
      </div>
    </div>
  </div>
</body>
</html>"""

    def _render_status(self) -> str:
        t = self._trainer().status_snapshot()
        state_cls = "ok" if t["running"] else ("bad" if t["error"] else "warn")
        state_txt = "running" if t["running"] else ("error" if t["error"] else "paused")
        err = (
            f"<div class='pill bad mono'>{html.escape(str(t['error']))}</div>"
            if t["error"]
            else ""
        )
        return (
            "<div id='status' class='card'>"
            "<div class='row' style='justify-content:space-between;'>"
            "<div class='row'>"
            f"<span class='pill {state_cls}'>{state_txt}</span>"
            f"<button class='btn' hx-post='/action/toggle' hx-target='#status' hx-swap='outerHTML'>{'Pause' if t['running'] else 'Resume'}</button>"
            "</div>"
            f"<div class='muted'>steps/s {t['steps_per_s']:.1f}</div>"
            "</div>"
            "<div class='row'>"
            f"<span class='k'>Location</span><span class='v mono'>{html.escape(str(t['location']))}</span>"
            f"<span class='k'>Frontier</span><span class='v mono'>{html.escape(str(t['frontier_hex']))}</span>"
            "</div>"
            f"<div class='mono' style='font-size:13px; margin:6px 0 8px;'>{html.escape(str(t['frontier_line']))}</div>"
            "<div class='row'>"
            f"<span class='k'>step</span><span class='v'>{t['step']}</span>"
            f"<span class='k'>active</span><span class='v'>{t['active']}</span>"
            f"<span class='k'>proposers</span><span class='v'>{t['proposers']}</span>"
            f"<span class='k'>winner</span><span class='v'>{t['winner']}</span>"
            f"<span class='k'>match/adv</span><span class='v'>{t['match']}/{t['advance']}</span>"
            f"<span class='k'>R</span><span class='v'>{t['return']:+.3f}</span>"
            f"<span class='k'>payout</span><span class='v'>{t['jackpot']:.3f}</span>"
            f"<span class='k'>reservoir</span><span class='v'>{t['reservoir']:.2f}</span>"
            f"<span class='k'>drift</span><span class='v'>{t['drift']:+.4f}</span>"
            f"<span class='k'>EPA</span><span class='v'>{t['epa']:.4f}</span>"
            f"<span class='k'>glide</span><span class='v'>{t['glide']:.3f}</span>"
            f"<span class='k'>root_frac</span><span class='v'>{t['root_frac']:.3f}</span>"
            f"<span class='k'>caps_nonroot</span><span class='v'>{t['caps_nonroot']}</span>"
            f"<span class='k'>caps_t1k</span><span class='v'>{t['caps_t1k']:.3f}</span>"
            f"<span class='k'>caps_half_life</span><span class='v'>{t['caps_half_life']:.1f}</span>"
            f"<span class='k'>chaos_sub</span><span class='v'>{t['chaos_avg_sub']:.1f}</span>"
            f"<span class='k'>chaos_E</span><span class='v'>{t['chaos_energy']:.4f}</span>"
            "</div>"
            f"{err}"
            "</div>"
        )

    def _render_supertokens(self, qs: dict[str, list[str]]) -> str:
        sort = (qs.get("sort", ["energy"])[0] or "energy").strip()
        desc = _parse_bool(
            qs.get("desc", ["on"])[0] if "desc" in qs else None, default=True
        )
        min_energy = _parse_float(qs.get("min_energy", ["0"])[0], default=0.0)
        limit = _parse_int(qs.get("limit", ["120"])[0], default=120, lo=1, hi=2000)
        chaos_steps = _parse_int(
            qs.get("chaos_steps", ["2000"])[0], default=2000, lo=1, hi=50000
        )
        q = qs.get("q", [""])[0]
        rows = self._trainer().supertokens(
            sort_by=sort, desc=desc, min_energy=min_energy, query=q, limit=limit
        )

        head = (
            "<div class='muted' style='margin:8px 0;'>"
            f"rows={len(rows)} sort={html.escape(sort)} {'desc' if desc else 'asc'} min_energy={min_energy:.3f}"
            "</div>"
        )
        table_head = (
            "<table><thead><tr>"
            "<th>id</th><th>age</th><th>energy</th><th>inactive</th><th>len</th><th>conf</th><th>frontier</th><th>txt</th><th>lineage</th><th></th>"
            "</tr></thead><tbody>"
        )
        body = []
        for r in rows:
            frontier = "yes" if r["contains_frontier"] else "no"
            body.append(
                "<tr>"
                f"<td class='mono'>{r['id']}</td>"
                f"<td>{r['age']}</td>"
                f"<td>{r['energy']:.3f}</td>"
                f"<td>{r['inactive']}</td>"
                f"<td>{r['len']}</td>"
                f"<td>{r['conf']:.3f}</td>"
                f"<td>{frontier}</td>"
                f"<td class='mono'>{html.escape(r['text'])}</td>"
                f"<td class='mono'>{html.escape(r['lineage'])}</td>"
                f"<td><button class='btn' hx-get='/partials/chaos?token_id={r['id']}&steps={chaos_steps}' hx-target='#chaos'>Chaos</button></td>"
                "</tr>"
            )
        tail = "</tbody></table>"
        return head + table_head + "".join(body) + tail

    def _render_chaos(self, qs: dict[str, list[str]]) -> str:
        token_id = _parse_int(qs.get("token_id", ["-1"])[0], default=-1)
        steps = _parse_int(qs.get("steps", ["2000"])[0], default=2000, lo=1, hi=50000)
        if token_id < 0:
            return "<div class='card'><strong>Chaos Game</strong><p class='muted'>missing token_id.</p></div>"
        return self._trainer().chaos_svg(token_id=token_id, steps=steps)

    def _render_gap(self, qs: dict[str, list[str]]) -> str:
        limit = _parse_int(qs.get("limit", ["24"])[0], default=24, lo=1, hi=200)
        scatter_limit = _parse_int(
            qs.get("scatter", ["220"])[0], default=220, lo=16, hi=1000
        )
        snap = self._trainer().gap_snapshot(limit=limit, scatter_limit=scatter_limit)
        rows = snap["rows"]
        scatter = snap["scatter"]
        ptr = int(snap["ptr"])

        width, height = 420, 220
        pad = 10.0
        dots = []
        for p in scatter:
            x = ((p["x"] + 1.0) * 0.5) * (width - 2 * pad) + pad
            y = (1.0 - (p["y"] + 1.0) * 0.5) * (height - 2 * pad) + pad
            dots.append(
                f"<circle cx='{x:.2f}' cy='{y:.2f}' r='{p['r']:.2f}' fill='hsl({p['h']},85%,62%)' fill-opacity='0.65'/>"
            )
        head = (
            "<div class='card'>"
            "<div class='row' style='justify-content:space-between;'>"
            "<strong>Gap Buffer</strong>"
            f"<span class='muted'>ptr={ptr} active={len(scatter)}</span>"
            "</div>"
            f"<svg width='{width}' height='{height}' viewBox='0 0 {width} {height}' class='plot'>"
            "<rect x='0' y='0' width='100%' height='100%' fill='#0b0f17'/>"
            + "".join(dots)
            + "</svg>"
            "<table><thead><tr><th>slot</th><th>energy</th><th>emitter</th><th>step</th><th>round</th><th>x</th><th>y</th></tr></thead><tbody>"
        )
        body = []
        for r in rows:
            body.append(
                "<tr>"
                f"<td class='mono'>{r['slot']}</td>"
                f"<td>{r['energy']:.4f}</td>"
                f"<td class='mono'>{r['emitter']}</td>"
                f"<td>{r['step']}</td>"
                f"<td>{r['round']}</td>"
                f"<td>{r['x']:.3f}</td>"
                f"<td>{r['y']:.3f}</td>"
                "</tr>"
            )
        return head + "".join(body) + "</tbody></table></div>"

    def _render_infer_result(self, out: dict[str, Any]) -> str:
        text = str(out.get("output_text", "") or "")
        debug = out.get("debug", [])
        debug_html = ""
        if isinstance(debug, list) and debug:
            debug_html = (
                "<div class='muted' style='margin-top:6px;'>"
                + "<br/>".join(html.escape(str(x)) for x in debug[:8])
                + "</div>"
            )
        return (
            "<div class='card'>"
            f"<div class='muted'>mode={html.escape(str(out.get('token_space', '?')))} "
            f"gen={int(out.get('generated_tokens', 0))} "
            f"passes={int(out.get('passes', 1))} "
            f"window={int(out.get('window', 1))} "
            f"top_k={int(out.get('top_k', 1))} "
            f"temp={float(out.get('temperature', 0.0)):.2f}</div>"
            f"<div style='margin-top:8px; white-space:pre-wrap;'>{html.escape(text)}</div>"
            f"{debug_html}"
            "</div>"
        )


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        description="Emera HTMX dashboard with threaded training"
    )
    p.add_argument("--host", type=str, default="127.0.0.1")
    p.add_argument("--port", type=int, default=8787)
    p.add_argument("--seed", type=int, default=42)
    p.add_argument(
        "--token-space",
        type=str,
        choices=["byte_parity", "gpt2"],
        default="gpt2",
    )
    p.add_argument("--gpt2-model-name", type=str, default="gpt2")
    p.add_argument("--base-tokens", type=int, default=None)
    p.add_argument("--d-latent", type=int, default=32)
    p.add_argument("--gap-dim", type=int, default=16)
    p.add_argument("--gap-len", type=int, default=128)
    p.add_argument("--k-rounds", type=int, default=6)
    p.add_argument("--chaos-substeps-per-round", type=int, default=1)
    p.add_argument("--chaos-svg-default-steps", type=int, default=2000)
    p.add_argument("--chaos-svg-max-steps", type=int, default=50000)
    p.add_argument(
        "--gap-read-backend", type=str, choices=["auto", "numpy", "jax"], default="auto"
    )
    p.add_argument("--gap-read-batch-size", type=int, default=128)
    p.add_argument("--capsule-frontier-window", type=int, default=48)
    p.add_argument("--capsule-mint-parent-pool", type=int, default=10)
    p.add_argument("--season-topology-jitter", type=float, default=0.02)
    p.add_argument("--chaos-min-substeps", type=int, default=1)
    p.add_argument("--chaos-max-substeps", type=int, default=64)
    p.add_argument("--chaos-substep-cost", type=float, default=0.004)
    p.add_argument("--corpus-file", type=str, default="data/bible.txt")
    p.add_argument("--world-len", type=int, default=200_000)
    p.add_argument("--world-vocab-size", type=int, default=50257)
    p.add_argument("--steps-per-tick", type=int, default=1)
    p.add_argument("--sleep-ms", type=float, default=0.0)
    return p.parse_args()


def main() -> None:
    args = parse_args()
    corpus = args.corpus_file
    if corpus and not Path(corpus).exists():
        raise FileNotFoundError(f"corpus file not found: {corpus}")

    base_tokens = int(args.base_tokens) if args.base_tokens is not None else None
    world_vocab = int(args.world_vocab_size)
    if args.token_space == "gpt2":
        if base_tokens is None:
            base_tokens = 50257
        if args.world_vocab_size == 512:
            world_vocab = base_tokens

    cfg = replace(
        EmeraConfig(),
        seed=int(args.seed),
        token_space=str(args.token_space),
        gpt2_model_name=str(args.gpt2_model_name),
        base_tokens=int(base_tokens)
        if base_tokens is not None
        else EmeraConfig().base_tokens,
        d_latent=max(int(args.d_latent), 4),
        gap_dim=max(int(args.gap_dim), 4),
        gap_len=max(int(args.gap_len), 4),
        k_rounds=max(int(args.k_rounds), 1),
        chaos_substeps_per_round=max(int(args.chaos_substeps_per_round), 1),
        chaos_svg_default_steps=max(int(args.chaos_svg_default_steps), 1),
        chaos_svg_max_steps=max(
            int(args.chaos_svg_max_steps), int(args.chaos_svg_default_steps), 1
        ),
        gap_read_backend=str(args.gap_read_backend),
        gap_read_batch_size=max(int(args.gap_read_batch_size), 1),
        capsule_frontier_window=max(int(args.capsule_frontier_window), 1),
        capsule_mint_parent_pool=max(int(args.capsule_mint_parent_pool), 2),
        season_topology_jitter=max(float(args.season_topology_jitter), 0.0),
        chaos_min_substeps=max(int(args.chaos_min_substeps), 1),
        chaos_max_substeps=max(int(args.chaos_max_substeps), 1),
        chaos_substep_cost=max(float(args.chaos_substep_cost), 0.0),
        corpus_file=corpus,
        world_len=int(args.world_len),
        world_vocab_size=int(world_vocab),
    )
    cfg.validate()

    trainer = Trainer(
        cfg=cfg,
        steps_per_tick=int(args.steps_per_tick),
        sleep_s=float(args.sleep_ms) / 1000.0,
    )
    trainer.start()

    DashboardHandler.trainer = trainer
    server = ThreadingHTTPServer((args.host, int(args.port)), DashboardHandler)
    print(f"Emera dashboard running at http://{args.host}:{args.port}")
    print("Press Ctrl+C to stop.")
    try:
        server.serve_forever(poll_interval=0.5)
    except KeyboardInterrupt:
        pass
    finally:
        server.shutdown()
        trainer.stop()


if __name__ == "__main__":
    main()
-e 

===== engine.py =====
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Optional

import numpy as np
import zlib

from config import EmeraConfig
from cooperation import CooperationStats
from gap_field import GapField
from genome import (
    Proposal,
    SuperToken,
    create_initial_population,
    make_initial_super_token,
    mint_child_from_parent,
    mint_child_from_parents,
)
from identity import BaseIdentity, create_base_identity
from ledger import (
    attempt_cost_with_base,
    base_toll,
    discovery_cost,
    jackpot_reward_with_base,
    realized_return,
    silence_credit_with_coeffs,
)
from metrics import MetricsTracker
from world import (
    World,
    decode_gpt2_tokens,
    encode_gpt2_tokens,
    encode_utf8_parity_tokens,
)


def _normalize(v: np.ndarray, eps: float = 1e-8) -> np.ndarray:
    n = np.linalg.norm(v)
    if n <= eps:
        return np.zeros_like(v)
    return v / n


@dataclass
class StepResult:
    stats: dict
    events: list[str]


class EmeraEngine:
    def __init__(self, cfg: EmeraConfig):
        cfg.validate()
        self.cfg = cfg
        self.rng = np.random.default_rng(cfg.seed)
        self.world = World.create(cfg, self.rng)
        self.base_identity: BaseIdentity = create_base_identity(cfg, self.rng)
        self.base_latent = self.base_identity.latent.astype(np.float32)
        self.base_vocab_size = int(self.base_latent.shape[0])
        self.decode_token_ids = np.arange(
            min(cfg.world_vocab_size, self.base_vocab_size), dtype=np.int32
        )
        self.decode_latent = self.base_latent[self.decode_token_ids]
        self.gap = GapField(cfg)
        identity_bank = self._build_initial_identity_bank(cfg.initial_super_tokens)
        initial_population = create_initial_population(
            cfg=cfg,
            rng=self.rng,
            start_token_id=self.base_vocab_size,
            birth_step=0,
            identity_bank=identity_bank,
        )
        self.super_tokens: dict[int, SuperToken] = {}
        self.token_anchor_slot: dict[int, int] = {}
        self.lineage_depth: dict[int, int] = {
            int(tid): 0 for tid in initial_population.keys()
        }
        self.max_symbio_depth_ever = 0
        self.next_token_id = self.base_vocab_size + cfg.initial_super_tokens
        self.capsule_half_life_ema = 0.0
        self.capsule_death_events = 0
        self.step_idx = 0
        self._seed_initial_capsules(initial_population)
        self._sync_super_tokens_from_gap()
        self.coop = CooperationStats(alpha=cfg.ema_alpha)
        self.metrics = MetricsTracker()
        self.last_resonance_strength: dict[int, float] = {}
        self.laws = {
            "attempt_cost_base": float(cfg.attempt_cost_base),
            "jackpot_base": float(cfg.jackpot_base),
            "silence_log_coeff": float(cfg.silence_log_coeff),
            "silence_exp_coeff": float(cfg.silence_exp_coeff),
            "ambient_dissipation": float(cfg.ambient_dissipation),
            "spawn_cost": float(cfg.spawn_cost),
            "mint_delta": float(cfg.mint_delta),
            "pareto_alpha": float(cfg.pareto_alpha_init),
        }
        active0 = float(max(len(self.super_tokens), 1))
        self.law_ema = {
            "active": active0,
            "match_rate": 0.0,
            "proposal_pressure": 0.0,
            "birth_rate": 0.0,
            "death_rate": 0.0,
        }
        self.law_drive_ema = {
            "collapse_drive": 0.0,
            "overfire": 0.0,
            "underfire": 0.0,
            "imbalance": 0.0,
            "err_active": 0.0,
            "err_match": 0.0,
        }
        self.season_wave_ema = 0.0
        self.energy_reservoir = float(
            np.clip(cfg.energy_reservoir_init, 0.0, cfg.energy_reservoir_cap)
        )
        self.total_energy_ref = self._total_energy()

    def _frontier_latent(self) -> np.ndarray:
        tid = int(np.clip(self.world.peek(1)[0], 0, self.base_latent.shape[0] - 1))
        return self.base_latent[tid]

    def _sample_identity_bytes_from_world(self, length: int) -> np.ndarray:
        l = max(1, int(length))
        if self.world.length <= 0:
            return self.rng.integers(0, 256, size=(l,), dtype=np.int32)
        start = int(self.rng.integers(0, self.world.length))
        idx = (start + np.arange(l, dtype=np.int64)) % self.world.length
        vals = self.world.tokens[idx].astype(np.int32)
        if self.cfg.token_space == "byte_parity":
            vals = (vals & 0xFF).astype(np.int32)
        return vals

    def _build_initial_identity_bank(self, count: int) -> list[np.ndarray]:
        bank: list[np.ndarray] = []
        for _ in range(max(0, int(count))):
            bank.append(self._sample_identity_bytes_from_world(2))
        return bank

    def _seed_initial_capsules(self, initial_population: dict[int, SuperToken]) -> None:
        if not initial_population:
            return
        slots = np.arange(int(self.cfg.gap_len), dtype=np.int32)
        self.rng.shuffle(slots)
        for i, token in enumerate(initial_population.values()):
            slot = int(slots[i % slots.size])
            self.gap.clear_slot(slot)
            self.gap.points[slot] = np.asarray(token.signature, dtype=np.float32)
            self.gap.velocity[slot] = 0.0
            self.gap.phase[slot] = float(token.phase)
            self.gap.omega[slot] = float(token.omega)
            self.gap.energy[slot] = float(
                np.clip(token.energy, 0.0, self.cfg.token_energy_cap)
            )
            self.gap.emitter_id[slot] = int(token.token_id)
            self.gap.step_idx[slot] = int(self.step_idx)
            self.gap.round_idx[slot] = 0
            self.gap.genome_fragment[slot].fill(-1)
            ident = np.asarray(token.identity_bytes, dtype=np.int32).reshape(-1)
            n = min(int(ident.size), int(self.gap.genome_fragment.shape[1]))
            if n > 0:
                self.gap.genome_fragment[slot, :n] = ident[:n]
                self.gap.genome_len[slot] = int(n)
                self.gap.genome_weight[slot] = 1.0
            self.gap.ifs_fragment[slot] = np.asarray(token.ifs, dtype=np.float32)
            self.gap.ifs_weight[slot] = 1.0
            self.gap._store_capsule(slot, token, lineage_depth=0)
            self.token_anchor_slot[int(token.token_id)] = int(slot)

    def _token_from_gap_slot(self, slot: int) -> SuperToken:
        s = int(slot)
        parent_a = int(self.gap.capsule_parent_a[s])
        parent_b = int(self.gap.capsule_parent_b[s])
        n = int(self.gap.capsule_identity_len[s])
        ident = np.asarray(
            self.gap.capsule_identity[s, : max(n, 0)], dtype=np.int32
        ).copy()
        if ident.size <= 0:
            ident = np.asarray([32], dtype=np.int32)
        if parent_a < 0 and parent_b < 0:
            if ident.size >= 2:
                ident = ident[:2]
            elif ident.size == 1:
                vmax = self._identity_symbol_max()
                ident = np.asarray(
                    [int(ident[0]), int(self.rng.integers(0, vmax + 1))], dtype=np.int32
                )
            else:
                ident = self._sample_identity_bytes_from_world(2)
        traits = np.asarray(self.gap.capsule_traits[s], dtype=np.float32)
        return SuperToken(
            token_id=int(self.gap.capsule_token_id[s]),
            parent_a=parent_a,
            parent_b=parent_b,
            energy=float(np.clip(self.gap.energy[s], 0.0, self.cfg.token_energy_cap)),
            inactivity_steps=int(self.gap.capsule_inactivity_steps[s]),
            state_vec=np.asarray(
                self.gap.capsule_state_vec[s], dtype=np.float32
            ).copy(),
            signature=np.asarray(
                self.gap.capsule_signature[s], dtype=np.float32
            ).copy(),
            proposal_drift=np.asarray(
                self.gap.capsule_proposal_drift[s], dtype=np.float32
            ).copy(),
            ifs=np.asarray(self.gap.capsule_ifs[s], dtype=np.float32).copy(),
            phase=float(self.gap.phase[s]),
            omega=float(self.gap.omega[s]),
            activation_threshold=float(traits[0]),
            emission_amplitude=float(traits[1]),
            emission_decay=float(traits[2]),
            silence_growth_rate=float(traits[3]),
            resonance_width=float(traits[4]),
            phase_coupling=float(traits[5]),
            velocity_coupling=float(traits[6]),
            proposal_length_bias=float(traits[7]),
            identity_bytes=ident,
            birth_step=int(self.gap.capsule_birth_step[s]),
            low_energy_steps=int(self.gap.capsule_low_energy_steps[s]),
            max_paid_bet=float(self.gap.capsule_max_paid_bet[s]),
            max_silent_correct=int(self.gap.capsule_max_silent_correct[s]),
        )

    def _sync_super_tokens_from_gap(self) -> None:
        live = self.gap.live_capsule_slots(
            min_energy=max(self.cfg.min_viable_energy * 0.25, 1e-12)
        )
        chosen: dict[int, int] = {}
        for slot in live.tolist():
            tid = int(self.gap.capsule_token_id[int(slot)])
            if tid < 0:
                continue
            prev = chosen.get(tid)
            if prev is None:
                chosen[tid] = int(slot)
                continue
            prev_key = (
                int(self.gap.step_idx[prev]),
                float(self.gap.energy[prev]),
            )
            cur_key = (
                int(self.gap.step_idx[int(slot)]),
                float(self.gap.energy[int(slot)]),
            )
            if cur_key > prev_key:
                chosen[tid] = int(slot)

        self.super_tokens = {}
        self.token_anchor_slot = {}
        for tid, slot in chosen.items():
            tok = self._token_from_gap_slot(int(slot))
            self.super_tokens[int(tid)] = tok
            self.token_anchor_slot[int(tid)] = int(slot)
            if int(tid) not in self.lineage_depth:
                d = int(self.gap.capsule_lineage_depth[int(slot)])
                self.lineage_depth[int(tid)] = max(d, 0)
        # Slots that reference stale duplicate token ids are cleared to preserve one-capsule-per-id.
        keep_slots = set(int(s) for s in self.token_anchor_slot.values())
        for slot in live.tolist():
            s = int(slot)
            tid = int(self.gap.capsule_token_id[s])
            if tid < 0:
                continue
            if s not in keep_slots:
                self.gap.clear_slot(s)

    def _refresh_capsule_slot(
        self, token: SuperToken, slot: int, read_strength: float = 0.0
    ) -> None:
        s = int(slot)
        if s < 0 or s >= int(self.cfg.gap_len):
            return
        self.gap.energy[s] = float(
            np.clip(token.energy, 0.0, self.cfg.token_energy_cap)
        )
        self.gap.phase[s] = float(token.phase)
        self.gap.omega[s] = float(token.omega)
        self.gap.emitter_id[s] = int(token.token_id)
        self.gap.step_idx[s] = int(self.step_idx)
        self.gap.genome_fragment[s].fill(-1)
        ident = np.asarray(token.identity_bytes, dtype=np.int32).reshape(-1)
        n = min(int(ident.size), int(self.gap.genome_fragment.shape[1]))
        if n > 0:
            vmax = self._identity_symbol_max()
            self.gap.genome_fragment[s, :n] = np.clip(ident[:n], 0, vmax).astype(
                np.int32, copy=False
            )
            self.gap.genome_len[s] = int(n)
            self.gap.genome_weight[s] = float(max(read_strength, 1e-3))
        else:
            self.gap.genome_len[s] = 0
            self.gap.genome_weight[s] = 0.0
        self.gap.ifs_fragment[s] = np.asarray(token.ifs, dtype=np.float32)
        self.gap.ifs_weight[s] = float(max(read_strength, 1e-3))
        depth = int(self.lineage_depth.get(int(token.token_id), 0))
        self.gap._store_capsule(s, token, lineage_depth=depth)

    def _write_capsule_emission(
        self,
        token: SuperToken,
        point: np.ndarray,
        velocity: np.ndarray,
        energy: float,
        read_strength: float,
        round_idx: int,
    ) -> int:
        old_slot = self.token_anchor_slot.get(int(token.token_id), -1)
        write_slot = int(self.gap.ptr)
        self.gap.write(
            point=point,
            velocity=velocity,
            phase=float(token.phase),
            omega=float(token.omega),
            energy=float(np.clip(energy, 0.0, self.cfg.token_energy_cap)),
            genome_fragment=token.identity_bytes,
            genome_weight=read_strength,
            ifs_fragment=token.ifs,
            ifs_weight=read_strength,
            emitter_id=int(token.token_id),
            step_idx=self.step_idx,
            round_idx=round_idx,
            capsule_token=token,
            lineage_depth=int(self.lineage_depth.get(int(token.token_id), 0)),
        )
        self.token_anchor_slot[int(token.token_id)] = int(write_slot)
        if (
            int(old_slot) >= 0
            and int(old_slot) != int(write_slot)
            and int(self.gap.capsule_token_id[int(old_slot)]) == int(token.token_id)
        ):
            self.gap.clear_slot(int(old_slot))
        return int(write_slot)

    def _frontier_local_slots(self) -> np.ndarray:
        n = int(self.cfg.gap_len)
        if n <= 0:
            return np.zeros((0,), dtype=np.int32)
        win = int(np.clip(self.cfg.capsule_frontier_window, 1, n))
        if win >= n:
            return np.arange(n, dtype=np.int32)
        center = int(self.world.cursor % n)
        half = int(win // 2)
        offsets = np.arange(-half, -half + win, dtype=np.int32)
        return ((center + offsets) % n).astype(np.int32, copy=False)

    def _local_live_token_ids(self) -> list[int]:
        local = self._frontier_local_slots()
        if local.size == 0:
            return []
        local_set = set(int(x) for x in local.tolist())
        out: list[int] = []
        for tid, slot in self.token_anchor_slot.items():
            if int(slot) not in local_set:
                continue
            tok = self.super_tokens.get(int(tid))
            if tok is None:
                continue
            if float(tok.energy) <= float(self.cfg.min_viable_energy):
                continue
            out.append(int(tid))
        return out

    def _identity_symbol_max(self) -> int:
        if self.cfg.token_space == "byte_parity":
            return 255
        return max(int(self.base_vocab_size) - 1, 0)

    def _sample_gap_genome_fragment(
        self, preferred_emitters: set[int] | None = None
    ) -> np.ndarray | None:
        lens = np.asarray(self.gap.genome_len, dtype=np.int32)
        valid = lens > 0
        if not np.any(valid):
            return None

        if preferred_emitters:
            emit = np.asarray(self.gap.emitter_id, dtype=np.int64)
            pref_ids = np.fromiter((int(x) for x in preferred_emitters), dtype=np.int64)
            pref_mask = np.isin(emit, pref_ids)
            pick = valid & pref_mask
            if np.any(pick):
                valid = pick

        idx = np.where(valid)[0]
        if idx.size == 0:
            return None
        w = np.asarray(self.gap.energy[idx], dtype=np.float64) * np.maximum(
            np.asarray(self.gap.genome_weight[idx], dtype=np.float64), 1e-6
        )
        w = np.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)
        sw = float(np.sum(w))
        if sw <= 1e-12:
            j = int(self.rng.integers(0, idx.size))
        else:
            probs = w / sw
            j = int(self.rng.choice(idx.size, p=probs))
        slot = int(idx[j])
        n = int(self.gap.genome_len[slot])
        if n <= 0:
            return None
        vmax = self._identity_symbol_max()
        frag = np.asarray(self.gap.genome_fragment[slot, :n], dtype=np.int32)
        return np.clip(frag, 0, vmax).astype(np.int32, copy=True)

    def _mix_gap_genome_fragments(
        self, fragments: list[np.ndarray]
    ) -> np.ndarray | None:
        seq = [
            np.asarray(f, dtype=np.int32).reshape(-1)
            for f in fragments
            if np.asarray(f).size > 0
        ]
        if not seq:
            return None
        max_len = max(int(self.cfg.proposal_lmax), 1)
        vmax = self._identity_symbol_max()
        if len(seq) == 1:
            out = seq[0].copy()
        else:
            a = seq[0]
            b = seq[1]
            mode = int(self.rng.integers(0, 4))
            if mode == 0:
                na = max(1, a.size // 2)
                nb = max(1, b.size - b.size // 2)
                out = np.concatenate([a[:na], b[-nb:]], axis=0)
            elif mode == 1:
                n = max(1, min(max(a.size, b.size), max_len))
                out = np.empty((n,), dtype=np.int32)
                for i in range(n):
                    ai = min(i, a.size - 1)
                    bi = min(i, b.size - 1)
                    out[i] = int(a[ai]) if (i % 2 == 0) else int(b[bi])
            elif mode == 2:
                out = np.concatenate([a, b], axis=0)
            else:
                out = np.concatenate([b, a], axis=0)
            if out.size > max_len:
                start = int(self.rng.integers(0, out.size - max_len + 1))
                out = out[start : start + max_len]
        if out.size == 0:
            return None
        out = np.clip(out, 0, vmax).astype(np.int32, copy=False)
        if out.size > 0 and self.rng.random() < 0.10:
            k = int(self.rng.integers(0, out.size))
            out = out.copy()
            out[k] = int(self.rng.integers(0, vmax + 1))
        if out.size < max_len and self.rng.random() < 0.08:
            out = np.concatenate(
                [
                    out,
                    np.asarray([int(self.rng.integers(0, vmax + 1))], dtype=np.int32),
                ],
                axis=0,
            )
        return out[:max_len].astype(np.int32, copy=False)

    def _gap_identity_override(self, preferred_emitters: set[int]) -> np.ndarray | None:
        frags: list[np.ndarray] = []
        primary = self._sample_gap_genome_fragment(
            preferred_emitters if preferred_emitters else None
        )
        if primary is not None:
            frags.append(primary)
        if len(preferred_emitters) >= 2:
            secondary = self._sample_gap_genome_fragment(preferred_emitters)
            if secondary is not None:
                frags.append(secondary)
        if len(frags) < 2:
            ambient = self._sample_gap_genome_fragment(None)
            if ambient is not None:
                frags.append(ambient)
        return self._mix_gap_genome_fragments(frags)

    def _sample_gap_ifs_fragment(
        self, preferred_emitters: set[int] | None = None
    ) -> np.ndarray | None:
        w_base = np.asarray(self.gap.ifs_weight, dtype=np.float32)
        valid = w_base > 1e-9
        if not np.any(valid):
            return None

        if preferred_emitters:
            emit = np.asarray(self.gap.emitter_id, dtype=np.int64)
            pref_ids = np.fromiter((int(x) for x in preferred_emitters), dtype=np.int64)
            pref_mask = np.isin(emit, pref_ids)
            pick = valid & pref_mask
            if np.any(pick):
                valid = pick

        idx = np.where(valid)[0]
        if idx.size == 0:
            return None
        w = np.asarray(self.gap.energy[idx], dtype=np.float64) * np.maximum(
            np.asarray(self.gap.ifs_weight[idx], dtype=np.float64), 1e-6
        )
        w = np.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)
        sw = float(np.sum(w))
        if sw <= 1e-12:
            j = int(self.rng.integers(0, idx.size))
        else:
            probs = w / sw
            j = int(self.rng.choice(idx.size, p=probs))
        slot = int(idx[j])
        return np.asarray(self.gap.ifs_fragment[slot], dtype=np.float32).copy()

    def _mix_gap_ifs_fragments(self, fragments: list[np.ndarray]) -> np.ndarray | None:
        seq = [
            np.asarray(f, dtype=np.float32).reshape(int(self.cfg.num_ifs), 2, 3)
            for f in fragments
            if np.asarray(f).size > 0
        ]
        if not seq:
            return None
        if len(seq) == 1:
            out = seq[0].copy()
        else:
            a = seq[0]
            b = seq[1]
            mode = int(self.rng.integers(0, 3))
            if mode == 0:
                alpha = float(self.rng.uniform(0.30, 0.70))
                out = (alpha * a + (1.0 - alpha) * b).astype(np.float32)
            elif mode == 1:
                choose_b = self.rng.random((int(self.cfg.num_ifs), 1, 1)) < 0.5
                out = np.where(choose_b, b, a).astype(np.float32)
            else:
                out = np.mean(np.stack(seq[: min(3, len(seq))], axis=0), axis=0).astype(
                    np.float32
                )
        out += self.rng.normal(
            0.0,
            float(self.cfg.ifs_mutation_scale) * 0.35,
            size=out.shape,
        ).astype(np.float32)
        return np.clip(out, -4.0, 4.0).astype(np.float32)

    def _gap_ifs_override(self, preferred_emitters: set[int]) -> np.ndarray | None:
        frags: list[np.ndarray] = []
        primary = self._sample_gap_ifs_fragment(
            preferred_emitters if preferred_emitters else None
        )
        if primary is not None:
            frags.append(primary)
        if len(preferred_emitters) >= 2:
            secondary = self._sample_gap_ifs_fragment(preferred_emitters)
            if secondary is not None:
                frags.append(secondary)
        if len(frags) < 2:
            ambient = self._sample_gap_ifs_fragment(None)
            if ambient is not None:
                frags.append(ambient)
        return self._mix_gap_ifs_fragments(frags)

    def _gap_to_latent(self, gap_vec: np.ndarray) -> np.ndarray:
        out = np.zeros((self.cfg.d_latent,), dtype=np.float32)
        out[: self.cfg.gap_dim] = gap_vec
        out += 0.12 * self._frontier_latent()
        return out.astype(np.float32)

    def _identity_tokens_at_cursor(self, identity_bytes: np.ndarray) -> np.ndarray:
        return self._identity_suffix_tokens_at_cursor(identity_bytes, offset=0)

    def _identity_suffix_tokens_at_cursor(
        self, identity_bytes: np.ndarray, offset: int
    ) -> np.ndarray:
        b = np.asarray(identity_bytes, dtype=np.int32).reshape(-1)
        if b.size == 0:
            return np.zeros((0,), dtype=np.int32)
        start = int(np.clip(offset, 0, b.size - 1))
        if self.cfg.token_space == "byte_parity":
            suffix = np.clip(b[start:], 0, 255).astype(np.int32)
        else:
            suffix = np.clip(b[start:], 0, max(self.base_vocab_size - 1, 0)).astype(
                np.int32
            )
        if suffix.size == 0:
            return np.zeros((0,), dtype=np.int32)
        if self.cfg.token_space == "byte_parity":
            frontier_tid = int(self.world.peek(1)[0])
            p0 = (frontier_tid // 256) & 1
            parity = (p0 + np.arange(suffix.size, dtype=np.int32)) & 1
            return (suffix + 256 * parity).astype(np.int32)
        return suffix.astype(np.int32)

    def _identity_frontier_offsets(self, identity_bytes: np.ndarray) -> np.ndarray:
        b = np.asarray(identity_bytes, dtype=np.int32).reshape(-1)
        if b.size == 0:
            return np.zeros((0,), dtype=np.int32)
        if self.cfg.token_space == "byte_parity":
            frontier_byte = int(self.world.peek(1)[0]) & 0xFF
            return np.where(np.clip(b, 0, 255) == frontier_byte)[0].astype(np.int32)
        frontier_tid = int(np.clip(self.world.peek(1)[0], 0, self.base_vocab_size - 1))
        return np.where(np.clip(b, 0, self.base_vocab_size - 1) == frontier_tid)[
            0
        ].astype(np.int32)

    def _identity_contains_frontier_byte(self, identity_bytes: np.ndarray) -> bool:
        return bool(self._identity_frontier_offsets(identity_bytes).size > 0)

    def _best_identity_alignment(
        self, identity_bytes: np.ndarray
    ) -> tuple[np.ndarray, int, float, bool]:
        offsets = self._identity_frontier_offsets(identity_bytes)
        if offsets.size == 0:
            return np.zeros((0,), dtype=np.int32), 0, 0.0, False
        best_ids = np.zeros((0,), dtype=np.int32)
        best_key = (-1, -1.0, -1, 0)
        best_m = 0
        best_direct = 0.0
        for off in offsets.tolist():
            ids = self._identity_suffix_tokens_at_cursor(
                identity_bytes, offset=int(off)
            )
            if ids.size == 0:
                continue
            m = int(self.world.match_prefix_len(ids.tolist()))
            direct = float(m) / float(max(ids.size, 1))
            key = (m, direct, int(ids.size), -int(off))
            if key > best_key:
                best_key = key
                best_ids = ids
                best_m = m
                best_direct = direct
        if best_ids.size == 0:
            return np.zeros((0,), dtype=np.int32), 0, 0.0, False
        return best_ids, best_m, best_direct, True

    def _identity_prefix_fraction(self, identity_bytes: np.ndarray) -> float:
        _, _, direct, found = self._best_identity_alignment(identity_bytes)
        if not found:
            return 0.0
        return float(direct)

    def _decode_from_identity(
        self, identity_bytes: np.ndarray, resonance_strength: float
    ) -> tuple[np.ndarray, float, float]:
        ids, m, direct, found = self._best_identity_alignment(identity_bytes)
        if ids.size == 0 or not found:
            # Fallback for contrastive probes: decode full identity from offset 0 even
            # when it does not contain the current frontier byte.
            ids = self._identity_tokens_at_cursor(identity_bytes)
            if ids.size == 0:
                return ids, 0.0, 0.0
            m = int(self.world.match_prefix_len(ids.tolist()))
            direct = float(m) / float(max(ids.size, 1))
            frontier_match = 1.0 if int(ids[0]) == int(self.world.peek(1)[0]) else 0.0
        else:
            frontier_match = 1.0
        rs = float(np.clip(resonance_strength, 0.0, 1.0))
        conf = float(
            np.clip(0.05 + 0.80 * direct + 0.10 * frontier_match + 0.05 * rs, 0.0, 1.0)
        )
        if m == int(ids.size):
            conf = max(conf, 0.90)
        quality = float(np.clip(conf * (0.35 + 0.65 * direct), 0.0, 1.5))
        return ids, conf, quality

    def _attempt_cost(self, token: SuperToken) -> float:
        return attempt_cost_with_base(
            token=token,
            cfg=self.cfg,
            base_cost=float(self.laws["attempt_cost_base"]),
        )

    def _silence_credit(self, token: SuperToken) -> float:
        return silence_credit_with_coeffs(
            token=token,
            cfg=self.cfg,
            log_coeff=float(self.laws["silence_log_coeff"]),
            exp_coeff=float(self.laws["silence_exp_coeff"]),
        )

    def _jackpot(
        self,
        match_len: int,
        proposal_len: int,
        quality: float,
        rarity: float,
        inactivity_steps: int,
    ) -> float:
        return jackpot_reward_with_base(
            cfg=self.cfg,
            jackpot_base_value=float(self.laws["jackpot_base"]),
            match_len=match_len,
            proposal_len=proposal_len,
            quality=quality,
            rarity=rarity,
            inactivity_steps=inactivity_steps,
        )

    def _total_energy(self) -> float:
        gap_e = float(np.sum(np.clip(self.gap.energy, 0.0, None)))
        return float(self.energy_reservoir + gap_e)

    def _register_lineage_depth(self, token: SuperToken) -> None:
        pa = int(token.parent_a)
        pb = int(token.parent_b)
        if pa < 0 and pb < 0:
            depth = 0
        else:
            da = int(self.lineage_depth.get(pa, 0)) if pa >= 0 else 0
            db = int(self.lineage_depth.get(pb, 0)) if pb >= 0 else 0
            depth = 1 + max(da, db)
        self.lineage_depth[int(token.token_id)] = int(depth)
        if depth > self.max_symbio_depth_ever:
            self.max_symbio_depth_ever = int(depth)

    def _lineage_metrics(self) -> tuple[int, float]:
        n = len(self.super_tokens)
        if n <= 0:
            return 0, 1.0
        max_depth = 0
        roots = 0
        for tid, tok in self.super_tokens.items():
            depth = int(self.lineage_depth.get(int(tid), 0))
            if depth <= 0 and int(tok.parent_a) >= 0:
                depth = 1
            if depth > max_depth:
                max_depth = depth
            if depth <= 0:
                roots += 1
        return int(max_depth), float(roots) / float(n)

    def _gap_compression_ratio(self) -> float:
        active = np.asarray(self.gap.energy > 1e-9)
        if not np.any(active):
            return 1.0
        lens = np.asarray(self.gap.genome_len[active], dtype=np.int32)
        frags = np.asarray(self.gap.genome_fragment[active], dtype=np.int32)
        seq: list[np.ndarray] = []
        for row, n in zip(frags, lens):
            k = int(n)
            if k <= 0:
                continue
            seq.append(np.clip(row[:k], 0, 65535).astype(np.uint16, copy=False))
        if not seq:
            return 1.0
        raw = np.concatenate(seq, axis=0).tobytes()
        if len(raw) <= 0:
            return 1.0
        comp = zlib.compress(raw, level=9)
        return float(len(comp)) / float(max(len(raw), 1))

    def _reservoir_add(self, amount: float) -> None:
        if not self.cfg.strict_energy_budget:
            return
        if amount <= 0.0:
            return
        self.energy_reservoir = float(
            np.clip(
                self.energy_reservoir + float(amount),
                0.0,
                self.cfg.energy_reservoir_cap,
            )
        )

    def _reservoir_take(self, requested: float) -> float:
        req = max(float(requested), 0.0)
        if req <= 0.0:
            return 0.0
        if not self.cfg.strict_energy_budget:
            return req
        paid = min(req, self.energy_reservoir)
        self.energy_reservoir -= paid
        return float(paid)

    def _drain_token_energy(self, token: SuperToken, requested: float) -> float:
        req = max(float(requested), 0.0)
        if req <= 0.0:
            return 0.0
        current = float(token.energy) if np.isfinite(token.energy) else 0.0
        current = max(current, 0.0)
        drained = min(current, req)
        token.energy = current - drained
        self._reservoir_add(drained)
        return float(drained)

    def _drain_token_to_gap(self, token: SuperToken, requested: float) -> float:
        req = max(float(requested), 0.0)
        if req <= 0.0:
            return 0.0
        current = float(token.energy) if np.isfinite(token.energy) else 0.0
        current = max(current, 0.0)
        drained = min(current, req)
        token.energy = current - drained
        return float(drained)

    def _credit_token_energy(self, token: SuperToken, requested: float) -> float:
        paid = self._reservoir_take(requested)
        token.energy += paid
        return float(paid)

    def _update_law_emas(
        self, match_flag: float, proposal_pressure: float, births: int, deaths: int
    ) -> None:
        d = float(np.clip(self.cfg.adaptation_ema_decay, 0.0, 0.9999))
        one = 1.0 - d
        active = float(max(len(self.super_tokens), 1))
        birth_rate = float(births) / active
        death_rate = float(deaths) / active
        self.law_ema["active"] = d * self.law_ema["active"] + one * float(
            len(self.super_tokens)
        )
        self.law_ema["match_rate"] = d * self.law_ema["match_rate"] + one * float(
            match_flag
        )
        self.law_ema["proposal_pressure"] = d * self.law_ema[
            "proposal_pressure"
        ] + one * float(proposal_pressure)
        self.law_ema["birth_rate"] = d * self.law_ema["birth_rate"] + one * birth_rate
        self.law_ema["death_rate"] = d * self.law_ema["death_rate"] + one * death_rate

    def _adapt_natural_laws(self) -> tuple[str, int]:
        if not self.cfg.dynamic_laws:
            return "", 0
        if self.step_idx % self.cfg.law_update_interval != 0:
            return "", 0

        lr = float(max(self.cfg.adaptation_rate, 0.0))
        if lr <= 0.0:
            return "", 0

        ema = self.law_ema
        tgt_active = max(float(self.cfg.target_active_super), 1.0)
        raw_err_active = (tgt_active - ema["active"]) / tgt_active
        raw_err_match = float(self.cfg.target_match_rate) - ema["match_rate"]
        raw_err_pressure = ema["proposal_pressure"] - float(
            self.cfg.target_proposal_pressure
        )
        raw_imbalance = (ema["death_rate"] - ema["birth_rate"]) - float(
            self.cfg.target_birth_death_gap
        )
        raw_collapse_drive = (
            0.7 * raw_err_active + 0.6 * raw_err_match + 0.4 * raw_imbalance
        )
        raw_overfire = max(raw_err_pressure, 0.0)
        raw_underfire = max(-raw_err_pressure, 0.0)

        # Low-pass filter adaptation signals so law updates stay responsive but less jumpy.
        signal_decay = float(np.clip(self.cfg.adaptation_signal_decay, 0.0, 0.9999))
        one = 1.0 - signal_decay
        self.law_drive_ema["err_active"] = (
            signal_decay * self.law_drive_ema["err_active"] + one * raw_err_active
        )
        self.law_drive_ema["err_match"] = (
            signal_decay * self.law_drive_ema["err_match"] + one * raw_err_match
        )
        self.law_drive_ema["imbalance"] = (
            signal_decay * self.law_drive_ema["imbalance"] + one * raw_imbalance
        )
        self.law_drive_ema["collapse_drive"] = (
            signal_decay * self.law_drive_ema["collapse_drive"]
            + one * raw_collapse_drive
        )
        self.law_drive_ema["overfire"] = (
            signal_decay * self.law_drive_ema["overfire"] + one * raw_overfire
        )
        self.law_drive_ema["underfire"] = (
            signal_decay * self.law_drive_ema["underfire"] + one * raw_underfire
        )

        err_active = float(self.law_drive_ema["err_active"])
        err_match = float(self.law_drive_ema["err_match"])
        imbalance = float(self.law_drive_ema["imbalance"])
        collapse_drive = float(self.law_drive_ema["collapse_drive"])
        overfire = max(float(self.law_drive_ema["overfire"]), 0.0)
        underfire = max(float(self.law_drive_ema["underfire"]), 0.0)

        def upd_mul(key: str, drive: float, lo: float, hi: float) -> None:
            val = float(self.laws[key]) * float(np.exp(lr * drive))
            self.laws[key] = float(np.clip(val, lo, hi))

        upd_mul(
            "attempt_cost_base",
            0.8 * overfire - 0.8 * collapse_drive - 0.7 * underfire,
            self.cfg.attempt_cost_min,
            self.cfg.attempt_cost_max,
        )
        upd_mul(
            "jackpot_base",
            1.0 * collapse_drive + 0.8 * underfire - 0.3 * overfire,
            self.cfg.jackpot_base_min,
            self.cfg.jackpot_base_max,
        )
        if not self.cfg.conserve_total_energy:
            upd_mul(
                "silence_log_coeff",
                0.7 * collapse_drive - 0.3 * overfire - 0.5 * underfire,
                self.cfg.silence_log_min,
                self.cfg.silence_log_max,
            )
            upd_mul(
                "silence_exp_coeff",
                0.9 * collapse_drive - 0.3 * overfire - 0.6 * underfire,
                self.cfg.silence_exp_min,
                self.cfg.silence_exp_max,
            )
        upd_mul(
            "ambient_dissipation",
            0.5 * overfire - 0.8 * collapse_drive,
            self.cfg.ambient_dissipation_min,
            self.cfg.ambient_dissipation_max,
        )
        upd_mul(
            "spawn_cost",
            -0.8 * imbalance - 0.4 * collapse_drive,
            self.cfg.spawn_cost_min,
            self.cfg.spawn_cost_max,
        )

        mint_delta = float(self.laws["mint_delta"]) + lr * (
            -0.25 * collapse_drive - 0.15 * imbalance
        )
        self.laws["mint_delta"] = float(
            np.clip(mint_delta, self.cfg.mint_delta_min, self.cfg.mint_delta_max)
        )

        pareto_alpha = float(self.laws["pareto_alpha"]) - lr * (
            imbalance + 0.4 * err_active + 0.25 * err_match
        )
        self.laws["pareto_alpha"] = float(
            np.clip(pareto_alpha, self.cfg.pareto_alpha_min, self.cfg.pareto_alpha_max)
        )

        season_note, seasonal_births = self._apply_seasonal_forcing()
        law_text = (
            "laws "
            f"attempt={self.laws['attempt_cost_base']:.3f} "
            f"jackpot={self.laws['jackpot_base']:.3f} "
            f"sil_log={self.laws['silence_log_coeff']:.4f} "
            f"sil_exp={self.laws['silence_exp_coeff']:.4f} "
            f"diss={self.laws['ambient_dissipation']:.4f} "
            f"spawn={self.laws['spawn_cost']:.3f} "
            f"mint_delta={self.laws['mint_delta']:.3f} "
            f"pareto_a={self.laws['pareto_alpha']:.3f}"
        )
        if season_note:
            law_text = f"{law_text} | {season_note}"
        return law_text, seasonal_births

    def _seasonal_energy_boost(self, count: int, renewal: float) -> int:
        if count <= 0:
            return 0
        live_slots = self.gap.live_capsule_slots(min_energy=0.0)
        if live_slots.size <= 0:
            return 0
        local = set(int(x) for x in self._frontier_local_slots().tolist())
        candidates = [int(s) for s in live_slots.tolist() if int(s) in local]
        if not candidates:
            candidates = [int(s) for s in live_slots.tolist()]
        ranked = sorted(
            candidates,
            key=lambda s: (
                float(self.gap.energy[s]),
                int(self.gap.capsule_inactivity_steps[s]),
                int(self.gap.step_idx[s]),
            ),
        )
        boosts = 0
        for slot in ranked:
            if boosts >= int(count):
                break
            paid = self._reservoir_take(float(self.cfg.season_revival_energy))
            if paid <= 1e-8:
                break
            self.gap.energy[slot] = float(
                np.clip(
                    float(self.gap.energy[slot]) + paid, 0.0, self.cfg.token_energy_cap
                )
            )
            jitter_scale = float(self.cfg.season_topology_jitter) * (
                0.3 + 0.7 * float(renewal)
            )
            if jitter_scale > 0.0:
                self.gap.velocity[slot] = np.tanh(
                    np.asarray(self.gap.velocity[slot], dtype=np.float32)
                    + self.rng.normal(
                        0.0, jitter_scale, size=(self.cfg.gap_dim,)
                    ).astype(np.float32)
                ).astype(np.float32)
                self.gap.points[slot] = np.tanh(
                    np.asarray(self.gap.points[slot], dtype=np.float32)
                    + 0.5 * np.asarray(self.gap.velocity[slot], dtype=np.float32)
                ).astype(np.float32)
            boosts += 1
        return int(boosts)

    def _apply_seasonal_forcing(self) -> tuple[str, int]:
        if not self.cfg.seasons_enabled:
            return "", 0

        phase = (
            2.0
            * np.pi
            * (
                float(self.step_idx % self.cfg.season_period)
                / float(self.cfg.season_period)
            )
        )
        raw_wave = float(np.sin(phase))
        season_decay = float(np.clip(self.cfg.season_wave_decay, 0.0, 0.9999))
        self.season_wave_ema = (
            season_decay * self.season_wave_ema + (1.0 - season_decay) * raw_wave
        )
        wave = float(np.clip(self.season_wave_ema, -1.0, 1.0))
        renewal = max(wave, 0.0)
        austerity = max(-wave, 0.0)
        s = float(self.cfg.season_strength)

        def upd_mul(key: str, drive: float, lo: float, hi: float) -> None:
            val = float(self.laws[key]) * float(np.exp(s * drive))
            self.laws[key] = float(np.clip(val, lo, hi))

        upd_mul(
            "attempt_cost_base",
            0.50 * austerity - 0.50 * renewal,
            self.cfg.attempt_cost_min,
            self.cfg.attempt_cost_max,
        )
        upd_mul(
            "jackpot_base",
            0.60 * renewal - 0.20 * austerity,
            self.cfg.jackpot_base_min,
            self.cfg.jackpot_base_max,
        )
        upd_mul(
            "ambient_dissipation",
            0.55 * austerity - 0.35 * renewal,
            self.cfg.ambient_dissipation_min,
            self.cfg.ambient_dissipation_max,
        )
        upd_mul(
            "spawn_cost",
            0.65 * austerity - 0.85 * renewal,
            self.cfg.spawn_cost_min,
            self.cfg.spawn_cost_max,
        )

        mint_delta = float(self.laws["mint_delta"]) + s * (
            0.030 * austerity - 0.040 * renewal
        )
        self.laws["mint_delta"] = float(
            np.clip(mint_delta, self.cfg.mint_delta_min, self.cfg.mint_delta_max)
        )

        boosts = 0
        if renewal > 0.20 and self.cfg.season_revival_spores > 0:
            wave_scale = 0.4 + 1.6 * renewal
            quota = int(
                max(1, np.ceil(float(self.cfg.season_revival_spores) * wave_scale))
            )
            boosts = self._seasonal_energy_boost(quota, renewal)

        if boosts > 0:
            return f"season wave={wave:+.2f} renewal_boosts={boosts}", 0
        return f"season wave={wave:+.2f}", 0

    def _decode_from_state(
        self,
        state: np.ndarray,
        drift: np.ndarray,
        length_bias: float,
        resonance_strength: float,
    ) -> tuple[np.ndarray, float, float]:
        x = float(length_bias) + 2.2 * float(resonance_strength)
        frac = 1.0 / (1.0 + np.exp(-x))
        length = 1 + int(np.floor(frac * float(self.cfg.proposal_lmax - 1)))
        length = int(np.clip(length, 1, self.cfg.proposal_lmax))
        ids = np.zeros((length,), dtype=np.int32)
        confs = np.zeros((length,), dtype=np.float32)
        for k in range(length):
            query = _normalize(state + float(k) * drift)
            scores = self.decode_latent @ query
            top2 = np.argpartition(scores, -2)[-2:]
            if scores[top2[0]] >= scores[top2[1]]:
                i1, i2 = int(top2[0]), int(top2[1])
            else:
                i1, i2 = int(top2[1]), int(top2[0])
            margin = float(scores[i1] - scores[i2])
            conf = 1.0 / (1.0 + np.exp(-self.cfg.confidence_scale * margin))
            ids[k] = int(self.decode_token_ids[i1])
            confs[k] = float(conf)
        confidence = float(np.mean(confs))
        quality = float(
            confidence * (0.5 + 0.5 * np.clip(resonance_strength, 0.0, 1.0))
        )
        return ids, confidence, quality

    def _raw_decode_for_token(
        self, token: SuperToken, resonance_strength: float
    ) -> tuple[np.ndarray, float, float]:
        return self._decode_from_identity(token.identity_bytes, resonance_strength)

    def _proposal_for_token(
        self, token: SuperToken, resonance_strength: float
    ) -> Optional[Proposal]:
        ids, conf, quality = self._raw_decode_for_token(token, resonance_strength)
        if ids.size <= 0:
            return None
        bet = self._proposal_bet(token, conf)
        if bet <= 0.0:
            return None
        rarity = self.world.rarity_of_sequence(ids.tolist())
        jack_est = self._jackpot(
            match_len=len(ids),
            proposal_len=len(ids),
            quality=quality,
            rarity=rarity,
            inactivity_steps=token.inactivity_steps,
        )
        exp_ret = conf * jack_est - bet
        if self.cfg.obligatory_proposals:
            return Proposal(
                token_id=token.token_id,
                tokens=ids,
                confidence=conf,
                quality=quality,
                expected_return=float(exp_ret),
                bet=float(bet),
            )
        if conf < token.activation_threshold:
            return None
        pressure_deficit = max(
            float(self.cfg.target_proposal_pressure)
            - float(self.law_ema["proposal_pressure"]),
            0.0,
        )
        explore_prob = float(np.clip(0.05 + 0.45 * pressure_deficit, 0.05, 0.55))
        if exp_ret <= 0.0 and self.rng.random() > explore_prob:
            return None
        return Proposal(
            token_id=token.token_id,
            tokens=ids,
            confidence=conf,
            quality=quality,
            expected_return=float(exp_ret),
            bet=float(bet),
        )

    def _proposal_bet(self, token: SuperToken, confidence: float) -> float:
        unit = max(self._attempt_cost(token), 0.0)
        energy = max(float(token.energy), 0.0)
        cap = float(self.cfg.proposal_bet_max_energy_frac) * energy
        if cap <= 0.0:
            return 0.0
        x = float(self.cfg.proposal_bet_conf_gain) * (
            float(confidence) - float(token.activation_threshold)
        )
        risk = 1.0 / (1.0 + np.exp(-np.clip(x, -40.0, 40.0)))
        floor = float(np.clip(self.cfg.proposal_bet_floor_frac, 0.0, 1.0))
        target = (
            unit
            * float(self.cfg.proposal_bet_unit_scale)
            * (floor + (1.0 - floor) * float(risk))
        )
        target = max(target, float(self.cfg.proposal_min_bet))
        return float(min(target, cap))

    def _gate_proposals_to_frontier(
        self, proposals: list[Proposal]
    ) -> tuple[list[Proposal], int, int]:
        if not proposals:
            return [], 0, 0

        frontier_tid = int(self.world.peek(1)[0])
        frontier_props: list[Proposal] = []
        other_props: list[Proposal] = []
        for p in proposals:
            if p.tokens.size > 0 and int(p.tokens[0]) == frontier_tid:
                frontier_props.append(p)
            else:
                other_props.append(p)

        if frontier_props:
            n = min(len(other_props), int(self.cfg.proposal_frontier_contrast))
            if n > 0:
                idx = np.asarray(
                    self.rng.choice(len(other_props), size=n, replace=False),
                    dtype=np.int64,
                )
                sampled = [other_props[int(i)] for i in idx.tolist()]
            else:
                sampled = []
            gated = frontier_props + sampled
            return gated, len(frontier_props), len(gated)

        n_fallback = min(len(other_props), int(self.cfg.proposal_frontier_fallback))
        if n_fallback > 0:
            ranked = sorted(
                other_props,
                key=lambda p: (
                    float(p.confidence),
                    float(p.expected_return),
                    int(p.token_id),
                ),
                reverse=True,
            )
            gated = ranked[:n_fallback]
            return gated, 0, len(gated)

        return [], 0, 0

    def _spawn_frontier_specialist(self, frontier_tid: int) -> Optional[SuperToken]:
        e = self._reservoir_take(float(self.cfg.frontier_rescue_energy))
        if e <= 1e-8:
            return None

        tid = self.next_token_id
        l = 2
        ident = self.world.peek(l).astype(np.int32)
        if self.cfg.token_space == "byte_parity":
            ident = (ident & 0xFF).astype(np.int32)
        token = make_initial_super_token(
            self.cfg,
            self.rng,
            tid,
            birth_step=self.step_idx,
            identity_bytes=ident,
        )
        target = self.base_latent[
            int(np.clip(frontier_tid, 0, self.base_latent.shape[0] - 1))
        ].astype(np.float32)
        noise = self.rng.normal(
            0.0, float(self.cfg.frontier_rescue_noise), size=target.shape
        ).astype(np.float32)

        token.state_vec = _normalize((target + noise).astype(np.float32)).astype(
            np.float32
        )
        token.signature = _normalize(token.state_vec[: self.cfg.gap_dim]).astype(
            np.float32
        )
        token.phase = float(np.arctan2(token.state_vec[1], token.state_vec[0]))
        token.proposal_drift = _normalize(
            0.70 * token.proposal_drift + 0.30 * target
        ).astype(np.float32)
        token.activation_threshold = float(min(token.activation_threshold, 0.35))
        token.proposal_length_bias = float(min(token.proposal_length_bias, -0.80))
        token.energy = float(min(e, self.cfg.token_energy_cap))

        self._register_lineage_depth(token)
        slot = int(self.gap.ptr)
        self.gap.write(
            point=np.asarray(token.signature, dtype=np.float32),
            velocity=np.zeros((self.cfg.gap_dim,), dtype=np.float32),
            phase=float(token.phase),
            omega=float(token.omega),
            energy=float(token.energy),
            genome_fragment=token.identity_bytes,
            genome_weight=1.0,
            ifs_fragment=token.ifs,
            ifs_weight=1.0,
            emitter_id=int(token.token_id),
            step_idx=self.step_idx,
            round_idx=-1,
            capsule_token=token,
            lineage_depth=int(self.lineage_depth.get(int(token.token_id), 0)),
        )
        self.super_tokens[tid] = token
        self.token_anchor_slot[int(tid)] = int(slot)
        self.next_token_id += 1
        return token

    def _proposal_for_pair_fusion(
        self, a: SuperToken, b: SuperToken
    ) -> tuple[np.ndarray, float, float]:
        rs = 0.5 * (
            self.last_resonance_strength.get(a.token_id, 0.0)
            + self.last_resonance_strength.get(b.token_id, 0.0)
        )
        ia = np.asarray(a.identity_bytes, dtype=np.int32).reshape(-1)
        ib = np.asarray(b.identity_bytes, dtype=np.int32).reshape(-1)
        if ia.size == 0:
            ia = np.asarray([32], dtype=np.int32)
        if ib.size == 0:
            ib = np.asarray([32], dtype=np.int32)
        na = max(1, ia.size // 2)
        nb = max(1, ib.size - ib.size // 2)
        fused = np.concatenate([ia[:na], ib[-nb:]], axis=0)[
            : max(1, self.cfg.proposal_lmax)
        ]
        ids_f, conf_f, q_f = self._decode_from_identity(fused, rs)
        ids_a, conf_a, q_a = self._decode_from_identity(
            ia, self.last_resonance_strength.get(a.token_id, 0.0)
        )
        ids_b, conf_b, q_b = self._decode_from_identity(
            ib, self.last_resonance_strength.get(b.token_id, 0.0)
        )

        candidates = [(ids_f, conf_f, q_f), (ids_a, conf_a, q_a), (ids_b, conf_b, q_b)]
        ids, conf, quality = max(
            candidates, key=lambda x: x[1] if x[0].size > 0 else -1.0
        )

        align = float(
            np.clip(np.dot(_normalize(a.state_vec), _normalize(b.state_vec)), 0.0, 1.0)
        )
        conf = float(np.clip(conf + 0.18 * align, 0.0, 1.0))
        quality = float(np.clip(quality + 0.22 * align, 0.0, 1.5))
        return ids, conf, quality

    def _evaluate_proposal_return(
        self,
        proposal_tokens: np.ndarray,
        confidence: float,
        quality: float,
        inactivity_steps: int,
        attempt_total: float,
    ) -> float:
        p_len = int(proposal_tokens.size)
        if p_len <= 0:
            d = discovery_cost(1, self.cfg)
            b = base_toll(1, self.cfg)
            return realized_return(0.0, attempt_total, d, b)

        m = self.world.match_prefix_len(proposal_tokens.tolist())
        adv = max(1, m)
        unmatched = 0 if m == p_len else max(1, p_len - m)
        d = 0.0 if m == p_len else discovery_cost(unmatched, self.cfg)
        rarity = self.world.rarity_of_sequence(proposal_tokens[: max(m, 1)].tolist())
        q = float(confidence * (float(m) / max(float(p_len), 1.0)))
        j = self._jackpot(
            match_len=m,
            proposal_len=p_len,
            quality=q,
            rarity=rarity,
            inactivity_steps=inactivity_steps,
        )
        b = base_toll(adv, self.cfg)
        return realized_return(j, attempt_total, d, b)

    def _proposal_realized_components(
        self, proposal: Proposal
    ) -> dict[str, float | int]:
        p_len = int(proposal.tokens.size)
        if p_len <= 0:
            adv = 1
            disc = discovery_cost(1, self.cfg)
            j = 0.0
            score = realized_return(
                j, float(proposal.bet), disc, base_toll(adv, self.cfg)
            )
            return {
                "match_len": 0,
                "proposal_len": 0,
                "advance_len": adv,
                "discovery_cost": float(disc),
                "jackpot": float(j),
                "score": float(score),
            }

        m = self.world.match_prefix_len(proposal.tokens.tolist())
        adv = max(1, m)
        full = m == p_len
        unmatched = 0 if full else max(1, p_len - m)
        disc = 0.0 if full else discovery_cost(unmatched, self.cfg)
        rarity = self.world.rarity_of_sequence(proposal.tokens[: max(m, 1)].tolist())
        q = float(proposal.quality * (float(m) / max(float(p_len), 1.0)))
        tok = self.super_tokens.get(proposal.token_id)
        inactivity = tok.inactivity_steps if tok is not None else 0
        j = self._jackpot(
            match_len=m,
            proposal_len=p_len,
            quality=q,
            rarity=rarity,
            inactivity_steps=inactivity,
        )
        score = realized_return(j, float(proposal.bet), disc, base_toll(adv, self.cfg))
        return {
            "match_len": int(m),
            "proposal_len": int(p_len),
            "advance_len": int(adv),
            "discovery_cost": float(disc),
            "jackpot": float(j),
            "score": float(score),
        }

    def _ablation_returns(
        self, a: SuperToken, b: SuperToken
    ) -> tuple[float, float, float]:
        ids_a, conf_a, q_a = self._raw_decode_for_token(
            a, self.last_resonance_strength.get(a.token_id, 0.0)
        )
        ids_b, conf_b, q_b = self._raw_decode_for_token(
            b, self.last_resonance_strength.get(b.token_id, 0.0)
        )
        ids_ab, conf_ab, q_ab = self._proposal_for_pair_fusion(a, b)
        r_a = self._evaluate_proposal_return(
            proposal_tokens=ids_a,
            confidence=conf_a,
            quality=q_a,
            inactivity_steps=a.inactivity_steps,
            attempt_total=self._attempt_cost(a),
        )
        r_b = self._evaluate_proposal_return(
            proposal_tokens=ids_b,
            confidence=conf_b,
            quality=q_b,
            inactivity_steps=b.inactivity_steps,
            attempt_total=self._attempt_cost(b),
        )
        r_ab = self._evaluate_proposal_return(
            proposal_tokens=ids_ab,
            confidence=conf_ab,
            quality=q_ab,
            inactivity_steps=max(a.inactivity_steps, b.inactivity_steps),
            attempt_total=max(self._attempt_cost(a), self._attempt_cost(b)),
        )
        return r_ab, r_a, r_b

    def _adapt_winner_from_truth(
        self, winner: SuperToken, gt_tokens: np.ndarray, match_len: int
    ) -> None:
        if gt_tokens.size == 0:
            return
        if match_len > 0:
            target = _normalize(
                np.mean(self.base_latent[gt_tokens[:match_len]], axis=0)
            )
            winner.state_vec = _normalize(
                (1.0 - 0.06) * winner.state_vec + 0.06 * target
            ).astype(np.float32)
        if match_len < gt_tokens.size:
            miss = self.base_latent[int(gt_tokens[match_len])]
            winner.state_vec = _normalize(
                (1.0 - 0.18) * winner.state_vec + 0.18 * miss
            ).astype(np.float32)
            winner.proposal_drift = _normalize(
                0.92 * winner.proposal_drift + 0.08 * (miss - winner.state_vec)
            ).astype(np.float32)
        winner.signature = _normalize(
            0.95 * winner.signature + 0.05 * winner.state_vec[: self.cfg.gap_dim]
        ).astype(np.float32)
        winner.phase = float(np.arctan2(winner.state_vec[1], winner.state_vec[0]))

    def _apply_jackpot(self, winner: SuperToken, jackpot: float) -> None:
        if jackpot <= 0.0:
            return
        payout = self._reservoir_take(jackpot)
        if payout <= 0.0:
            return
        a = winner.parent_a
        b = winner.parent_b
        parent_a = self.super_tokens.get(a)
        parent_b = self.super_tokens.get(b)
        if parent_a is None and parent_b is None:
            winner.energy += payout
            return

        paid_to_parents = 0.0
        parent_share = float(self.cfg.parent_reward_share) * payout
        if parent_a is not None:
            parent_a.energy += parent_share
            paid_to_parents += parent_share
        if parent_b is not None and parent_b is not parent_a:
            parent_b.energy += parent_share
            paid_to_parents += parent_share

        # Preserve full payout accounting: winner gets child share plus any
        # unclaimed remainder (dead/missing parent or configured residual).
        winner_base = float(self.cfg.child_reward_share) * payout
        winner.energy += max(0.0, payout - paid_to_parents - winner_base) + winner_base

    def _self_copy_cycle(
        self, proposal_evals: list[tuple[Proposal, dict[str, float | int]]]
    ) -> tuple[int, list[str]]:
        cfg = self.cfg
        if not bool(cfg.self_copy_enabled):
            return 0, []
        if int(cfg.self_copy_max_per_step) <= 0:
            return 0, []
        if int(self.step_idx % int(cfg.self_copy_interval)) != 0:
            return 0, []
        if not proposal_evals:
            return 0, []

        copy_cost = float(cfg.self_copy_cost)
        parent_contrib_frac = float(np.clip(cfg.self_copy_parent_contrib_frac, 0.0, 1.0))
        parent_contrib = copy_cost * parent_contrib_frac
        min_energy = float(cfg.self_copy_min_energy)
        min_match_frac = float(cfg.self_copy_min_match_frac)
        min_score = float(cfg.self_copy_min_score)

        births = 0
        events: list[str] = []
        ranked = sorted(
            proposal_evals,
            key=lambda pe: (
                float(pe[1].get("score", 0.0)),
                int(pe[1].get("match_len", 0)),
                float(pe[0].confidence),
            ),
            reverse=True,
        )

        for proposal, evals in ranked:
            if births >= int(cfg.self_copy_max_per_step):
                break
            parent = self.super_tokens.get(int(proposal.token_id))
            if parent is None:
                continue

            plen = max(int(evals.get("proposal_len", 0)), 1)
            mlen = max(int(evals.get("match_len", 0)), 0)
            match_frac = float(mlen) / float(plen)
            score = float(evals.get("score", 0.0))
            if match_frac < min_match_frac or score < min_score:
                continue
            if float(parent.energy) < (min_energy + parent_contrib):
                continue

            if parent_contrib > 0.0:
                paid = self._drain_token_energy(parent, parent_contrib)
            else:
                paid = 0.0
            if parent_contrib > 0.0 and paid <= 1e-8:
                continue
            child_energy = self._reservoir_take(copy_cost)
            if child_energy <= 1e-8:
                continue

            gap_ident = self._gap_identity_override({int(parent.token_id)})
            gap_ifs = self._gap_ifs_override({int(parent.token_id)})
            child = mint_child_from_parent(
                cfg=cfg,
                rng=self.rng,
                new_id=self.next_token_id,
                parent=parent,
                spawn_energy=child_energy,
                pareto_alpha=float(self.laws["pareto_alpha"]),
                identity_override=gap_ident,
                ifs_override=gap_ifs,
                birth_step=self.step_idx,
            )
            self._register_lineage_depth(child)
            child_slot = int(self.gap.ptr)
            self.gap.write(
                point=np.asarray(child.signature, dtype=np.float32),
                velocity=np.zeros((self.cfg.gap_dim,), dtype=np.float32),
                phase=float(child.phase),
                omega=float(child.omega),
                energy=float(np.clip(child.energy, 0.0, self.cfg.token_energy_cap)),
                genome_fragment=child.identity_bytes,
                genome_weight=1.0,
                ifs_fragment=child.ifs,
                ifs_weight=1.0,
                emitter_id=int(child.token_id),
                step_idx=self.step_idx,
                round_idx=-1,
                capsule_token=child,
                lineage_depth=int(self.lineage_depth.get(int(child.token_id), 0)),
            )
            self.super_tokens[child.token_id] = child
            self.token_anchor_slot[int(child.token_id)] = int(child_slot)
            self.next_token_id += 1
            births += 1
            events.append(
                f"copy {child.token_id} <- {parent.token_id} score={score:.3f} match={mlen}/{plen} "
                f"parent_pay={paid:.3f}/{copy_cost:.3f} "
                f"gap_genome={1 if gap_ident is not None else 0} glen={int(gap_ident.size) if gap_ident is not None else 0} "
                f"gap_ifs={1 if gap_ifs is not None else 0}"
            )

        return births, events

    def _mint_cycle(self) -> tuple[int, list[str]]:
        births = 0
        events: list[str] = []
        spawn_cost = float(self.laws["spawn_cost"])
        mint_delta = float(self.laws["mint_delta"])
        parent_contrib_frac = float(np.clip(self.cfg.mint_parent_contrib_frac, 0.0, 1.0))
        half_spawn = 0.5 * spawn_cost * parent_contrib_frac
        local_slots = self._frontier_local_slots()
        if local_slots.size <= 0:
            return births, events

        local_mass: dict[int, float] = {}
        for slot in local_slots.tolist():
            s = int(slot)
            tid = int(self.gap.capsule_token_id[s])
            if tid < 0:
                continue
            tok = self.super_tokens.get(tid)
            if tok is None:
                continue
            mass = float(np.clip(self.gap.energy[s], 0.0, self.cfg.token_energy_cap))
            mass *= 0.25 + 0.75 * float(np.clip(self.gap.genome_weight[s], 0.0, 1.0))
            local_mass[tid] = local_mass.get(tid, 0.0) + mass
        if len(local_mass) < 2:
            # Fallback to global capsule anchors so minting does not stall when
            # the frontier neighborhood becomes sparse.
            for tid, slot in self.token_anchor_slot.items():
                tok = self.super_tokens.get(int(tid))
                if tok is None:
                    continue
                s = int(slot)
                if s < 0 or s >= int(self.cfg.gap_len):
                    continue
                mass = float(np.clip(self.gap.energy[s], 0.0, self.cfg.token_energy_cap))
                if mass <= 0.0:
                    continue
                mass *= 0.25 + 0.75 * float(np.clip(self.gap.genome_weight[s], 0.0, 1.0))
                local_mass[int(tid)] = local_mass.get(int(tid), 0.0) + mass
        if len(local_mass) < 2:
            return births, events

        ranked = sorted(local_mass.items(), key=lambda kv: kv[1], reverse=True)
        ranked = ranked[: max(int(self.cfg.capsule_mint_parent_pool), 2)]
        candidate_ids = [int(tid) for tid, _ in ranked]

        pair_candidates: list[tuple[float, int, int, float]] = []
        for i in range(len(candidate_ids)):
            for j in range(i + 1, len(candidate_ids)):
                a_id = int(candidate_ids[i])
                b_id = int(candidate_ids[j])
                substrate = float(
                    np.log1p(local_mass.get(a_id, 0.0) + local_mass.get(b_id, 0.0))
                )
                syn = float(max(self.coop.synergy(a_id, b_id), 0.0))
                pair_score = substrate + 0.6 * syn
                pair_candidates.append((pair_score, a_id, b_id, syn))
        pair_candidates.sort(key=lambda x: x[0], reverse=True)
        nonroot_live = sum(
            1
            for tid in self.super_tokens.keys()
            if int(self.lineage_depth.get(int(tid), 0)) > 0
        )
        collapse_mode = bool(nonroot_live <= 0)

        for _, a_id, b_id, syn in pair_candidates:
            if births >= 2:
                break
            a = self.super_tokens.get(a_id)
            b = self.super_tokens.get(b_id)
            if a is None or b is None:
                continue
            if float(a.energy) < half_spawn or float(b.energy) < half_spawn:
                continue
            r_ab, r_a, r_b = self._ablation_returns(a, b)
            substrate_boost = 0.05 * float(
                np.log1p(local_mass.get(a_id, 0.0) + local_mass.get(b_id, 0.0))
            ) + 0.08 * float(syn)
            merit = (r_ab + substrate_boost) - (max(r_a, r_b) + mint_delta)
            exploratory = False
            if merit <= 0.0:
                if not collapse_mode:
                    continue
                # Lineage rescue: when ecology collapses to roots, allow occasional
                # exploratory pair births from the strongest local pairs.
                if births > 0:
                    continue
                excite = float(np.log1p(local_mass.get(a_id, 0.0) + local_mass.get(b_id, 0.0)))
                p_explore = float(np.clip(0.20 + 0.12 * excite + 0.10 * syn, 0.20, 0.55))
                if self.rng.random() > p_explore:
                    continue
                exploratory = True

            if half_spawn > 0.0:
                paid_a = self._drain_token_energy(a, half_spawn)
                paid_b = self._drain_token_energy(b, half_spawn)
            else:
                paid_a = 0.0
                paid_b = 0.0
            child_energy = self._reservoir_take(spawn_cost)
            if child_energy <= 1e-8:
                continue
            gap_ident = self._gap_identity_override({int(a_id), int(b_id)})
            gap_ifs = self._gap_ifs_override({int(a_id), int(b_id)})
            child = mint_child_from_parents(
                cfg=self.cfg,
                rng=self.rng,
                new_id=self.next_token_id,
                parent_a=a,
                parent_b=b,
                spawn_energy=child_energy,
                pareto_alpha=float(self.laws["pareto_alpha"]),
                identity_override=gap_ident,
                ifs_override=gap_ifs,
                birth_step=self.step_idx,
            )
            child_slot = int(self.gap.ptr)
            self._register_lineage_depth(child)
            self.gap.write(
                point=np.asarray(child.signature, dtype=np.float32),
                velocity=np.zeros((self.cfg.gap_dim,), dtype=np.float32),
                phase=float(child.phase),
                omega=float(child.omega),
                energy=float(np.clip(child.energy, 0.0, self.cfg.token_energy_cap)),
                genome_fragment=child.identity_bytes,
                genome_weight=1.0,
                ifs_fragment=child.ifs,
                ifs_weight=1.0,
                emitter_id=int(child.token_id),
                step_idx=self.step_idx,
                round_idx=-1,
                capsule_token=child,
                lineage_depth=int(self.lineage_depth.get(int(child.token_id), 0)),
            )
            self.super_tokens[child.token_id] = child
            self.token_anchor_slot[int(child.token_id)] = int(child_slot)
            self.next_token_id += 1
            births += 1
            events.append(
                f"mint {child.token_id} <- ({a_id},{b_id}) syn={syn:.3f} local={local_mass.get(a_id, 0.0) + local_mass.get(b_id, 0.0):.3f} "
                f"parent_pay={paid_a+paid_b:.3f}/{spawn_cost:.3f} "
                f"explore={1 if exploratory else 0} merit={merit:.3f} "
                f"ab={r_ab:.3f} a={r_a:.3f} b={r_b:.3f} "
                f"gap_genome={1 if gap_ident is not None else 0} glen={int(gap_ident.size) if gap_ident is not None else 0} "
                f"gap_ifs={1 if gap_ifs is not None else 0}"
            )
        return births, events

    def step(self) -> StepResult:
        self.step_idx += 1
        events: list[str] = []
        cfg = self.cfg
        if not self.cfg.conserve_total_energy and cfg.energy_inflow_per_step > 0.0:
            self._reservoir_add(cfg.energy_inflow_per_step)

        # Source-of-truth sync: organisms exist only as gap capsules.
        self._sync_super_tokens_from_gap()

        active_rounds: dict[int, int] = {}
        last_strength: dict[int, float] = {}
        prefix_match_frac: dict[int, float] = {}
        step_chaos_energy = 0.0
        step_chaos_substeps = 0
        step_chaos_organisms = 0
        for tid in self._local_live_token_ids():
            token = self.super_tokens.get(int(tid))
            if token is None or float(token.energy) <= cfg.min_viable_energy:
                continue
            prefix_match_frac[int(tid)] = self._identity_prefix_fraction(
                token.identity_bytes
            )

        # K synchronized rounds; only local frontier-neighborhood capsules can act.
        for round_idx in range(cfg.k_rounds):
            if self.cfg.conserve_total_energy:
                lost = float(
                    np.sum(self.gap.energy) * max(1.0 - self.cfg.gap_decay, 0.0)
                )
                self._reservoir_add(lost)
            self.gap.decay()
            round_ids = [
                int(tid)
                for tid in self._local_live_token_ids()
                if int(tid) in self.super_tokens
                and float(self.super_tokens[int(tid)].energy) > cfg.min_viable_energy
            ]
            if not round_ids:
                continue
            batch_size = max(int(self.cfg.gap_read_batch_size), 1)
            d_gap = int(self.cfg.gap_dim)
            for i0 in range(0, len(round_ids), batch_size):
                chunk_ids = round_ids[i0 : i0 + batch_size]
                live_ids: list[int] = []
                batch_sig = np.zeros((batch_size, d_gap), dtype=np.float32)
                batch_width = np.zeros((batch_size,), dtype=np.float32)
                batch_phase = np.zeros((batch_size,), dtype=np.float32)
                batch_coupling = np.zeros((batch_size,), dtype=np.float32)
                batch_valid = np.zeros((batch_size,), dtype=np.float32)
                for tid in chunk_ids:
                    token = self.super_tokens.get(int(tid))
                    if token is None or token.energy <= cfg.min_viable_energy:
                        continue
                    j = len(live_ids)
                    if j >= batch_size:
                        break
                    live_ids.append(int(tid))
                    batch_sig[j] = np.asarray(token.signature, dtype=np.float32)
                    batch_width[j] = float(token.resonance_width)
                    batch_phase[j] = float(token.phase)
                    batch_coupling[j] = float(token.phase_coupling)
                    batch_valid[j] = 1.0
                if not live_ids:
                    continue

                batch_resonance, batch_strength = self.gap.read_many(
                    receiver_signature=batch_sig,
                    resonance_width=batch_width,
                    receiver_phase=batch_phase,
                    phase_coupling=batch_coupling,
                    valid_mask=batch_valid,
                )

                for j, tid in enumerate(live_ids):
                    token = self.super_tokens.get(int(tid))
                    if token is None or token.energy <= cfg.min_viable_energy:
                        continue
                    read_strength = float(batch_strength[j])
                    resonance_latent = self._gap_to_latent(
                        np.asarray(batch_resonance[j], dtype=np.float32)
                    )
                    # Energy-regulated chaos iterations: min steps are free,
                    # additional steps cost chaos_substep_cost each.
                    min_sub = max(int(cfg.chaos_min_substeps), 1)
                    max_sub = max(int(cfg.chaos_max_substeps), min_sub)
                    cost_per = float(cfg.chaos_substep_cost)
                    if cost_per > 0.0 and max_sub > min_sub:
                        spare = max(
                            float(token.energy) - float(cfg.min_viable_energy), 0.0
                        )
                        affordable = int(spare / cost_per)
                        substeps = min(min_sub + affordable, max_sub)
                    else:
                        substeps = max_sub
                    chaos_paid = 0.0
                    if cost_per > 0.0 and substeps > min_sub:
                        chaos_paid = float(substeps - min_sub) * cost_per
                        self._drain_token_energy(token, chaos_paid)
                    step_chaos_energy += chaos_paid
                    step_chaos_substeps += substeps
                    step_chaos_organisms += 1
                    vel_sum = np.zeros((2,), dtype=np.float32)
                    vel2 = vel_sum
                    for _ in range(substeps):
                        vel2 = token.chaos_step(self.rng, resonance_latent)
                        vel_sum += vel2.astype(np.float32)
                    vel2 = (vel_sum / float(substeps)).astype(np.float32)
                    last_strength[tid] = read_strength

                    prefix_score = float(prefix_match_frac.get(tid, 0.0))
                    wake_score = 0.85 * prefix_score + 0.15 * read_strength
                    activate = wake_score >= token.activation_threshold
                    anchor_slot = int(self.token_anchor_slot.get(int(tid), -1))
                    if not activate:
                        if anchor_slot >= 0:
                            self._refresh_capsule_slot(
                                token, anchor_slot, read_strength=read_strength
                            )
                        continue
                    emit, vel, eng = token.emission(cfg, vel2)
                    if eng <= cfg.min_viable_energy:
                        if anchor_slot >= 0:
                            self._refresh_capsule_slot(
                                token, anchor_slot, read_strength=read_strength
                            )
                        continue
                    eng_paid = self._drain_token_to_gap(token, eng)
                    if eng_paid <= cfg.min_viable_energy:
                        if self.cfg.conserve_total_energy and eng_paid > 0.0:
                            self._reservoir_add(eng_paid)
                        if anchor_slot >= 0:
                            self._refresh_capsule_slot(
                                token, anchor_slot, read_strength=read_strength
                            )
                        continue
                    if self.cfg.conserve_total_energy:
                        overwritten_slot = int(self.gap.ptr)
                        overwritten_tid = int(
                            self.gap.capsule_token_id[overwritten_slot]
                        )
                        overwritten = float(self.gap.energy[overwritten_slot])
                        if overwritten > 0.0 and overwritten_tid != int(token.token_id):
                            self._reservoir_add(overwritten)
                    self._write_capsule_emission(
                        token=token,
                        point=emit,
                        velocity=vel,
                        energy=float(token.energy),
                        read_strength=read_strength,
                        round_idx=round_idx,
                    )
                    active_rounds[tid] = active_rounds.get(tid, 0) + 1

        self.last_resonance_strength = last_strength

        local_live_ids = self._local_live_token_ids()
        active_ids = [
            tid
            for tid, cnt in active_rounds.items()
            if cnt > 0 and tid in self.super_tokens
        ]
        if self.cfg.obligatory_proposals:
            frontier_ids: list[int] = []
            non_frontier_ids: list[int] = []
            for tid in local_live_ids:
                token = self.super_tokens.get(int(tid))
                if token is None or float(token.energy) <= cfg.min_viable_energy:
                    continue
                if self._identity_contains_frontier_byte(token.identity_bytes):
                    frontier_ids.append(int(tid))
                else:
                    non_frontier_ids.append(int(tid))

            if frontier_ids:
                n = min(len(non_frontier_ids), int(cfg.proposal_frontier_contrast))
                if n > 0:
                    ridx = np.asarray(
                        self.rng.choice(len(non_frontier_ids), size=n, replace=False),
                        dtype=np.int64,
                    )
                    contrast = [non_frontier_ids[int(i)] for i in ridx.tolist()]
                else:
                    contrast = []
                proposal_ids = frontier_ids + contrast
            else:
                n = min(len(non_frontier_ids), int(cfg.proposal_frontier_fallback))
                if n > 0 and len(non_frontier_ids) > n:
                    ridx = np.asarray(
                        self.rng.choice(len(non_frontier_ids), size=n, replace=False),
                        dtype=np.int64,
                    )
                    proposal_ids = [non_frontier_ids[int(i)] for i in ridx.tolist()]
                elif n > 0:
                    proposal_ids = list(non_frontier_ids)
                else:
                    proposal_ids = []
        else:
            active_set = set(int(x) for x in active_ids)
            proposal_ids = [
                int(tid) for tid in local_live_ids if int(tid) in active_set
            ]
            if not proposal_ids:
                proposal_ids = [int(tid) for tid in local_live_ids]

        proposals: list[Proposal] = []
        for tid in proposal_ids:
            token = self.super_tokens.get(int(tid))
            if token is None:
                continue
            prop = self._proposal_for_token(token, last_strength.get(tid, 0.0))
            if prop is not None:
                proposals.append(prop)
        proposals_raw_count = len(proposals)
        proposals, frontier_match_count, proposals_gated_count = (
            self._gate_proposals_to_frontier(proposals)
        )
        frontier_rescue_spawned = 0
        frontier_rescue_ids: set[int] = set()

        proposal_evals: list[tuple[Proposal, dict[str, float | int]]] = []
        for p in proposals:
            e = self._proposal_realized_components(p)
            proposal_evals.append((p, e))

        if int(cfg.frontier_rescue_max_per_step) > 0:
            best_match = max(
                (int(e["match_len"]) for _, e in proposal_evals), default=0
            )
            if best_match <= 1:
                frontier_tid = int(self.world.peek(1)[0])
                for _ in range(int(cfg.frontier_rescue_max_per_step)):
                    rescue_tok = self._spawn_frontier_specialist(frontier_tid)
                    if rescue_tok is None:
                        break
                    frontier_rescue_spawned += 1
                    frontier_rescue_ids.add(int(rescue_tok.token_id))
                    rescue_prop = self._proposal_for_token(rescue_tok, 0.0)
                    if rescue_prop is None:
                        continue
                    proposals.append(rescue_prop)
                    rescue_eval = self._proposal_realized_components(rescue_prop)
                    proposal_evals.append((rescue_prop, rescue_eval))
                    if int(rescue_eval["match_len"]) > 0:
                        break

        proposer_ids = [int(p.token_id) for p in proposals]

        winner: Optional[Proposal] = None
        winner_eval: Optional[dict[str, float | int]] = None
        winner_rank: tuple[float, float, float, float] | None = None
        if proposal_evals:
            for p, e in proposal_evals:
                if int(p.token_id) in frontier_rescue_ids:
                    continue
                m = int(e["match_len"])
                plen = max(int(e["proposal_len"]), 1)
                rank = (
                    float(m),
                    float(m) / float(plen),
                    float(p.confidence),
                    float(e["score"]),
                )
                if winner is None or winner_eval is None or winner_rank is None:
                    winner = p
                    winner_eval = e
                    winner_rank = rank
                    continue
                if rank > winner_rank:
                    winner = p
                    winner_eval = e
                    winner_rank = rank

        longest_silent_turn_id = -1
        longest_silent_turn_steps = 0
        longest_silent_turn_conf = 0.0
        longest_silent_turn_match_len = 0
        longest_silent_turn_proposal_len = 0
        longest_silent_turn_score = 0.0
        longest_silent_turn_won = 0
        longest_silent_turn_token_len = 0
        longest_silent_turn_parent_a = -1
        longest_silent_turn_parent_b = -1
        longest_silent_turn_identity_bytes: list[int] = []
        if proposal_evals:

            def _silence_for(prop: Proposal) -> int:
                tok = self.super_tokens.get(int(prop.token_id))
                if tok is None:
                    return -1
                return int(tok.inactivity_steps)

            best_prop, best_eval = max(
                proposal_evals,
                key=lambda pe: (
                    _silence_for(pe[0]),
                    float(pe[0].confidence),
                    int(pe[0].token_id),
                ),
            )
            best_tok = self.super_tokens.get(int(best_prop.token_id))
            longest_silent_turn_id = int(best_prop.token_id)
            longest_silent_turn_steps = max(_silence_for(best_prop), 0)
            longest_silent_turn_conf = float(best_prop.confidence)
            longest_silent_turn_match_len = int(best_eval["match_len"])
            longest_silent_turn_proposal_len = int(best_eval["proposal_len"])
            longest_silent_turn_score = float(best_eval["score"])
            longest_silent_turn_won = int(
                winner is not None and int(winner.token_id) == int(best_prop.token_id)
            )
            if best_tok is not None:
                ib = np.asarray(best_tok.identity_bytes, dtype=np.int32).reshape(-1)
                longest_silent_turn_token_len = int(ib.size)
                longest_silent_turn_parent_a = int(best_tok.parent_a)
                longest_silent_turn_parent_b = int(best_tok.parent_b)
                longest_silent_turn_identity_bytes = [int(x) for x in ib.tolist()]

        attempt_map: Dict[int, float] = {
            int(p.token_id): max(float(p.bet), 0.0) for p in proposals
        }
        attempt_total = float(sum(attempt_map.values()))

        if winner is None:
            match_len = 0
            advance_len = 1
            unmatched_len = 1
            disc_cost = discovery_cost(unmatched_len, cfg)
            jackpot = 0.0
            winner_id = None
            winner_from_frontier_rescue = 0
            winner_conf = 0.0
            proposal_len = 0
            winner_super_token_len = 0
            winner_inactivity_before = 0
        else:
            assert winner_eval is not None
            proposal_len = int(winner_eval["proposal_len"])
            gt_tokens = self.world.peek(proposal_len)
            match_len = int(winner_eval["match_len"])
            advance_len = int(winner_eval["advance_len"])
            disc_cost = float(winner_eval["discovery_cost"])
            jackpot = float(winner_eval["jackpot"])
            winner_id = winner.token_id
            winner_from_frontier_rescue = int(
                int(winner.token_id) in frontier_rescue_ids
            )
            winner_conf = winner.confidence
            winner_token_ref = self.super_tokens.get(winner.token_id)
            if winner_token_ref is not None:
                winner_super_token_len = int(
                    np.asarray(winner_token_ref.identity_bytes, dtype=np.int32).size
                )
                winner_inactivity_before = int(winner_token_ref.inactivity_steps)
            else:
                winner_super_token_len = 0
                winner_inactivity_before = 0

        base = base_toll(advance_len, cfg)

        for token in self.super_tokens.values():
            diss_rate = float(self.laws["ambient_dissipation"])
            self._drain_token_energy(token, max(token.energy, 0.0) * diss_rate)

        actual_metabolic_tax = 0.0
        if cfg.metabolic_tax_rate > 0.0:
            for token in self.super_tokens.values():
                actual_metabolic_tax += self._drain_token_energy(
                    token,
                    max(token.energy, 0.0) * float(cfg.metabolic_tax_rate),
                )

        actual_attempt_total = 0.0
        for tid, c in attempt_map.items():
            token = self.super_tokens.get(tid)
            if token is not None:
                actual_attempt_total += self._drain_token_energy(token, c)

        actual_contrastive_bonus = 0.0
        actual_contrastive_penalty = 0.0
        if bool(self.cfg.contrastive_enabled):
            for p, e in proposal_evals:
                token = self.super_tokens.get(int(p.token_id))
                if token is None:
                    continue
                plen = max(int(e["proposal_len"]), 1)
                mlen = int(e["match_len"])
                acc = float(np.clip(float(mlen) / float(plen), 0.0, 1.0))
                conf = float(np.clip(p.confidence, 0.0, 1.0))
                bonus_req = float(self.cfg.contrastive_correct_reward) * conf * acc
                wrong = float(1.0 - acc)
                penalty_req = (
                    float(self.cfg.contrastive_wrong_penalty)
                    * (conf ** float(self.cfg.contrastive_wrong_exp))
                    * wrong
                )

                if penalty_req > 0.0:
                    actual_contrastive_penalty += self._drain_token_energy(
                        token, penalty_req
                    )
                if bonus_req > 0.0:
                    actual_contrastive_bonus += self._credit_token_energy(
                        token, bonus_req
                    )

        actual_discovery_cost = 0.0
        if winner is not None and disc_cost > 0.0:
            token = self.super_tokens.get(winner.token_id)
            if token is not None:
                actual_discovery_cost += self._drain_token_energy(token, disc_cost)

        actual_base_toll = 0.0
        if base > 0.0:
            for token in self.super_tokens.values():
                actual_base_toll += self._drain_token_energy(token, base)

        actual_jackpot = 0.0
        if winner is not None and jackpot > 0.0:
            token = self.super_tokens.get(winner.token_id)
            if token is not None:
                before = float(self.energy_reservoir)
                self._apply_jackpot(token, jackpot)
                after = float(self.energy_reservoir)
                if self.cfg.strict_energy_budget:
                    actual_jackpot = max(before - after, 0.0)
                else:
                    actual_jackpot = jackpot
        if winner is not None:
            token = self.super_tokens.get(winner.token_id)
            if token is not None:
                self._adapt_winner_from_truth(token, gt_tokens, match_len)

        r_step = float(
            actual_jackpot
            + actual_contrastive_bonus
            - actual_attempt_total
            - actual_discovery_cost
            - actual_base_toll
            - actual_contrastive_penalty
        )

        if winner is not None and winner_eval is not None:
            token = self.super_tokens.get(winner.token_id)
            if token is not None:
                if match_len > 0:
                    token.max_silent_correct = max(
                        int(token.max_silent_correct),
                        int(winner_inactivity_before),
                    )
                if float(winner_eval["score"]) > 0.0 and float(winner.bet) > 0.0:
                    token.max_paid_bet = max(
                        float(token.max_paid_bet), float(winner.bet)
                    )

        proposer_set = set(proposer_ids)
        inactive_ids: list[int] = []
        silence_req: list[float] = []
        for tid, token in list(self.super_tokens.items()):
            if tid in proposer_set:
                token.inactivity_steps = 0
            else:
                token.inactivity_steps += 1
                inactive_ids.append(tid)
                silence_req.append(self._silence_credit(token))
        if inactive_ids and silence_req:
            total_req = float(sum(silence_req))
            if not self.cfg.strict_energy_budget:
                for tid, req in zip(inactive_ids, silence_req):
                    token = self.super_tokens.get(tid)
                    if token is not None:
                        token.energy += req
            elif total_req > 0.0 and self.energy_reservoir > 0.0:
                if self.cfg.conserve_total_energy:
                    step_pool = (
                        actual_attempt_total + actual_discovery_cost + actual_base_toll
                    )
                    relief_budget = 0.0
                    active_now = len(self.super_tokens)
                    target_active = max(float(self.cfg.target_active_super), 1.0)
                    if (
                        active_now > 0
                        and float(active_now)
                        < float(self.cfg.survivor_relief_active_frac) * target_active
                    ):
                        relief_budget = min(
                            self.energy_reservoir,
                            float(self.cfg.survivor_relief_reservoir_frac)
                            * self.energy_reservoir,
                        )
                    max_pay = min(
                        total_req, self.energy_reservoir, step_pool + relief_budget
                    )
                else:
                    max_pay = min(total_req, self.energy_reservoir)
                factor = min(1.0, max_pay / total_req) if total_req > 0.0 else 0.0
                paid_total = 0.0
                for tid, req in zip(inactive_ids, silence_req):
                    token = self.super_tokens.get(tid)
                    if token is None:
                        continue
                    pay = req * factor
                    if pay < cfg.min_viable_energy:
                        continue
                    token.energy += pay
                    paid_total += pay
                self.energy_reservoir = max(0.0, self.energy_reservoir - paid_total)

        self.world.advance(advance_len)

        if winner is not None and winner.token_id in self.super_tokens:
            wt = self.super_tokens[winner.token_id]
            contrib = self.gap.contribution_weights(
                receiver_signature=wt.signature,
                resonance_width=wt.resonance_width,
                receiver_phase=wt.phase,
                phase_coupling=wt.phase_coupling,
            )
            if not contrib:
                contrib = {winner.token_id: 1.0}
            self.coop.update(contrib, r_step)

        self_copy_births = 0
        if proposal_evals:
            b_copy, copy_events = self._self_copy_cycle(proposal_evals)
            self_copy_births += b_copy
            events.extend(copy_events)
        births = int(self_copy_births)

        for token in self.super_tokens.values():
            if not np.isfinite(token.energy):
                token.energy = 0.0
            token.energy = float(np.clip(token.energy, 0.0, self.cfg.token_energy_cap))
            if token.energy < cfg.min_viable_energy:
                token.low_energy_steps = int(getattr(token, "low_energy_steps", 0)) + 1
            else:
                token.low_energy_steps = 0
            token.state_vec = np.nan_to_num(
                token.state_vec, nan=0.0, posinf=0.0, neginf=0.0
            ).astype(np.float32)
            token.signature = np.nan_to_num(
                token.signature, nan=0.0, posinf=0.0, neginf=0.0
            ).astype(np.float32)
            token.proposal_drift = np.nan_to_num(
                token.proposal_drift, nan=0.0, posinf=0.0, neginf=0.0
            ).astype(np.float32)
            if not np.isfinite(token.phase):
                token.phase = 0.0
            if not np.isfinite(token.omega):
                token.omega = float(self.cfg.omega_base)

        dead_ids: list[int] = []
        dead_ages: list[int] = []
        dead_token_energy = 0.0
        recycled_gap = 0.0
        for tid, token in list(self.super_tokens.items()):
            slot = int(self.token_anchor_slot.get(int(tid), -1))
            anchored = (
                slot >= 0
                and slot < int(self.cfg.gap_len)
                and int(self.gap.capsule_token_id[slot]) == int(tid)
            )
            dead = False
            if not anchored:
                dead = True
            elif token.energy <= 0.0:
                dead = True
            elif (
                token.energy < cfg.min_viable_energy
                and int(getattr(token, "low_energy_steps", 0))
                > cfg.survivor_grace_steps
            ):
                dead = True
            if dead:
                dead_ids.append(int(tid))
                dead_token_energy += max(float(token.energy), 0.0)
                dead_ages.append(
                    max(
                        int(self.step_idx)
                        - int(getattr(token, "birth_step", self.step_idx)),
                        0,
                    )
                )
                if anchored:
                    recycled_gap += float(max(self.gap.energy[slot], 0.0))
                    self.gap.clear_slot(slot)
                self.token_anchor_slot.pop(int(tid), None)
                del self.super_tokens[int(tid)]
                continue
            if anchored:
                self._refresh_capsule_slot(
                    token, slot, read_strength=last_strength.get(int(tid), 0.0)
                )

        if dead_ages:
            for age in dead_ages:
                if self.capsule_death_events <= 0:
                    self.capsule_half_life_ema = float(age)
                else:
                    self.capsule_half_life_ema = 0.95 * float(
                        self.capsule_half_life_ema
                    ) + 0.05 * float(age)
                self.capsule_death_events += 1

        if dead_ids:
            if self.cfg.conserve_total_energy:
                recycled = recycled_gap + dead_token_energy
            else:
                recycled = float(
                    self.cfg.death_recycle_fraction
                ) * recycled_gap + float(self.cfg.death_recycle_flat) * float(
                    len(dead_ids)
                )
            self._reservoir_add(recycled)
            if recycled > 0.0:
                events.append(f"decompose +{recycled:.3f}")

        self.coop.prune_dead(self.super_tokens.keys())

        if (self.step_idx % cfg.mint_interval) == 0 and len(self.super_tokens) >= 2:
            b, mint_events = self._mint_cycle()
            births += b
            events.extend(mint_events)

        deaths = len(dead_ids)
        if deaths > 0:
            events.append(f"deaths {deaths}")

        active_ref = max(len(self.super_tokens) + deaths, 1)
        proposal_pressure = float(len(proposer_ids) / float(active_ref))
        match_flag = 1.0 if match_len > 0 else 0.0
        self._update_law_emas(
            match_flag=match_flag,
            proposal_pressure=proposal_pressure,
            births=births,
            deaths=deaths,
        )
        law_event, seasonal_births = self._adapt_natural_laws()
        births += int(seasonal_births)
        if law_event:
            events.append(law_event)

        # Final sync after births/deaths/overwrites.
        self._sync_super_tokens_from_gap()

        discovery_advance = max(advance_len - match_len, 0)
        max_symbio_depth, root_only_fraction = self._lineage_metrics()
        gap_compression_ratio = self._gap_compression_ratio()

        live_slots = self.gap.live_capsule_slots(
            min_energy=max(self.cfg.min_viable_energy * 0.25, 1e-12)
        )
        nonroot_live_capsules = 0
        lineage_persistence_1k = 0.0
        if live_slots.size > 0:
            parent_a = np.asarray(self.gap.capsule_parent_a[live_slots], dtype=np.int64)
            parent_b = np.asarray(self.gap.capsule_parent_b[live_slots], dtype=np.int64)
            depth = np.asarray(
                self.gap.capsule_lineage_depth[live_slots], dtype=np.int32
            )
            age = (
                int(self.step_idx)
                - np.asarray(self.gap.capsule_birth_step[live_slots], dtype=np.int64)
            ).astype(np.int64)
            nonroot_mask = (depth > 0) | (parent_a >= 0) | (parent_b >= 0)
            nonroot_live_capsules = int(np.sum(nonroot_mask))
            if nonroot_live_capsules > 0:
                lineage_persistence_1k = float(
                    np.sum(nonroot_mask & (age >= 1000)) / float(nonroot_live_capsules)
                )

        stats = {
            "step": self.step_idx,
            "active_super": len(self.super_tokens),
            "gap_read_backend": str(self.gap.read_backend),
            "max_symbio_depth": int(max_symbio_depth),
            "max_symbio_depth_ever": int(self.max_symbio_depth_ever),
            "root_only_fraction": float(root_only_fraction),
            "gap_compression_ratio": float(gap_compression_ratio),
            "proposers": len(proposer_ids),
            "proposers_raw": int(proposals_raw_count),
            "proposers_frontier": int(frontier_match_count),
            "proposers_gated": int(proposals_gated_count),
            "frontier_rescue_spawned": int(frontier_rescue_spawned),
            "winner_from_frontier_rescue": int(winner_from_frontier_rescue),
            "winner_id": -1 if winner_id is None else int(winner_id),
            "winner_conf": float(winner_conf),
            "proposal_len": int(proposal_len),
            "winner_super_token_len": int(winner_super_token_len),
            "match_len": int(match_len),
            "advance_len": int(advance_len),
            "discovery_advance": int(discovery_advance),
            "attempt_total": float(attempt_total),
            "attempt_total_actual": float(actual_attempt_total),
            "attempt_avg": float(attempt_total / max(len(proposer_ids), 1)),
            "contrastive_bonus": float(actual_contrastive_bonus),
            "contrastive_penalty": float(actual_contrastive_penalty),
            "discovery_cost": float(actual_discovery_cost),
            "base_toll": float(actual_base_toll),
            "jackpot": float(actual_jackpot),
            "jackpot_requested": float(jackpot),
            "realized_return": float(r_step),
            "energy_spent": float(
                actual_attempt_total
                + actual_contrastive_penalty
                + actual_discovery_cost
                + actual_base_toll
                + actual_metabolic_tax
            ),
            "metabolic_tax": float(actual_metabolic_tax),
            "births": int(births),
            "self_copy_births": int(self_copy_births),
            "deaths": int(deaths),
            "cursor": int(self.world.cursor),
            "longest_silent_turn_id": int(longest_silent_turn_id),
            "longest_silent_turn_steps": int(longest_silent_turn_steps),
            "longest_silent_turn_conf": float(longest_silent_turn_conf),
            "longest_silent_turn_match_len": int(longest_silent_turn_match_len),
            "longest_silent_turn_proposal_len": int(longest_silent_turn_proposal_len),
            "longest_silent_turn_score": float(longest_silent_turn_score),
            "longest_silent_turn_won": int(longest_silent_turn_won),
            "longest_silent_turn_token_len": int(longest_silent_turn_token_len),
            "longest_silent_turn_parent_a": int(longest_silent_turn_parent_a),
            "longest_silent_turn_parent_b": int(longest_silent_turn_parent_b),
            "longest_silent_turn_identity_bytes": list(
                longest_silent_turn_identity_bytes
            ),
            "law_attempt_cost_base": float(self.laws["attempt_cost_base"]),
            "law_jackpot_base": float(self.laws["jackpot_base"]),
            "law_spawn_cost": float(self.laws["spawn_cost"]),
            "law_mint_delta": float(self.laws["mint_delta"]),
            "law_pareto_alpha": float(self.laws["pareto_alpha"]),
            "reservoir": float(self.energy_reservoir),
            "energy_total": float(self._total_energy()),
            "energy_drift": float(self._total_energy() - self.total_energy_ref),
            "nonroot_live_capsules": int(nonroot_live_capsules),
            "capsule_half_life": float(self.capsule_half_life_ema),
            "lineage_persistence_1k": float(lineage_persistence_1k),
            "chaos_energy_spent": float(step_chaos_energy),
            "chaos_avg_substeps": float(
                step_chaos_substeps / max(step_chaos_organisms, 1)
            ),
        }
        self.metrics.update(stats, events)
        return StepResult(stats=stats, events=events)

    def run(self, steps: int) -> list[StepResult]:
        out: list[StepResult] = []
        for _ in range(max(1, int(steps))):
            out.append(self.step())
        return out

    def encode_prompt_tokens(self, prompt: str) -> np.ndarray:
        text = str(prompt or "")
        if self.cfg.token_space == "gpt2":
            ids = encode_gpt2_tokens(text, self.cfg.gpt2_model_name)
        else:
            ids = encode_utf8_parity_tokens(text)
        if not ids:
            return np.zeros((0,), dtype=np.int32)
        arr = np.asarray(ids, dtype=np.int32)
        return np.clip(arr, 0, self.base_vocab_size - 1).astype(np.int32)

    def decode_tokens_text(
        self, token_ids: list[int] | np.ndarray, max_chars: int = 2000
    ) -> str:
        ids = [int(x) for x in np.asarray(token_ids, dtype=np.int32).tolist()]
        if not ids:
            return ""
        if self.cfg.token_space == "gpt2":
            text = decode_gpt2_tokens(ids, self.cfg.gpt2_model_name)
        else:
            raw = bytes(
                (int(t) & 0xFF) for t in ids if 0 <= int(t) < self.base_vocab_size
            )
            text = raw.decode("utf-8", errors="replace")
        if max_chars > 0 and len(text) > max_chars:
            return text[:max_chars] + "..."
        return text

    def _infer_identity_tokens_for_frontier(
        self, identity_bytes: np.ndarray, frontier_tid: int
    ) -> np.ndarray:
        b = np.asarray(identity_bytes, dtype=np.int32).reshape(-1)
        if b.size == 0:
            return np.zeros((0,), dtype=np.int32)
        frontier = int(np.clip(frontier_tid, 0, self.base_vocab_size - 1))
        if self.cfg.token_space == "byte_parity":
            frontier_symbol = frontier & 0xFF
            offsets = np.where(np.clip(b, 0, 255) == frontier_symbol)[0].astype(
                np.int32
            )
        else:
            offsets = np.where(np.clip(b, 0, self.base_vocab_size - 1) == frontier)[
                0
            ].astype(np.int32)
        if offsets.size == 0:
            suffix = b[: max(1, min(b.size, self.cfg.proposal_lmax))]
            if self.cfg.token_space == "byte_parity":
                suffix = np.clip(suffix, 0, 255).astype(np.int32)
                p0 = (frontier // 256) & 1
                parity = (p0 + np.arange(suffix.size, dtype=np.int32)) & 1
                return (suffix + 256 * parity).astype(np.int32)
            return np.clip(suffix, 0, self.base_vocab_size - 1).astype(np.int32)

        best = np.zeros((0,), dtype=np.int32)
        best_len = -1
        for off in offsets.tolist():
            suffix = b[int(off) : int(off) + int(self.cfg.proposal_lmax)]
            if suffix.size <= 0:
                continue
            if self.cfg.token_space == "byte_parity":
                suffix = np.clip(suffix, 0, 255).astype(np.int32)
                p0 = (frontier // 256) & 1
                parity = (p0 + np.arange(suffix.size, dtype=np.int32)) & 1
                ids = (suffix + 256 * parity).astype(np.int32)
            else:
                ids = np.clip(suffix, 0, self.base_vocab_size - 1).astype(np.int32)
            if int(ids.size) > best_len:
                best = ids
                best_len = int(ids.size)
        return best

    def _infer_next_token(
        self,
        frontier_tid: int,
        right_tid: int | None = None,
        top_k: int = 16,
        temperature: float = 0.0,
        recent_tokens: list[int] | None = None,
    ) -> tuple[int, dict]:
        candidates: dict[int, float] = {}
        details: list[tuple[int, int, float]] = []
        if not self.super_tokens:
            fallback = int(np.clip(frontier_tid, 0, self.base_vocab_size - 1))
            return fallback, {"used": 0, "top": []}

        pool = sorted(
            self.super_tokens.values(),
            key=lambda t: float(t.energy),
            reverse=True,
        )[: max(1, int(top_k) * 8)]
        frontier = int(np.clip(frontier_tid, 0, self.base_vocab_size - 1))
        right = (
            None
            if right_tid is None
            else int(np.clip(right_tid, 0, self.base_vocab_size - 1))
        )
        for token in pool:
            ids = self._infer_identity_tokens_for_frontier(
                token.identity_bytes, frontier
            )
            if ids.size <= 0:
                continue
            if ids.size >= 2:
                nxt = int(ids[1])
            else:
                nxt = int(ids[0])
            energy_norm = float(
                np.clip(
                    float(token.energy) / max(float(self.cfg.token_energy_cap), 1e-6),
                    0.0,
                    1.0,
                )
            )
            length_norm = float(
                np.clip(
                    float(ids.size) / max(float(self.cfg.proposal_lmax), 1.0), 0.0, 1.0
                )
            )
            conf = 0.30 + 0.45 * length_norm + 0.25 * energy_norm
            score = float(np.clip(conf * (0.25 + 0.75 * energy_norm), 0.0, 5.0))
            if right is not None and ids.size >= 3 and int(ids[2]) == right:
                score += 0.35
            elif right is not None and nxt == right:
                score += 0.10
            candidates[nxt] = candidates.get(nxt, 0.0) + score
            details.append((int(token.token_id), nxt, score))

        if not candidates:
            fallback = int(np.clip(frontier, 0, self.base_vocab_size - 1))
            return fallback, {"used": 0, "top": []}

        if recent_tokens:
            tail = recent_tokens[-32:]
            freq: dict[int, int] = {}
            for t in tail:
                tid = int(t)
                freq[tid] = freq.get(tid, 0) + 1
            for tid, count in freq.items():
                if tid in candidates:
                    candidates[tid] -= 0.18 * float(count)

        ranked = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)
        if float(temperature) <= 1e-8:
            choice = int(ranked[0][0])
        else:
            top = ranked[: max(1, int(top_k))]
            toks = np.asarray([int(t) for t, _ in top], dtype=np.int32)
            vals = np.asarray([float(s) for _, s in top], dtype=np.float64)
            vals = vals - float(np.max(vals))
            probs = np.exp(vals / max(float(temperature), 1e-6))
            probs = probs / max(float(np.sum(probs)), 1e-12)
            idx = int(self.rng.choice(len(toks), p=probs))
            choice = int(toks[idx])

        preview = [(int(t), float(s)) for t, s in ranked[: max(1, int(top_k))]]
        return choice, {"used": int(len(details)), "top": preview}

    def _infer_next_distribution(
        self,
        frontier_tid: int,
        right_tid: int | None = None,
        top_k: int = 64,
        temperature: float = 1.0,
        recent_tokens: list[int] | None = None,
    ) -> tuple[int, dict[int, float], dict]:
        candidates: dict[int, float] = {}
        details: list[tuple[int, int, float]] = []
        if not self.super_tokens:
            fallback = int(np.clip(frontier_tid, 0, self.base_vocab_size - 1))
            return fallback, {fallback: 1.0}, {"used": 0, "top": [(fallback, 1.0)]}

        pool = sorted(
            self.super_tokens.values(),
            key=lambda t: float(t.energy),
            reverse=True,
        )[: max(1, max(int(top_k), 1) * 8)]
        frontier = int(np.clip(frontier_tid, 0, self.base_vocab_size - 1))
        right = (
            None
            if right_tid is None
            else int(np.clip(right_tid, 0, self.base_vocab_size - 1))
        )
        for token in pool:
            ids = self._infer_identity_tokens_for_frontier(
                token.identity_bytes, frontier
            )
            if ids.size <= 0:
                continue
            nxt = int(ids[1]) if ids.size >= 2 else int(ids[0])
            energy_norm = float(
                np.clip(
                    float(token.energy) / max(float(self.cfg.token_energy_cap), 1e-6),
                    0.0,
                    1.0,
                )
            )
            length_norm = float(
                np.clip(
                    float(ids.size) / max(float(self.cfg.proposal_lmax), 1.0), 0.0, 1.0
                )
            )
            conf = 0.30 + 0.45 * length_norm + 0.25 * energy_norm
            score = float(np.clip(conf * (0.25 + 0.75 * energy_norm), 0.0, 5.0))
            if right is not None and ids.size >= 3 and int(ids[2]) == right:
                score += 0.35
            elif right is not None and nxt == right:
                score += 0.10
            candidates[nxt] = candidates.get(nxt, 0.0) + score
            details.append((int(token.token_id), nxt, score))

        if not candidates:
            fallback = int(np.clip(frontier, 0, self.base_vocab_size - 1))
            return fallback, {fallback: 1.0}, {"used": 0, "top": [(fallback, 1.0)]}

        if recent_tokens:
            tail = recent_tokens[-32:]
            freq: dict[int, int] = {}
            for t in tail:
                tid = int(t)
                freq[tid] = freq.get(tid, 0) + 1
            for tid, count in freq.items():
                if tid in candidates:
                    candidates[tid] -= 0.18 * float(count)

        ranked = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)
        if int(top_k) > 0:
            support = ranked[: max(1, int(top_k))]
        else:
            support = ranked
        toks = np.asarray([int(t) for t, _ in support], dtype=np.int32)
        vals = np.asarray([float(s) for _, s in support], dtype=np.float64)
        vals = vals - float(np.max(vals))
        temp = max(float(temperature), 1e-6)
        probs = np.exp(vals / temp)
        probs = probs / max(float(np.sum(probs)), 1e-12)
        choice = int(toks[int(np.argmax(probs))])
        dist = {int(t): float(p) for t, p in zip(toks.tolist(), probs.tolist())}
        preview = [(int(t), float(s)) for t, s in support[: min(16, len(support))]]
        return choice, dist, {"used": int(len(details)), "top": preview}

    def infer_generate(
        self,
        prompt: str,
        max_new_tokens: int = 64,
        diffusion_passes: int = 2,
        window_size: int = 16,
        top_k: int = 16,
        temperature: float = 0.0,
    ) -> dict:
        prompt_ids = self.encode_prompt_tokens(prompt)
        if prompt_ids.size <= 0:
            prompt_ids = np.asarray(
                [int(np.clip(self.world.peek(1)[0], 0, self.base_vocab_size - 1))],
                dtype=np.int32,
            )

        seq: list[int] = [int(x) for x in prompt_ids.tolist()]
        max_new = max(1, int(max_new_tokens))
        passes = max(1, int(diffusion_passes))
        win = max(1, int(window_size))
        prompt_len = int(len(seq))

        debug_steps: list[str] = []

        # Initial autoregressive fill.
        for i in range(max_new):
            frontier = seq[-1]
            nxt, diag = self._infer_next_token(
                frontier_tid=frontier,
                right_tid=None,
                top_k=top_k,
                temperature=temperature,
                recent_tokens=seq,
            )
            seq.append(int(nxt))
            if i < 8:
                debug_steps.append(
                    f"gen[{i}] frontier={frontier} -> {nxt} cands={diag['used']}"
                )

        # Sliding-window diffusion refinement over generated segment only.
        final_len = len(seq)
        gen_start = prompt_len
        gen_end = final_len
        for pass_idx in range(1, passes):
            for start in range(gen_start, gen_end):
                end = min(start + win, gen_end)
                for pos in range(start, end):
                    left = seq[pos - 1] if pos > 0 else seq[0]
                    right = seq[pos + 1] if pos + 1 < len(seq) else None
                    nxt, _ = self._infer_next_token(
                        frontier_tid=left,
                        right_tid=right,
                        top_k=top_k,
                        temperature=temperature,
                        recent_tokens=seq[:pos],
                    )
                    seq[pos] = int(nxt)
            if pass_idx <= 2:
                debug_steps.append(f"diffusion_pass[{pass_idx}] done")

        out = seq[prompt_len:]
        return {
            "token_space": self.cfg.token_space,
            "prompt_tokens": int(prompt_len),
            "generated_tokens": int(len(out)),
            "passes": int(passes),
            "window": int(win),
            "top_k": int(top_k),
            "temperature": float(temperature),
            "output_token_ids": [int(x) for x in out],
            "output_text": self.decode_tokens_text(out, max_chars=4000),
            "debug": debug_steps[:16],
        }
-e 

===== eval_horizon.py =====
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import math
import time
from dataclasses import asdict, replace
from pathlib import Path
from typing import Any

import numpy as np

from config import EmeraConfig
from engine import EmeraEngine
from world import encode_gpt2_tokens, encode_utf8_parity_tokens


def _parse_bool(v: str | None, default: bool | None = None) -> bool | None:
    if v is None:
        return default
    return v.strip().lower() in {"1", "true", "yes", "on"}


def _parse_horizons(text: str) -> list[int]:
    vals: list[int] = []
    for part in str(text).split(","):
        part = part.strip()
        if not part:
            continue
        vals.append(max(int(part), 1))
    uniq = sorted(set(vals))
    if not uniq:
        raise ValueError("No valid horizons provided.")
    return uniq


def _encode_text_tokens(text: str, token_space: str, gpt2_model_name: str, base_tokens: int) -> np.ndarray:
    if token_space == "gpt2":
        ids = encode_gpt2_tokens(text, gpt2_model_name)
        arr = np.asarray(ids, dtype=np.int32)
        return np.clip(arr, 0, max(base_tokens - 1, 0)).astype(np.int32)
    ids = encode_utf8_parity_tokens(text)
    return np.asarray(ids, dtype=np.int32)


def _snap_to_newline(raw: bytes, idx: int) -> int:
    idx = int(np.clip(idx, 0, len(raw)))
    if idx <= 0 or idx >= len(raw):
        return idx
    right = raw.find(b"\n", idx, min(len(raw), idx + 4096))
    if right >= 0:
        return int(right + 1)
    left = raw.rfind(b"\n", max(0, idx - 4096), idx)
    if left >= 0:
        return int(left + 1)
    return idx


def _split_corpus_bytes(raw: bytes, train_frac: float, val_frac: float) -> tuple[bytes, bytes, bytes]:
    n = len(raw)
    if n < 64:
        raise ValueError("Corpus is too small to split.")
    i_train = _snap_to_newline(raw, int(n * train_frac))
    i_val = _snap_to_newline(raw, int(n * (train_frac + val_frac)))
    i_train = int(np.clip(i_train, 1, n - 2))
    i_val = int(np.clip(i_val, i_train + 1, n - 1))
    return raw[:i_train], raw[i_train:i_val], raw[i_val:]


def _sample_starts(token_count: int, max_h: int, num_contexts: int, seed: int) -> np.ndarray:
    upper = int(token_count - max_h - 1)
    if upper <= 0:
        return np.zeros((0,), dtype=np.int64)
    all_idx = np.arange(upper, dtype=np.int64)
    if num_contexts <= 0 or num_contexts >= upper:
        return all_idx
    rng = np.random.default_rng(seed)
    pick = np.asarray(rng.choice(upper, size=num_contexts, replace=False), dtype=np.int64)
    pick.sort()
    return pick


def _evaluate_horizon(
    engine: EmeraEngine,
    tokens: np.ndarray,
    horizons: list[int],
    num_contexts: int,
    rollout_len: int,
    top_k: int,
    temperature: float,
    prob_floor: float,
    seed: int,
) -> dict[str, Any]:
    max_h = int(max(horizons))
    starts = _sample_starts(int(tokens.size), max_h=max_h, num_contexts=num_contexts, seed=seed)
    if starts.size == 0:
        raise ValueError("Split too small for requested horizons.")

    per_h: dict[int, dict[str, float]] = {
        int(h): {"nll_sum": 0.0, "acc_sum": 0.0, "count": 0.0}
        for h in horizons
    }
    rollout_n = max(1, min(int(rollout_len), max_h))
    exact_sum = 0.0
    first_err_sum = 0.0

    floor = max(float(prob_floor), 1e-15)
    for start in starts.tolist():
        seq: list[int] = [int(tokens[start])]
        preds: list[int] = []
        dists: list[dict[int, float]] = []
        for _ in range(max_h):
            nxt, dist, _ = engine._infer_next_distribution(
                frontier_tid=seq[-1],
                right_tid=None,
                top_k=int(top_k),
                temperature=float(temperature),
                recent_tokens=seq,
            )
            preds.append(int(nxt))
            dists.append(dist)
            seq.append(int(nxt))

        truth = np.asarray(tokens[start + 1 : start + max_h + 1], dtype=np.int32)
        for h in horizons:
            idx = int(h - 1)
            true_tok = int(truth[idx])
            p = float(dists[idx].get(true_tok, floor))
            p = max(p, floor)
            per_h[h]["nll_sum"] += -math.log(p)
            per_h[h]["acc_sum"] += 1.0 if int(preds[idx]) == true_tok else 0.0
            per_h[h]["count"] += 1.0

        first_err = rollout_n
        exact = 1.0
        for i in range(rollout_n):
            if int(preds[i]) != int(truth[i]):
                first_err = i
                exact = 0.0
                break
        exact_sum += exact
        first_err_sum += float(first_err)

    by_h: dict[str, Any] = {}
    for h in horizons:
        c = max(per_h[h]["count"], 1.0)
        by_h[str(h)] = {
            "acc": per_h[h]["acc_sum"] / c,
            "nll": per_h[h]["nll_sum"] / c,
        }

    long_h = [h for h in horizons if h >= 32]
    if not long_h:
        long_h = horizons[-max(1, len(horizons) // 2) :]

    lr_score = float(np.mean([by_h[str(h)]["acc"] for h in long_h]))
    lr_nll = float(np.mean([by_h[str(h)]["nll"] for h in long_h]))
    contexts = float(starts.size)
    return {
        "contexts": int(starts.size),
        "horizons": by_h,
        "lr_horizons": [int(h) for h in long_h],
        "lr_score": lr_score,
        "lr_nll": lr_nll,
        "rollout_len": int(rollout_n),
        "rollout_exact_match_rate": exact_sum / contexts,
        "rollout_first_error_mean": first_err_sum / contexts,
    }


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Held-out horizon-conditioned probability benchmark for Emera")
    p.add_argument("--corpus-file", type=str, default="data/bible.txt")
    p.add_argument("--token-space", type=str, choices=["byte_parity", "gpt2"], default="byte_parity")
    p.add_argument("--gpt2-model-name", type=str, default="gpt2")
    p.add_argument("--seed", type=int, default=42)
    p.add_argument("--train-frac", type=float, default=0.70)
    p.add_argument("--val-frac", type=float, default=0.15)
    p.add_argument("--train-steps", type=int, default=2000)
    p.add_argument("--world-vocab-size", type=int, default=512)
    p.add_argument("--world-len", type=int, default=None)
    p.add_argument("--gap-read-backend", type=str, choices=["auto", "numpy", "jax"], default=None)
    p.add_argument("--gap-read-batch-size", type=int, default=None)
    p.add_argument("--horizons", type=str, default="1,2,4,8,16,32,64,128,256")
    p.add_argument("--num-contexts", type=int, default=512)
    p.add_argument("--rollout-len", type=int, default=256)
    p.add_argument("--eval-top-k", type=int, default=0)
    p.add_argument("--eval-temperature", type=float, default=1.0)
    p.add_argument("--prob-floor", type=float, default=1e-9)
    p.add_argument("--splits-dir", type=str, default="out/horizon_splits")
    p.add_argument("--output-json", type=str, default="out/horizon_eval.json")

    # High-impact knobs for tuning sweeps.
    p.add_argument("--proposal-lmax", type=int, default=None)
    p.add_argument("--proposal-frontier-fallback", type=int, default=None)
    p.add_argument("--proposal-frontier-contrast", type=int, default=None)
    p.add_argument("--frontier-rescue-max-per-step", type=int, default=None)
    p.add_argument("--frontier-rescue-energy", type=float, default=None)
    p.add_argument("--chaos-min-substeps", type=int, default=None)
    p.add_argument("--chaos-max-substeps", type=int, default=None)
    p.add_argument("--chaos-substep-cost", type=float, default=None)
    p.add_argument("--self-copy-enabled", type=str, default=None)
    p.add_argument("--dynamic-laws", type=str, default=None)
    p.add_argument("--seasons-enabled", type=str, default=None)
    return p.parse_args()


def main() -> None:
    args = parse_args()
    horizons = _parse_horizons(args.horizons)
    if args.train_steps < 1:
        raise ValueError("--train-steps must be >= 1")
    if args.num_contexts < 1:
        raise ValueError("--num-contexts must be >= 1")
    if not (0.0 < args.train_frac < 1.0):
        raise ValueError("--train-frac must be in (0, 1)")
    if not (0.0 < args.val_frac < 1.0):
        raise ValueError("--val-frac must be in (0, 1)")
    if args.train_frac + args.val_frac >= 1.0:
        raise ValueError("train_frac + val_frac must be < 1.0")

    raw = Path(args.corpus_file).read_bytes()
    train_raw, val_raw, test_raw = _split_corpus_bytes(raw, train_frac=float(args.train_frac), val_frac=float(args.val_frac))
    splits_dir = Path(args.splits_dir)
    splits_dir.mkdir(parents=True, exist_ok=True)
    train_file = splits_dir / "train.txt"
    val_file = splits_dir / "val.txt"
    test_file = splits_dir / "test.txt"
    train_file.write_bytes(train_raw)
    val_file.write_bytes(val_raw)
    test_file.write_bytes(test_raw)

    train_text = train_raw.decode("utf-8", errors="ignore")
    val_text = val_raw.decode("utf-8", errors="ignore")
    test_text = test_raw.decode("utf-8", errors="ignore")

    base_tokens = 50257 if args.token_space == "gpt2" else 512
    val_tokens = _encode_text_tokens(val_text, args.token_space, args.gpt2_model_name, base_tokens=base_tokens)
    test_tokens = _encode_text_tokens(test_text, args.token_space, args.gpt2_model_name, base_tokens=base_tokens)
    train_tokens = _encode_text_tokens(train_text, args.token_space, args.gpt2_model_name, base_tokens=base_tokens)

    cfg = EmeraConfig()
    world_len = int(args.world_len) if args.world_len is not None else int(max(train_tokens.size, 1))
    updates: dict[str, Any] = {
        "seed": int(args.seed),
        "token_space": str(args.token_space),
        "gpt2_model_name": str(args.gpt2_model_name),
        "corpus_file": str(train_file),
        "world_len": int(world_len),
        "world_vocab_size": int(args.world_vocab_size),
        "log_every": max(int(args.train_steps), 1),
    }
    if args.token_space == "gpt2":
        updates["base_tokens"] = 50257
        if args.world_vocab_size == 512:
            updates["world_vocab_size"] = 50257
    if args.proposal_lmax is not None:
        updates["proposal_lmax"] = int(args.proposal_lmax)
    if args.gap_read_backend is not None:
        updates["gap_read_backend"] = str(args.gap_read_backend)
    if args.gap_read_batch_size is not None:
        updates["gap_read_batch_size"] = int(args.gap_read_batch_size)
    if args.proposal_frontier_fallback is not None:
        updates["proposal_frontier_fallback"] = int(args.proposal_frontier_fallback)
    if args.proposal_frontier_contrast is not None:
        updates["proposal_frontier_contrast"] = int(args.proposal_frontier_contrast)
    if args.frontier_rescue_max_per_step is not None:
        updates["frontier_rescue_max_per_step"] = int(args.frontier_rescue_max_per_step)
    if args.frontier_rescue_energy is not None:
        updates["frontier_rescue_energy"] = float(args.frontier_rescue_energy)
    if args.chaos_min_substeps is not None:
        updates["chaos_min_substeps"] = int(args.chaos_min_substeps)
    if args.chaos_max_substeps is not None:
        updates["chaos_max_substeps"] = int(args.chaos_max_substeps)
    if args.chaos_substep_cost is not None:
        updates["chaos_substep_cost"] = float(args.chaos_substep_cost)
    b_self_copy = _parse_bool(args.self_copy_enabled, default=None)
    if b_self_copy is not None:
        updates["self_copy_enabled"] = bool(b_self_copy)
    b_laws = _parse_bool(args.dynamic_laws, default=None)
    if b_laws is not None:
        updates["dynamic_laws"] = bool(b_laws)
    b_seasons = _parse_bool(args.seasons_enabled, default=None)
    if b_seasons is not None:
        updates["seasons_enabled"] = bool(b_seasons)

    cfg = replace(cfg, **updates)
    cfg.validate()

    t0 = time.time()
    engine = EmeraEngine(cfg)
    for _ in range(int(args.train_steps)):
        engine.step()
    train_elapsed = time.time() - t0
    train_snap = engine.metrics.snapshot()
    train_distance = int(train_snap.get("distance", 0))
    effective_passes = float(train_distance) / max(float(cfg.world_len), 1.0)

    eval_kwargs = dict(
        horizons=horizons,
        num_contexts=int(args.num_contexts),
        rollout_len=int(args.rollout_len),
        top_k=int(args.eval_top_k),
        temperature=float(args.eval_temperature),
        prob_floor=float(args.prob_floor),
    )
    val_eval = _evaluate_horizon(
        engine=engine,
        tokens=val_tokens,
        seed=int(args.seed) + 11,
        **eval_kwargs,
    )
    test_eval = _evaluate_horizon(
        engine=engine,
        tokens=test_tokens,
        seed=int(args.seed) + 29,
        **eval_kwargs,
    )

    out = {
        "config": asdict(cfg),
        "train_steps": int(args.train_steps),
        "train_elapsed_sec": float(train_elapsed),
        "train_distance": int(train_distance),
        "effective_train_passes": float(effective_passes),
        "split_bytes": {
            "train": int(len(train_raw)),
            "val": int(len(val_raw)),
            "test": int(len(test_raw)),
        },
        "split_tokens": {
            "train": int(train_tokens.size),
            "val": int(val_tokens.size),
            "test": int(test_tokens.size),
        },
        "val": val_eval,
        "test": test_eval,
    }

    out_path = Path(args.output_json)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(json.dumps(out, indent=2, ensure_ascii=True), encoding="utf-8")
    print(
        f"horizon-eval done train_steps={args.train_steps} "
        f"val_lr={val_eval['lr_score']:.4f} val_lr_nll={val_eval['lr_nll']:.4f} "
        f"test_lr={test_eval['lr_score']:.4f} test_lr_nll={test_eval['lr_nll']:.4f} "
        f"wrote={out_path}"
    )


if __name__ == "__main__":
    main()
-e 

===== gap_field.py =====
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Iterable

import numpy as np

from config import EmeraConfig

_JAX_STATE: dict[str, Any] = {
    "checked": False,
    "ok": False,
    "has_accel": False,
    "jax": None,
    "jnp": None,
    "kernel": None,
}


def _normalize_vec(v: np.ndarray, eps: float = 1e-8) -> np.ndarray:
    n = np.linalg.norm(v)
    if n <= eps:
        return np.zeros_like(v)
    return v / n


def _ensure_jax_state() -> dict[str, Any]:
    if bool(_JAX_STATE.get("checked", False)):
        return _JAX_STATE
    _JAX_STATE["checked"] = True
    try:
        import contextlib
        import io

        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):
            import jax
            import jax.numpy as jnp

            devices = jax.devices()

        has_accel = any(str(getattr(d, "platform", "")).lower() != "cpu" for d in devices)

        @jax.jit
        def _read_many_jax_kernel(
            points: Any,
            phase: Any,
            energy: Any,
            receiver_signature: Any,
            resonance_width: Any,
            receiver_phase: Any,
            phase_coupling: Any,
            valid_mask: Any,
        ) -> tuple[Any, Any]:
            eff_energy = jnp.where(energy > 1e-9, energy, 0.0)
            sum_energy = jnp.sum(eff_energy)
            valid = jnp.clip(valid_mask.reshape(-1), 0.0, 1.0).astype(jnp.float32)

            pts_norm = points / jnp.maximum(jnp.linalg.norm(points, axis=1, keepdims=True), 1e-8)
            pts_norm = jnp.nan_to_num(pts_norm, nan=0.0, posinf=0.0, neginf=0.0)

            recv = receiver_signature / jnp.maximum(
                jnp.linalg.norm(receiver_signature, axis=1, keepdims=True),
                1e-8,
            )
            recv = jnp.nan_to_num(recv, nan=0.0, posinf=0.0, neginf=0.0)
            recv = recv * valid[:, None]

            sim = jnp.clip(recv @ pts_norm.T, -1.0, 1.0)
            width = jnp.maximum(resonance_width[:, None], 1e-3)
            geo = jnp.exp(-((1.0 - sim) ** 2) / (2.0 * width * width))

            phase_align = 0.5 * (1.0 + jnp.cos(phase[None, :] - receiver_phase[:, None]))
            pc = phase_coupling[:, None]
            phase_gate = (1.0 - pc) + pc * phase_align

            w = eff_energy[None, :] * geo * phase_gate
            w = jnp.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)
            w = w * valid[:, None]
            total_w = jnp.sum(w, axis=1)

            resonance = (w @ points) / jnp.maximum(total_w[:, None], 1e-9)
            strength = jnp.clip(total_w / jnp.maximum(sum_energy, 1e-9), 0.0, 1.0)

            has_energy = sum_energy > 1e-9
            resonance = jnp.where(has_energy, resonance, jnp.zeros_like(resonance))
            strength = jnp.where(has_energy, strength, jnp.zeros_like(strength))
            resonance = resonance * valid[:, None]
            strength = strength * valid
            return resonance.astype(jnp.float32), strength.astype(jnp.float32)

        _JAX_STATE["ok"] = True
        _JAX_STATE["has_accel"] = bool(has_accel)
        _JAX_STATE["jax"] = jax
        _JAX_STATE["jnp"] = jnp
        _JAX_STATE["kernel"] = _read_many_jax_kernel
    except Exception:
        _JAX_STATE["ok"] = False
        _JAX_STATE["has_accel"] = False
        _JAX_STATE["jax"] = None
        _JAX_STATE["jnp"] = None
        _JAX_STATE["kernel"] = None
    return _JAX_STATE


@dataclass
class GapRead:
    resonance: np.ndarray
    strength: float
    contributions: Dict[int, float]


class GapField:
    def __init__(self, cfg: EmeraConfig):
        self.cfg = cfg
        self.points = np.zeros((cfg.gap_len, cfg.gap_dim), dtype=np.float32)
        self.velocity = np.zeros((cfg.gap_len, cfg.gap_dim), dtype=np.float32)
        self.phase = np.zeros((cfg.gap_len,), dtype=np.float32)
        self.omega = np.zeros((cfg.gap_len,), dtype=np.float32)
        self.energy = np.zeros((cfg.gap_len,), dtype=np.float32)
        # Heritable payload lane: what was written, not just where/when it was emitted.
        self.genome_fragment = np.full((cfg.gap_len, max(int(cfg.proposal_lmax), 1)), -1, dtype=np.int32)
        self.genome_len = np.zeros((cfg.gap_len,), dtype=np.int16)
        self.genome_weight = np.zeros((cfg.gap_len,), dtype=np.float32)
        self.ifs_fragment = np.zeros((cfg.gap_len, int(cfg.num_ifs), 2, 3), dtype=np.float32)
        self.ifs_weight = np.zeros((cfg.gap_len,), dtype=np.float32)
        self.emitter_id = np.full((cfg.gap_len,), -1, dtype=np.int64)
        self.step_idx = np.zeros((cfg.gap_len,), dtype=np.int64)
        self.round_idx = np.zeros((cfg.gap_len,), dtype=np.int32)
        # Full substrate capsule payload: organism state lives in gap slots.
        self.capsule_token_id = np.full((cfg.gap_len,), -1, dtype=np.int64)
        self.capsule_parent_a = np.full((cfg.gap_len,), -1, dtype=np.int64)
        self.capsule_parent_b = np.full((cfg.gap_len,), -1, dtype=np.int64)
        self.capsule_birth_step = np.zeros((cfg.gap_len,), dtype=np.int64)
        self.capsule_inactivity_steps = np.zeros((cfg.gap_len,), dtype=np.int32)
        self.capsule_low_energy_steps = np.zeros((cfg.gap_len,), dtype=np.int32)
        self.capsule_lineage_depth = np.zeros((cfg.gap_len,), dtype=np.int16)
        self.capsule_max_paid_bet = np.zeros((cfg.gap_len,), dtype=np.float32)
        self.capsule_max_silent_correct = np.zeros((cfg.gap_len,), dtype=np.int32)
        self.capsule_state_vec = np.zeros((cfg.gap_len, cfg.d_latent), dtype=np.float32)
        self.capsule_signature = np.zeros((cfg.gap_len, cfg.gap_dim), dtype=np.float32)
        self.capsule_proposal_drift = np.zeros((cfg.gap_len, cfg.d_latent), dtype=np.float32)
        self.capsule_traits = np.zeros((cfg.gap_len, 8), dtype=np.float32)
        self.capsule_identity = np.full((cfg.gap_len, max(int(cfg.proposal_lmax), 1)), -1, dtype=np.int32)
        self.capsule_identity_len = np.zeros((cfg.gap_len,), dtype=np.int16)
        self.capsule_ifs = np.zeros((cfg.gap_len, int(cfg.num_ifs), 2, 3), dtype=np.float32)
        self.ptr = 0
        self.read_backend = self._resolve_read_backend(str(cfg.gap_read_backend))

    def _resolve_read_backend(self, requested: str) -> str:
        req = str(requested).strip().lower()
        if req not in {"auto", "numpy", "jax"}:
            req = "auto"
        if req == "numpy":
            return "numpy"
        jax_state = _ensure_jax_state()
        if req == "jax":
            return "jax" if bool(jax_state.get("ok", False)) else "numpy"
        if bool(jax_state.get("ok", False)) and bool(jax_state.get("has_accel", False)):
            est_work = (
                int(self.cfg.gap_len)
                * int(self.cfg.gap_dim)
                * int(max(self.cfg.gap_read_batch_size, 1))
            )
            # Auto mode prefers NumPy for tiny kernels where host/device overhead dominates.
            if est_work >= 500_000:
                return "jax"
        return "numpy"

    def decay(self) -> None:
        self.energy *= self.cfg.gap_decay
        self.velocity *= self.cfg.gap_velocity_decay
        self.genome_weight *= self.cfg.gap_decay
        self.ifs_weight *= self.cfg.gap_decay
        self.phase = (self.phase + self.omega) % (2.0 * np.pi)

    def write(
        self,
        point: np.ndarray,
        velocity: np.ndarray,
        phase: float,
        omega: float,
        energy: float,
        genome_fragment: np.ndarray | None,
        genome_weight: float,
        ifs_fragment: np.ndarray | None,
        ifs_weight: float,
        emitter_id: int,
        step_idx: int,
        round_idx: int,
        capsule_token: Any | None = None,
        lineage_depth: int | None = None,
    ) -> None:
        p = self.ptr
        self.points[p] = point.astype(np.float32, copy=False)
        self.velocity[p] = velocity.astype(np.float32, copy=False)
        self.phase[p] = float(phase)
        self.omega[p] = float(omega)
        self.energy[p] = max(float(energy), 0.0)
        self.genome_fragment[p].fill(-1)
        if genome_fragment is None:
            self.genome_len[p] = 0
            self.genome_weight[p] = 0.0
        else:
            frag = np.asarray(genome_fragment, dtype=np.int32).reshape(-1)
            n = min(int(frag.size), int(self.genome_fragment.shape[1]))
            if n > 0:
                vmax = 255 if self.cfg.token_space == "byte_parity" else max(int(self.cfg.base_tokens) - 1, 0)
                self.genome_fragment[p, :n] = np.clip(frag[:n], 0, vmax).astype(np.int32, copy=False)
                self.genome_len[p] = int(n)
                self.genome_weight[p] = max(float(genome_weight), 0.0)
            else:
                self.genome_len[p] = 0
                self.genome_weight[p] = 0.0
        if ifs_fragment is None:
            self.ifs_fragment[p].fill(0.0)
            self.ifs_weight[p] = 0.0
        else:
            raw = np.asarray(ifs_fragment, dtype=np.float32).reshape(-1)
            need = int(self.cfg.num_ifs) * 6
            packed = np.zeros((need,), dtype=np.float32)
            n = min(int(raw.size), need)
            if n > 0:
                packed[:n] = raw[:n]
            frag = packed.reshape(int(self.cfg.num_ifs), 2, 3)
            self.ifs_fragment[p] = np.clip(frag, -4.0, 4.0).astype(np.float32, copy=False)
            self.ifs_weight[p] = max(float(ifs_weight), 0.0)
        self.emitter_id[p] = int(emitter_id)
        self.step_idx[p] = int(step_idx)
        self.round_idx[p] = int(round_idx)
        if capsule_token is not None:
            self._store_capsule(p, capsule_token, lineage_depth)
        self.ptr = (p + 1) % self.cfg.gap_len

    def _store_capsule(self, slot: int, token: Any, lineage_depth: int | None = None) -> None:
        s = int(slot)
        ident = np.asarray(getattr(token, "identity_bytes"), dtype=np.int32).reshape(-1)
        n = min(int(ident.size), int(self.capsule_identity.shape[1]))
        self.capsule_token_id[s] = int(getattr(token, "token_id"))
        self.capsule_parent_a[s] = int(getattr(token, "parent_a", -1))
        self.capsule_parent_b[s] = int(getattr(token, "parent_b", -1))
        self.capsule_birth_step[s] = int(getattr(token, "birth_step", 0))
        self.capsule_inactivity_steps[s] = int(getattr(token, "inactivity_steps", 0))
        self.capsule_low_energy_steps[s] = int(getattr(token, "low_energy_steps", 0))
        if lineage_depth is not None:
            self.capsule_lineage_depth[s] = int(lineage_depth)
        self.capsule_max_paid_bet[s] = float(getattr(token, "max_paid_bet", 0.0))
        self.capsule_max_silent_correct[s] = int(getattr(token, "max_silent_correct", 0))
        self.capsule_state_vec[s] = np.asarray(getattr(token, "state_vec"), dtype=np.float32)
        self.capsule_signature[s] = np.asarray(getattr(token, "signature"), dtype=np.float32)
        self.capsule_proposal_drift[s] = np.asarray(getattr(token, "proposal_drift"), dtype=np.float32)
        self.capsule_traits[s, 0] = float(getattr(token, "activation_threshold", 0.5))
        self.capsule_traits[s, 1] = float(getattr(token, "emission_amplitude", 0.4))
        self.capsule_traits[s, 2] = float(getattr(token, "emission_decay", 0.15))
        self.capsule_traits[s, 3] = float(getattr(token, "silence_growth_rate", 1.0))
        self.capsule_traits[s, 4] = float(getattr(token, "resonance_width", 0.45))
        self.capsule_traits[s, 5] = float(getattr(token, "phase_coupling", 0.25))
        self.capsule_traits[s, 6] = float(getattr(token, "velocity_coupling", 0.20))
        self.capsule_traits[s, 7] = float(getattr(token, "proposal_length_bias", 0.0))
        self.capsule_identity[s].fill(-1)
        if n > 0:
            vmax = 255 if self.cfg.token_space == "byte_parity" else max(int(self.cfg.base_tokens) - 1, 0)
            self.capsule_identity[s, :n] = np.clip(ident[:n], 0, vmax).astype(np.int32, copy=False)
        self.capsule_identity_len[s] = int(n)
        self.capsule_ifs[s] = np.asarray(getattr(token, "ifs"), dtype=np.float32)

    def clear_slot(self, slot: int) -> None:
        s = int(slot)
        self.points[s] = 0.0
        self.velocity[s] = 0.0
        self.phase[s] = 0.0
        self.omega[s] = 0.0
        self.energy[s] = 0.0
        self.genome_fragment[s] = -1
        self.genome_len[s] = 0
        self.genome_weight[s] = 0.0
        self.ifs_fragment[s] = 0.0
        self.ifs_weight[s] = 0.0
        self.emitter_id[s] = -1
        self.step_idx[s] = 0
        self.round_idx[s] = 0
        self.capsule_token_id[s] = -1
        self.capsule_parent_a[s] = -1
        self.capsule_parent_b[s] = -1
        self.capsule_birth_step[s] = 0
        self.capsule_inactivity_steps[s] = 0
        self.capsule_low_energy_steps[s] = 0
        self.capsule_lineage_depth[s] = 0
        self.capsule_max_paid_bet[s] = 0.0
        self.capsule_max_silent_correct[s] = 0
        self.capsule_state_vec[s] = 0.0
        self.capsule_signature[s] = 0.0
        self.capsule_proposal_drift[s] = 0.0
        self.capsule_traits[s] = 0.0
        self.capsule_identity[s] = -1
        self.capsule_identity_len[s] = 0
        self.capsule_ifs[s] = 0.0

    def live_capsule_slots(self, min_energy: float = 0.0) -> np.ndarray:
        return np.where(
            (self.capsule_token_id >= 0) & (self.energy > float(min_energy))
        )[0].astype(np.int32, copy=False)

    def _read_many_numpy(
        self,
        receiver_signature: np.ndarray,
        resonance_width: np.ndarray,
        receiver_phase: np.ndarray,
        phase_coupling: np.ndarray,
        valid_mask: np.ndarray | None = None,
    ) -> tuple[np.ndarray, np.ndarray]:
        sig = np.asarray(receiver_signature, dtype=np.float32)
        widths = np.asarray(resonance_width, dtype=np.float32).reshape(-1)
        rphase = np.asarray(receiver_phase, dtype=np.float32).reshape(-1)
        pc = np.asarray(phase_coupling, dtype=np.float32).reshape(-1)

        n = int(sig.shape[0])
        if n <= 0:
            return (
                np.zeros((0, int(self.cfg.gap_dim)), dtype=np.float32),
                np.zeros((0,), dtype=np.float32),
            )
        if valid_mask is None:
            valid = np.ones((n,), dtype=np.float32)
        else:
            valid = np.asarray(valid_mask, dtype=np.float32).reshape(-1)
            if valid.size != n:
                raise ValueError("valid_mask must match receiver batch size")
            valid = np.clip(valid, 0.0, 1.0)

        energy = np.where(self.energy > 1e-9, self.energy, 0.0).astype(np.float32)
        sum_energy = float(np.sum(energy))
        if sum_energy <= 1e-9:
            return (
                np.zeros((n, int(self.cfg.gap_dim)), dtype=np.float32),
                np.zeros((n,), dtype=np.float32),
            )

        pts = self.points.astype(np.float32, copy=False)
        pts_norm = pts / np.maximum(np.linalg.norm(pts, axis=1, keepdims=True), 1e-8)
        pts_norm = np.nan_to_num(pts_norm, nan=0.0, posinf=0.0, neginf=0.0)

        recv = sig / np.maximum(np.linalg.norm(sig, axis=1, keepdims=True), 1e-8)
        recv = np.nan_to_num(recv, nan=0.0, posinf=0.0, neginf=0.0)
        recv = recv * valid.reshape(-1, 1)

        sim = np.clip(recv @ pts_norm.T, -1.0, 1.0)
        width = np.maximum(widths.reshape(-1, 1), 1e-3)
        geo = np.exp(-((1.0 - sim) ** 2) / (2.0 * width * width)).astype(np.float32)

        phase_align = 0.5 * (1.0 + np.cos(self.phase.reshape(1, -1) - rphase.reshape(-1, 1)))
        phase_gate = (1.0 - pc.reshape(-1, 1)) + pc.reshape(-1, 1) * phase_align.astype(np.float32)

        w = energy.reshape(1, -1) * geo * phase_gate
        w = np.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)
        w *= valid.reshape(-1, 1)
        total_w = np.sum(w, axis=1, dtype=np.float64).astype(np.float32)

        resonance = (w @ pts).astype(np.float32)
        resonance /= np.maximum(total_w.reshape(-1, 1), 1e-9)
        resonance *= valid.reshape(-1, 1)

        strength = np.clip(total_w / max(sum_energy, 1e-9), 0.0, 1.0).astype(np.float32)
        strength *= valid
        return resonance, strength

    def _read_many_jax(
        self,
        receiver_signature: np.ndarray,
        resonance_width: np.ndarray,
        receiver_phase: np.ndarray,
        phase_coupling: np.ndarray,
        valid_mask: np.ndarray | None = None,
    ) -> tuple[np.ndarray, np.ndarray]:
        jax_state = _ensure_jax_state()
        if not bool(jax_state.get("ok", False)):
            return self._read_many_numpy(
                receiver_signature=receiver_signature,
                resonance_width=resonance_width,
                receiver_phase=receiver_phase,
                phase_coupling=phase_coupling,
                valid_mask=valid_mask,
            )
        if valid_mask is None:
            valid = np.ones((int(np.asarray(receiver_signature).shape[0]),), dtype=np.float32)
        else:
            valid = np.asarray(valid_mask, dtype=np.float32).reshape(-1)
        jnp = jax_state["jnp"]
        kernel = jax_state["kernel"]
        r, s = kernel(
            jnp.asarray(self.points, dtype=jnp.float32),
            jnp.asarray(self.phase, dtype=jnp.float32),
            jnp.asarray(self.energy, dtype=jnp.float32),
            jnp.asarray(receiver_signature, dtype=jnp.float32),
            jnp.asarray(resonance_width, dtype=jnp.float32),
            jnp.asarray(receiver_phase, dtype=jnp.float32),
            jnp.asarray(phase_coupling, dtype=jnp.float32),
            jnp.asarray(valid, dtype=jnp.float32),
        )
        return np.asarray(r, dtype=np.float32), np.asarray(s, dtype=np.float32)

    def read_many(
        self,
        receiver_signature: np.ndarray,
        resonance_width: np.ndarray,
        receiver_phase: np.ndarray,
        phase_coupling: np.ndarray,
        valid_mask: np.ndarray | None = None,
    ) -> tuple[np.ndarray, np.ndarray]:
        sig = np.asarray(receiver_signature, dtype=np.float32)
        if sig.ndim != 2 or sig.shape[1] != int(self.cfg.gap_dim):
            raise ValueError(
                f"receiver_signature must have shape [N, {int(self.cfg.gap_dim)}], got {sig.shape}"
            )
        n = int(sig.shape[0])
        widths = np.asarray(resonance_width, dtype=np.float32).reshape(-1)
        rphase = np.asarray(receiver_phase, dtype=np.float32).reshape(-1)
        pc = np.asarray(phase_coupling, dtype=np.float32).reshape(-1)
        if widths.size != n or rphase.size != n or pc.size != n:
            raise ValueError("batch read inputs must have matching leading dimension")
        valid: np.ndarray | None = None
        if valid_mask is not None:
            valid = np.asarray(valid_mask, dtype=np.float32).reshape(-1)
            if valid.size != n:
                raise ValueError("valid_mask must match receiver batch size")
        if self.read_backend == "jax":
            return self._read_many_jax(sig, widths, rphase, pc, valid)
        return self._read_many_numpy(sig, widths, rphase, pc, valid)

    def _weights(
        self,
        receiver_signature: np.ndarray,
        resonance_width: float,
        receiver_phase: float,
        phase_coupling: float,
    ) -> tuple[np.ndarray, np.ndarray]:
        active = self.energy > 1e-9
        if not np.any(active):
            return active, np.zeros((0,), dtype=np.float32)
        pts = self.points[active]
        recv = _normalize_vec(receiver_signature)
        recv = np.nan_to_num(recv, nan=0.0, posinf=0.0, neginf=0.0)
        pts_norm = pts / np.maximum(np.linalg.norm(pts, axis=1, keepdims=True), 1e-8)
        pts_norm = np.nan_to_num(pts_norm, nan=0.0, posinf=0.0, neginf=0.0)
        sim = np.clip(pts_norm @ recv, -1.0, 1.0)

        width = max(float(resonance_width), 1e-3)
        geo = np.exp(-((1.0 - sim) ** 2) / (2.0 * width * width))

        phase_align = 0.5 * (1.0 + np.cos(self.phase[active] - float(receiver_phase)))
        phase_gate = (1.0 - float(phase_coupling)) + float(phase_coupling) * phase_align

        w = self.energy[active] * geo.astype(np.float32) * phase_gate.astype(np.float32)
        w = np.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)
        return active, w.astype(np.float32)

    def read(
        self,
        receiver_signature: np.ndarray,
        resonance_width: float,
        receiver_phase: float,
        phase_coupling: float,
        include_contributions: bool = True,
    ) -> GapRead:
        active, w = self._weights(
            receiver_signature=receiver_signature,
            resonance_width=resonance_width,
            receiver_phase=receiver_phase,
            phase_coupling=phase_coupling,
        )
        total_w = float(np.sum(w)) if w.size else 0.0
        if w.size == 0 or total_w <= 1e-9:
            return GapRead(
                resonance=np.zeros((self.cfg.gap_dim,), dtype=np.float32),
                strength=0.0,
                contributions={},
            )
        pts = self.points[active]
        resonance = np.sum(pts * w[:, None], axis=0) / max(total_w, 1e-9)

        active_energy = self.energy[active]
        strength = float(total_w / max(np.sum(active_energy), 1e-9))
        strength = float(np.clip(strength, 0.0, 1.0))

        if include_contributions:
            emit = self.emitter_id[active]
            contrib_raw: Dict[int, float] = {}
            for eid, wi in zip(emit.tolist(), w.tolist()):
                if eid < 0:
                    continue
                contrib_raw[eid] = contrib_raw.get(eid, 0.0) + float(wi)
            total = sum(contrib_raw.values())
            if total > 1e-9:
                contrib = {k: v / total for k, v in contrib_raw.items()}
            else:
                contrib = {}
        else:
            contrib = {}
        return GapRead(
            resonance=resonance.astype(np.float32),
            strength=strength,
            contributions=contrib,
        )

    def contribution_weights(
        self,
        receiver_signature: np.ndarray,
        resonance_width: float,
        receiver_phase: float,
        phase_coupling: float,
    ) -> Dict[int, float]:
        return self.read(
            receiver_signature=receiver_signature,
            resonance_width=resonance_width,
            receiver_phase=receiver_phase,
            phase_coupling=phase_coupling,
        ).contributions

    def purge_emitters(self, dead_emitters: Iterable[int]) -> None:
        dead = set(int(x) for x in dead_emitters)
        if not dead:
            return
        mask = np.isin(self.emitter_id, np.fromiter(dead, dtype=np.int64))
        if not np.any(mask):
            return
        for s in np.where(mask)[0].tolist():
            self.clear_slot(int(s))
-e 

===== genome.py =====
from __future__ import annotations

from dataclasses import dataclass

import numpy as np

from config import EmeraConfig


def _normalize(v: np.ndarray, eps: float = 1e-8) -> np.ndarray:
    n = np.linalg.norm(v)
    if n <= eps:
        return np.zeros_like(v)
    return v / n


def _sigmoid(x: float) -> float:
    return float(1.0 / (1.0 + np.exp(-x)))


def _identity_symbol_vocab(cfg: EmeraConfig) -> int:
    if cfg.token_space == "byte_parity":
        return 256
    return max(int(cfg.base_tokens), 1)


_ROOT_IDENTITY_LEN = 2


def _coerce_root_pair(
    identity_bytes: np.ndarray | None,
    vocab_size: int,
    rng: np.random.Generator,
) -> np.ndarray:
    vocab = max(int(vocab_size), 1)
    raw = np.asarray(identity_bytes, dtype=np.int32).reshape(-1) if identity_bytes is not None else np.zeros((0,), dtype=np.int32)
    if raw.size >= _ROOT_IDENTITY_LEN:
        out = raw[:_ROOT_IDENTITY_LEN].copy()
    elif raw.size == 1:
        out = np.asarray([int(raw[0]), int(rng.integers(0, vocab))], dtype=np.int32)
    else:
        out = rng.integers(0, vocab, size=(_ROOT_IDENTITY_LEN,), dtype=np.int32)
    return np.clip(out, 0, max(vocab - 1, 0)).astype(np.int32, copy=False)


def _compose_recursive_lineage_identity(
    parent_a: np.ndarray,
    parent_b: np.ndarray,
    max_len: int,
    vocab_size: int,
    rng: np.random.Generator,
) -> np.ndarray:
    a = np.asarray(parent_a, dtype=np.int32).reshape(-1)
    b = np.asarray(parent_b, dtype=np.int32).reshape(-1)
    if a.size <= 0:
        a = _coerce_root_pair(None, vocab_size, rng)
    if b.size <= 0:
        b = _coerce_root_pair(None, vocab_size, rng)
    limit = max(2, int(max_len))
    if int(a.size + b.size) <= limit:
        out = np.concatenate([a, b], axis=0)
    else:
        keep_a = max(1, min(int(a.size), limit // 2))
        keep_b = max(1, min(int(b.size), limit - keep_a))
        rem = max(limit - keep_a - keep_b, 0)
        if rem > 0:
            room_a = max(int(a.size) - keep_a, 0)
            add_a = min(rem, room_a)
            keep_a += add_a
            rem -= add_a
        if rem > 0:
            room_b = max(int(b.size) - keep_b, 0)
            add_b = min(rem, room_b)
            keep_b += add_b
        out = np.concatenate([a[-keep_a:], b[:keep_b]], axis=0)
    return np.clip(out[:limit], 0, max(int(vocab_size) - 1, 0)).astype(np.int32, copy=False)


@dataclass
class Proposal:
    token_id: int
    tokens: np.ndarray
    confidence: float
    quality: float
    expected_return: float
    bet: float


@dataclass
class SuperToken:
    token_id: int
    parent_a: int
    parent_b: int
    energy: float
    inactivity_steps: int
    state_vec: np.ndarray
    signature: np.ndarray
    proposal_drift: np.ndarray
    ifs: np.ndarray  # [num_ifs, 2, 3]
    phase: float
    omega: float
    activation_threshold: float
    emission_amplitude: float
    emission_decay: float
    silence_growth_rate: float
    resonance_width: float
    phase_coupling: float
    velocity_coupling: float
    proposal_length_bias: float
    identity_bytes: np.ndarray
    birth_step: int
    low_energy_steps: int
    max_paid_bet: float
    max_silent_correct: int

    def chaos_step(self, rng: np.random.Generator, resonance_latent: np.ndarray) -> np.ndarray:
        resonance_latent = np.nan_to_num(resonance_latent, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)
        idx = int(rng.integers(0, self.ifs.shape[0]))
        a = self.ifs[idx, :, :2]
        b = self.ifs[idx, :, 2]

        prev = self.state_vec[:2].copy()
        nxt = np.tanh(a @ prev + b + 0.12 * resonance_latent[:2]).astype(np.float32)
        vel2 = (nxt - prev).astype(np.float32)

        self.state_vec[:2] = nxt
        self.state_vec = np.tanh(
            (1.0 - 0.3 * np.clip(self.emission_decay, 0.0, 1.0)) * self.state_vec
            + 0.22 * resonance_latent
        ).astype(np.float32)
        self.state_vec = np.nan_to_num(self.state_vec, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)

        self.phase = float(np.arctan2(self.state_vec[1], self.state_vec[0]))
        return vel2

    def emission(self, cfg: EmeraConfig, vel2: np.ndarray) -> tuple[np.ndarray, np.ndarray, float]:
        emit = np.tanh(
            self.emission_amplitude * self.state_vec[: cfg.gap_dim]
            + cfg.beacon_strength * self.signature
        ).astype(np.float32)
        vel = np.zeros((cfg.gap_dim,), dtype=np.float32)
        vel[:2] = vel2.astype(np.float32)
        vel = np.tanh(vel * (0.5 + self.velocity_coupling)).astype(np.float32)
        eng = max(self.emission_amplitude * max(self.energy, 0.0) * 0.05, 1e-6)
        return emit, vel, float(eng)

    def proposal_length(self, cfg: EmeraConfig, resonance_strength: float) -> int:
        if not np.isfinite(resonance_strength):
            resonance_strength = 0.0
        x = self.proposal_length_bias + 2.2 * float(resonance_strength)
        if not np.isfinite(x):
            x = 0.0
        frac = _sigmoid(x)
        l = 1 + int(np.floor(frac * float(cfg.proposal_lmax - 1)))
        return int(np.clip(l, 1, cfg.proposal_lmax))

    def decode_tokens(
        self,
        base_latent: np.ndarray,
        cfg: EmeraConfig,
        resonance_strength: float,
        candidate_token_ids: np.ndarray | None = None,
    ) -> tuple[np.ndarray, float, float]:
        length = self.proposal_length(cfg, resonance_strength)
        ids = np.zeros((length,), dtype=np.int32)
        confs = np.zeros((length,), dtype=np.float32)
        if candidate_token_ids is None:
            candidate_token_ids = np.arange(base_latent.shape[0], dtype=np.int32)
        q = self.state_vec.copy()
        drift = self.proposal_drift
        for k in range(length):
            query = _normalize(q + float(k) * drift)
            scores = base_latent @ query
            top2 = np.argpartition(scores, -2)[-2:]
            if scores[top2[0]] >= scores[top2[1]]:
                i1, i2 = int(top2[0]), int(top2[1])
            else:
                i1, i2 = int(top2[1]), int(top2[0])
            margin = float(scores[i1] - scores[i2])
            conf = _sigmoid(cfg.confidence_scale * margin)
            ids[k] = int(candidate_token_ids[i1])
            confs[k] = float(conf)
        confidence = float(np.mean(confs))
        quality = float(confidence * (0.5 + 0.5 * np.clip(resonance_strength, 0.0, 1.0)))
        return ids, confidence, quality


def _template_ifs(cfg: EmeraConfig) -> np.ndarray:
    out = np.zeros((cfg.num_ifs, 2, 3), dtype=np.float32)
    for k in range(cfg.num_ifs):
        angle = (k / max(cfg.num_ifs, 1)) * 2.0 * np.pi
        scale = 0.65
        c = np.cos(angle) * scale
        s = np.sin(angle) * scale
        out[k, :, :2] = np.array([[c, -s], [s, c]], dtype=np.float32)
        out[k, :, 2] = np.array([0.05 * np.cos(angle), 0.05 * np.sin(angle)], dtype=np.float32)
    return out


def _sample_state_vec(cfg: EmeraConfig, rng: np.random.Generator) -> np.ndarray:
    v = rng.normal(0.0, 1.0, size=(cfg.d_latent,)).astype(np.float32)
    return _normalize(v)


def _sample_drift(cfg: EmeraConfig, rng: np.random.Generator) -> np.ndarray:
    v = rng.normal(0.0, cfg.proposal_drift_scale, size=(cfg.d_latent,)).astype(np.float32)
    return _normalize(v)


def make_initial_super_token(
    cfg: EmeraConfig,
    rng: np.random.Generator,
    token_id: int,
    birth_step: int = 0,
    identity_bytes: np.ndarray | None = None,
) -> SuperToken:
    state = _sample_state_vec(cfg, rng)
    sig = _normalize(state[: cfg.gap_dim]).astype(np.float32)
    ifs = (_template_ifs(cfg) + rng.normal(0.0, cfg.ifs_mutation_scale, size=(cfg.num_ifs, 2, 3))).astype(np.float32)
    omega = float(cfg.omega_base + cfg.omega_jitter * rng.normal())
    ident = _coerce_root_pair(identity_bytes, _identity_symbol_vocab(cfg), rng)
    return SuperToken(
        token_id=int(token_id),
        parent_a=-1,
        parent_b=-1,
        energy=float(cfg.initial_super_energy),
        inactivity_steps=0,
        state_vec=state.astype(np.float32),
        signature=sig.astype(np.float32),
        proposal_drift=_sample_drift(cfg, rng).astype(np.float32),
        ifs=ifs,
        phase=float(np.arctan2(state[1], state[0])),
        omega=omega,
        activation_threshold=float(cfg.activation_threshold_init),
        emission_amplitude=float(cfg.emission_amplitude_init),
        emission_decay=float(cfg.emission_decay_init),
        silence_growth_rate=float(cfg.silence_growth_init),
        resonance_width=float(cfg.resonance_width_init),
        phase_coupling=float(cfg.phase_coupling_init),
        velocity_coupling=float(cfg.velocity_coupling_init),
        proposal_length_bias=float(cfg.proposal_length_bias_init),
        identity_bytes=ident,
        birth_step=int(birth_step),
        low_energy_steps=0,
        max_paid_bet=0.0,
        max_silent_correct=0,
    )


def create_initial_population(
    cfg: EmeraConfig,
    rng: np.random.Generator,
    start_token_id: int,
    birth_step: int = 0,
    identity_bank: list[np.ndarray] | None = None,
) -> dict[int, SuperToken]:
    out: dict[int, SuperToken] = {}
    for i in range(cfg.initial_super_tokens):
        tid = start_token_id + i
        ident = None
        if identity_bank is not None and i < len(identity_bank):
            ident = identity_bank[i]
        out[tid] = make_initial_super_token(cfg, rng, tid, birth_step=birth_step, identity_bytes=ident)
    return out


def _mutate_scalar(v: float, scale: float, lo: float, hi: float, rng: np.random.Generator) -> float:
    m = float(v * np.exp(rng.normal(0.0, scale)))
    return float(np.clip(m, lo, hi))


def _sample_pareto_factor(
    rng: np.random.Generator,
    alpha: float,
    scale: float,
    clip_max: float,
) -> float:
    a = max(float(alpha), 1.01)
    raw = scale * (rng.pareto(a) + 1.0)
    raw = float(np.clip(raw, 1e-6, clip_max))
    sign = -1.0 if rng.random() < 0.5 else 1.0
    return float(np.exp(sign * (raw - 1.0)))


def _mutate_identity_bytes(
    parent_a: np.ndarray,
    parent_b: np.ndarray,
    max_len: int,
    vocab_size: int,
    rng: np.random.Generator,
) -> np.ndarray:
    a = np.asarray(parent_a, dtype=np.int32).reshape(-1)
    b = np.asarray(parent_b, dtype=np.int32).reshape(-1)
    if a.size == 0:
        a = np.asarray([32], dtype=np.int32)
    if b.size == 0:
        b = np.asarray([32], dtype=np.int32)

    modes = int(rng.integers(0, 5))
    if modes == 0:
        cand = a.copy()
    elif modes == 1:
        cand = b.copy()
    elif modes == 2:
        na = max(1, a.size // 2)
        nb = max(1, b.size - b.size // 2)
        cand = np.concatenate([a[:na], b[-nb:]], axis=0)
    elif modes == 3:
        nb = max(1, b.size // 2)
        na = max(1, a.size - a.size // 2)
        cand = np.concatenate([b[:nb], a[-na:]], axis=0)
    else:
        cand = np.concatenate([a, b], axis=0)

    limit = max(1, int(max_len))
    if cand.size > limit:
        start = int(rng.integers(0, cand.size - limit + 1))
        cand = cand[start : start + limit]
    if cand.size == 0:
        cand = np.asarray([32], dtype=np.int32)

    if rng.random() < 0.15:
        idx = int(rng.integers(0, cand.size))
        cand[idx] = int(rng.integers(0, max(int(vocab_size), 1)))
    if cand.size < limit and rng.random() < 0.15:
        cand = np.concatenate(
            [cand, np.asarray([int(rng.integers(0, max(int(vocab_size), 1)))], dtype=np.int32)],
            axis=0,
        )

    return np.clip(cand, 0, max(int(vocab_size) - 1, 0)).astype(np.int32)


def mint_child_from_parents(
    cfg: EmeraConfig,
    rng: np.random.Generator,
    new_id: int,
    parent_a: SuperToken,
    parent_b: SuperToken,
    spawn_energy: float,
    pareto_alpha: float,
    identity_override: np.ndarray | None = None,
    ifs_override: np.ndarray | None = None,
    birth_step: int = 0,
) -> SuperToken:
    state = _normalize(
        0.5 * (parent_a.state_vec + parent_b.state_vec)
        + rng.normal(0.0, cfg.mutation_scale, size=parent_a.state_vec.shape).astype(np.float32)
    ).astype(np.float32)
    sig = _normalize(state[: cfg.gap_dim]).astype(np.float32)

    parent_ifs_seed = 0.5 * (parent_a.ifs + parent_b.ifs)
    if ifs_override is None:
        ifs_seed = parent_ifs_seed.astype(np.float32)
    else:
        raw = np.asarray(ifs_override, dtype=np.float32).reshape(-1)
        need = int(cfg.num_ifs) * 6
        packed = np.zeros((need,), dtype=np.float32)
        n = min(int(raw.size), need)
        if n > 0:
            packed[:n] = raw[:n]
        ov = packed.reshape(int(cfg.num_ifs), 2, 3)
        ifs_seed = (0.75 * ov + 0.25 * parent_ifs_seed).astype(np.float32)
    child_ifs = (
        ifs_seed
        + rng.normal(0.0, cfg.ifs_mutation_scale, size=parent_a.ifs.shape).astype(np.float32)
    ).astype(np.float32)
    drift = _normalize(
        0.5 * (parent_a.proposal_drift + parent_b.proposal_drift)
        + rng.normal(0.0, cfg.proposal_drift_scale, size=parent_a.proposal_drift.shape).astype(np.float32)
    ).astype(np.float32)
    f = [
        _sample_pareto_factor(
            rng=rng,
            alpha=pareto_alpha,
            scale=cfg.pareto_mutation_scale,
            clip_max=cfg.pareto_clip,
        )
        for _ in range(8)
    ]

    vocab = _identity_symbol_vocab(cfg)
    ident = _compose_recursive_lineage_identity(
        parent_a=parent_a.identity_bytes,
        parent_b=parent_b.identity_bytes,
        max_len=cfg.proposal_lmax,
        vocab_size=vocab,
        rng=rng,
    )
    if identity_override is not None:
        ov = np.asarray(identity_override, dtype=np.int32).reshape(-1)
        if ov.size > 0 and ident.size > 0:
            ov = np.clip(ov, 0, max(vocab - 1, 0)).astype(np.int32, copy=False)
            edits = max(1, min(int(ident.size // 4), int(ov.size)))
            idx = np.asarray(rng.choice(int(ident.size), size=edits, replace=False), dtype=np.int32)
            for j, pos in enumerate(idx.tolist()):
                ident[pos] = int(ov[j % int(ov.size)])
            ident = np.clip(ident, 0, max(vocab - 1, 0)).astype(np.int32, copy=False)

    return SuperToken(
        token_id=int(new_id),
        parent_a=int(parent_a.token_id),
        parent_b=int(parent_b.token_id),
        energy=float(spawn_energy),
        inactivity_steps=0,
        state_vec=state,
        signature=sig,
        proposal_drift=drift,
        ifs=child_ifs,
        phase=float(np.arctan2(state[1], state[0])),
        omega=float(np.clip(0.5 * (parent_a.omega + parent_b.omega), 0.0, np.pi / 2.0)),
        activation_threshold=_mutate_scalar(
            0.5 * (parent_a.activation_threshold + parent_b.activation_threshold) * f[0],
            cfg.mutation_scale,
            0.01,
            0.99,
            rng,
        ),
        emission_amplitude=_mutate_scalar(
            0.5 * (parent_a.emission_amplitude + parent_b.emission_amplitude) * f[1],
            cfg.mutation_scale,
            0.05,
            3.0,
            rng,
        ),
        emission_decay=_mutate_scalar(
            0.5 * (parent_a.emission_decay + parent_b.emission_decay) * f[2],
            cfg.mutation_scale,
            0.0,
            1.0,
            rng,
        ),
        silence_growth_rate=_mutate_scalar(
            0.5 * (parent_a.silence_growth_rate + parent_b.silence_growth_rate) * f[3],
            cfg.mutation_scale,
            0.01,
            8.0,
            rng,
        ),
        resonance_width=_mutate_scalar(
            0.5 * (parent_a.resonance_width + parent_b.resonance_width) * f[4],
            cfg.mutation_scale,
            0.05,
            2.0,
            rng,
        ),
        phase_coupling=_mutate_scalar(
            0.5 * (parent_a.phase_coupling + parent_b.phase_coupling) * f[5],
            cfg.mutation_scale,
            0.0,
            1.0,
            rng,
        ),
        velocity_coupling=_mutate_scalar(
            0.5 * (parent_a.velocity_coupling + parent_b.velocity_coupling) * f[6],
            cfg.mutation_scale,
            0.0,
            1.0,
            rng,
        ),
        proposal_length_bias=_mutate_scalar(
            0.5 * (parent_a.proposal_length_bias + parent_b.proposal_length_bias) * f[7],
            cfg.mutation_scale,
            -3.0,
            3.0,
            rng,
        ),
        identity_bytes=ident,
        birth_step=int(birth_step),
        low_energy_steps=0,
        max_paid_bet=0.0,
        max_silent_correct=0,
    )


def mint_child_from_parent(
    cfg: EmeraConfig,
    rng: np.random.Generator,
    new_id: int,
    parent: SuperToken,
    spawn_energy: float,
    pareto_alpha: float,
    identity_override: np.ndarray | None = None,
    ifs_override: np.ndarray | None = None,
    birth_step: int = 0,
) -> SuperToken:
    state = _normalize(
        parent.state_vec
        + rng.normal(0.0, cfg.mutation_scale, size=parent.state_vec.shape).astype(np.float32)
    ).astype(np.float32)
    sig = _normalize(state[: cfg.gap_dim]).astype(np.float32)

    parent_ifs_seed = parent.ifs
    if ifs_override is None:
        ifs_seed = parent_ifs_seed.astype(np.float32)
    else:
        raw = np.asarray(ifs_override, dtype=np.float32).reshape(-1)
        need = int(cfg.num_ifs) * 6
        packed = np.zeros((need,), dtype=np.float32)
        n = min(int(raw.size), need)
        if n > 0:
            packed[:n] = raw[:n]
        ov = packed.reshape(int(cfg.num_ifs), 2, 3)
        ifs_seed = (0.80 * ov + 0.20 * parent_ifs_seed).astype(np.float32)
    child_ifs = (
        ifs_seed
        + rng.normal(0.0, cfg.ifs_mutation_scale, size=parent.ifs.shape).astype(np.float32)
    ).astype(np.float32)
    drift = _normalize(
        parent.proposal_drift
        + rng.normal(0.0, cfg.proposal_drift_scale, size=parent.proposal_drift.shape).astype(np.float32)
    ).astype(np.float32)
    f = [
        _sample_pareto_factor(
            rng=rng,
            alpha=pareto_alpha,
            scale=cfg.pareto_mutation_scale,
            clip_max=cfg.pareto_clip,
        )
        for _ in range(8)
    ]

    if identity_override is None:
        ident = _mutate_identity_bytes(
            parent_a=parent.identity_bytes,
            parent_b=parent.identity_bytes,
            max_len=cfg.proposal_lmax,
            vocab_size=_identity_symbol_vocab(cfg),
            rng=rng,
        )
    else:
        ident = np.asarray(identity_override, dtype=np.int32).reshape(-1)
        if ident.size == 0:
            ident = _mutate_identity_bytes(
                parent_a=parent.identity_bytes,
                parent_b=parent.identity_bytes,
                max_len=cfg.proposal_lmax,
                vocab_size=_identity_symbol_vocab(cfg),
                rng=rng,
            )
        else:
            vmax = _identity_symbol_vocab(cfg) - 1
            ident = np.clip(
                ident[: max(int(cfg.proposal_lmax), 1)],
                0,
                max(vmax, 0),
            ).astype(np.int32)

    return SuperToken(
        token_id=int(new_id),
        parent_a=int(parent.token_id),
        parent_b=-1,
        energy=float(spawn_energy),
        inactivity_steps=0,
        state_vec=state,
        signature=sig,
        proposal_drift=drift,
        ifs=child_ifs,
        phase=float(np.arctan2(state[1], state[0])),
        omega=_mutate_scalar(
            parent.omega,
            cfg.mutation_scale,
            0.0,
            np.pi / 2.0,
            rng,
        ),
        activation_threshold=_mutate_scalar(
            parent.activation_threshold * f[0],
            cfg.mutation_scale,
            0.01,
            0.99,
            rng,
        ),
        emission_amplitude=_mutate_scalar(
            parent.emission_amplitude * f[1],
            cfg.mutation_scale,
            0.05,
            3.0,
            rng,
        ),
        emission_decay=_mutate_scalar(
            parent.emission_decay * f[2],
            cfg.mutation_scale,
            0.0,
            1.0,
            rng,
        ),
        silence_growth_rate=_mutate_scalar(
            parent.silence_growth_rate * f[3],
            cfg.mutation_scale,
            0.01,
            8.0,
            rng,
        ),
        resonance_width=_mutate_scalar(
            parent.resonance_width * f[4],
            cfg.mutation_scale,
            0.05,
            2.0,
            rng,
        ),
        phase_coupling=_mutate_scalar(
            parent.phase_coupling * f[5],
            cfg.mutation_scale,
            0.0,
            1.0,
            rng,
        ),
        velocity_coupling=_mutate_scalar(
            parent.velocity_coupling * f[6],
            cfg.mutation_scale,
            0.0,
            1.0,
            rng,
        ),
        proposal_length_bias=_mutate_scalar(
            parent.proposal_length_bias * f[7],
            cfg.mutation_scale,
            -3.0,
            3.0,
            rng,
        ),
        identity_bytes=ident,
        birth_step=int(birth_step),
        low_energy_steps=0,
        max_paid_bet=0.0,
        max_silent_correct=0,
    )
-e 

===== identity.py =====
from __future__ import annotations

from dataclasses import dataclass

import numpy as np

from config import EmeraConfig


_WTE_CACHE: dict[str, np.ndarray] = {}


def _normalize_rows(x: np.ndarray, eps: float = 1e-8) -> np.ndarray:
    norms = np.linalg.norm(x, axis=1, keepdims=True)
    return x / np.maximum(norms, eps)


def hadamard_matrix(n: int) -> np.ndarray:
    if n < 1 or (n & (n - 1)) != 0:
        raise ValueError("Hadamard size must be a positive power of two.")
    h = np.array([[1.0]], dtype=np.float32)
    while h.shape[0] < n:
        h = np.block([[h, h], [h, -h]]).astype(np.float32)
    return h


def random_orthonormal_projection(
    rng: np.random.Generator,
    rows: int,
    cols: int,
) -> np.ndarray:
    m = rng.normal(size=(rows, cols)).astype(np.float32)
    q, _ = np.linalg.qr(m, mode="reduced")
    return q[:, :cols].astype(np.float32)


def _load_gpt2_wte(model_name: str) -> np.ndarray:
    cached = _WTE_CACHE.get(model_name)
    if cached is not None:
        return cached.copy()
    try:
        from huggingface_hub import hf_hub_download, list_repo_files
        from safetensors import safe_open
    except Exception as exc:  # pragma: no cover
        raise RuntimeError(
            "gpt2 token space requires `huggingface_hub` and `safetensors`."
        ) from exc

    files = list_repo_files(model_name)
    safes = [f for f in files if f.endswith(".safetensors")]
    if not safes:
        raise RuntimeError(f"No .safetensors weights found for model '{model_name}'.")
    preferred = ["model.safetensors"]
    chosen = None
    for name in preferred:
        if name in safes:
            chosen = name
            break
    if chosen is None:
        chosen = safes[0]
    path = hf_hub_download(repo_id=model_name, filename=chosen)
    with safe_open(path, framework="np") as sf:
        keys = list(sf.keys())
        for key in ("transformer.wte.weight", "wte.weight", "model.embed_tokens.weight"):
            if key in keys:
                w = np.asarray(sf.get_tensor(key), dtype=np.float32)
                _WTE_CACHE[model_name] = w
                return w.copy()
        # Fallback for custom checkpoints: first tensor with token embedding semantics.
        for key in keys:
            if "wte" in key or "embed_tokens" in key:
                w = np.asarray(sf.get_tensor(key), dtype=np.float32)
                _WTE_CACHE[model_name] = w
                return w.copy()
    raise RuntimeError(f"Could not locate GPT-2 input embedding tensor in {chosen}.")


def _ifs_from_latent(z: np.ndarray, cfg: EmeraConfig) -> np.ndarray:
    out = np.zeros((z.shape[0], cfg.num_ifs, 2, 3), dtype=np.float32)
    d = z.shape[1]
    for tid in range(z.shape[0]):
        vec = z[tid]
        for k in range(cfg.num_ifs):
            i0 = (2 + 3 * k) % d
            i1 = (3 + 3 * k) % d
            i2 = (4 + 3 * k) % d
            angle = np.pi * vec[i0]
            scale = 0.45 + 0.35 * abs(vec[i1])
            c = np.cos(angle)
            s = np.sin(angle)
            a = np.array([[c, -s], [s, c]], dtype=np.float32) * scale
            b = np.array([0.18 * vec[i1], 0.18 * vec[i2]], dtype=np.float32)
            out[tid, k, :, :2] = a
            out[tid, k, :, 2] = b
    return out


@dataclass
class BaseIdentity:
    latent: np.ndarray  # [512, d_latent]
    beacon: np.ndarray  # [512, gap_dim]
    ifs: np.ndarray  # [512, num_ifs, 2, 3]
    attractor0: np.ndarray  # [512, 2]
    phase0: np.ndarray  # [512]
    omega0: np.ndarray  # [512]


def create_base_identity(cfg: EmeraConfig, rng: np.random.Generator) -> BaseIdentity:
    if cfg.token_space == "gpt2":
        wte = _load_gpt2_wte(cfg.gpt2_model_name)
        vocab_size, emb_dim = int(wte.shape[0]), int(wte.shape[1])
        if cfg.base_tokens > vocab_size:
            raise ValueError(
                f"base_tokens={cfg.base_tokens} exceeds GPT-2 vocab size {vocab_size}."
            )
        if cfg.base_tokens < vocab_size:
            wte = wte[: cfg.base_tokens]
        p = random_orthonormal_projection(rng, emb_dim, cfg.d_latent)
        z = _normalize_rows(_normalize_rows(wte) @ p)
    else:
        h = hadamard_matrix(cfg.base_tokens) / np.sqrt(float(cfg.base_tokens))
        p = random_orthonormal_projection(rng, cfg.base_tokens, cfg.d_latent)
        z = _normalize_rows(h @ p)
    beacon = _normalize_rows(z[:, : cfg.gap_dim])
    ifs = _ifs_from_latent(z, cfg)
    attractor0 = z[:, :2].astype(np.float32)
    phase0 = np.arctan2(attractor0[:, 1], attractor0[:, 0]).astype(np.float32)
    omega0 = (cfg.omega_base + cfg.omega_jitter * z[:, 2]).astype(np.float32)
    return BaseIdentity(
        latent=z.astype(np.float32),
        beacon=beacon.astype(np.float32),
        ifs=ifs.astype(np.float32),
        attractor0=attractor0,
        phase0=phase0,
        omega0=omega0,
    )
-e 

===== ledger.py =====
from __future__ import annotations

import numpy as np

from config import EmeraConfig
from genome import SuperToken


def attempt_cost(token: SuperToken, cfg: EmeraConfig) -> float:
    return attempt_cost_with_base(token, cfg, cfg.attempt_cost_base)


def attempt_cost_with_base(token: SuperToken, cfg: EmeraConfig, base_cost: float) -> float:
    amp = max(float(token.emission_amplitude), 0.0)
    return float(base_cost * (1.0 + 0.25 * amp))


def discovery_cost(unmatched_len: int, cfg: EmeraConfig) -> float:
    return float(cfg.discovery_cost * max(int(unmatched_len), 0))


def base_toll(advance_len: int, cfg: EmeraConfig) -> float:
    return float(cfg.base_toll * max(int(advance_len), 0))


def silence_credit(token: SuperToken, cfg: EmeraConfig) -> float:
    return silence_credit_with_coeffs(token, cfg, cfg.silence_log_coeff, cfg.silence_exp_coeff)


def silence_credit_with_coeffs(
    token: SuperToken,
    cfg: EmeraConfig,
    log_coeff: float,
    exp_coeff: float,
) -> float:
    s = max(int(token.inactivity_steps), 0)
    g = max(float(token.silence_growth_rate), 1e-6)
    log_term = float(log_coeff) * g * np.log1p(float(s))
    expo = np.clip(cfg.silence_exp_rate * g * float(s), 0.0, 40.0)
    exp_term = float(exp_coeff) * (np.exp(expo) - 1.0)
    return float(max(log_term + exp_term, 0.0))


def jackpot_reward(
    cfg: EmeraConfig,
    match_len: int,
    proposal_len: int,
    quality: float,
    rarity: float,
    inactivity_steps: int,
) -> float:
    return jackpot_reward_with_base(
        cfg=cfg,
        jackpot_base_value=cfg.jackpot_base,
        match_len=match_len,
        proposal_len=proposal_len,
        quality=quality,
        rarity=rarity,
        inactivity_steps=inactivity_steps,
    )


def jackpot_reward_with_base(
    cfg: EmeraConfig,
    jackpot_base_value: float,
    match_len: int,
    proposal_len: int,
    quality: float,
    rarity: float,
    inactivity_steps: int,
) -> float:
    if match_len <= 0 or proposal_len <= 0:
        return 0.0
    length_factor = 1.0 + cfg.jackpot_length_scale * float(max(match_len - 1, 0))
    quality_factor = float(np.clip(quality, 0.0, 1.5))
    rarity_factor = float(np.clip(rarity, 0.0, 3.0))
    silence_mul = 1.0 + cfg.jackpot_silence_scale * np.log1p(float(max(inactivity_steps, 0)))
    return float(float(jackpot_base_value) * rarity_factor * length_factor * quality_factor * silence_mul)


def realized_return(
    jackpot: float,
    total_attempt_cost: float,
    discovery: float,
    base: float,
) -> float:
    return float(jackpot - total_attempt_cost - discovery - base)
-e 

===== metrics.py =====
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, List
import math


@dataclass
class MetricsTracker:
    steps: int = 0
    cumulative_distance: int = 0
    cumulative_predicted_advance: int = 0
    cumulative_raw_predicted_advance: int = 0
    cumulative_contextual_predicted_advance: int = 0
    cumulative_proposed_tokens: int = 0
    cumulative_discovery_advance: int = 0
    cumulative_energy_spent: float = 0.0
    cumulative_net_return: float = 0.0
    cumulative_payout: float = 0.0
    cumulative_births: int = 0
    cumulative_deaths: int = 0
    cumulative_nontrivial_matches: int = 0
    cumulative_full_matches: int = 0
    max_symbio_depth: int = 0
    last_gap_compression_ratio: float = 1.0
    best_gap_compression_ratio: float = float("inf")
    last_root_only_fraction: float = 1.0
    last_nonroot_live_capsules: int = 0
    last_capsule_half_life: float = 0.0
    last_lineage_persistence_1k: float = 0.0
    last_chaos_energy_spent: float = 0.0
    last_chaos_avg_substeps: float = 0.0
    last_events: List[str] = field(default_factory=list)

    def update(self, step_stats: Dict[str, float | int], events: list[str]) -> None:
        self.steps += 1
        adv = int(step_stats.get("advance_len", 0))
        match = int(step_stats.get("match_len", 0))
        p_len = int(step_stats.get("proposal_len", 0))
        rescue_winner = bool(int(step_stats.get("winner_from_frontier_rescue", 0)))
        match_for_glide = 0 if rescue_winner else match
        self.cumulative_distance += adv
        self.cumulative_raw_predicted_advance += match
        self.cumulative_predicted_advance += match_for_glide
        self.cumulative_contextual_predicted_advance += max(match_for_glide - 1, 0)
        self.cumulative_proposed_tokens += max(p_len, 0)
        self.cumulative_discovery_advance += int(step_stats.get("discovery_advance", 0))
        self.cumulative_energy_spent += float(step_stats.get("energy_spent", 0.0))
        self.cumulative_net_return += float(step_stats.get("realized_return", 0.0))
        self.cumulative_payout += float(step_stats.get("jackpot", 0.0))
        self.cumulative_births += int(step_stats.get("births", 0))
        self.cumulative_deaths += int(step_stats.get("deaths", 0))
        if match_for_glide >= 2:
            self.cumulative_nontrivial_matches += 1
        if p_len > 0 and match_for_glide == p_len:
            self.cumulative_full_matches += 1
        self.max_symbio_depth = max(
            self.max_symbio_depth, int(step_stats.get("max_symbio_depth", 0))
        )
        gcr = float(
            step_stats.get("gap_compression_ratio", self.last_gap_compression_ratio)
        )
        if not math.isfinite(gcr):
            gcr = self.last_gap_compression_ratio
        self.last_gap_compression_ratio = gcr
        self.best_gap_compression_ratio = min(self.best_gap_compression_ratio, gcr)
        self.last_root_only_fraction = float(
            step_stats.get("root_only_fraction", self.last_root_only_fraction)
        )
        self.last_nonroot_live_capsules = int(
            step_stats.get("nonroot_live_capsules", self.last_nonroot_live_capsules)
        )
        self.last_capsule_half_life = float(
            step_stats.get("capsule_half_life", self.last_capsule_half_life)
        )
        self.last_lineage_persistence_1k = float(
            step_stats.get("lineage_persistence_1k", self.last_lineage_persistence_1k)
        )
        self.last_chaos_energy_spent = float(
            step_stats.get("chaos_energy_spent", self.last_chaos_energy_spent)
        )
        self.last_chaos_avg_substeps = float(
            step_stats.get("chaos_avg_substeps", self.last_chaos_avg_substeps)
        )
        if events:
            self.last_events.extend(events)
            self.last_events = self.last_events[-16:]

    def snapshot(self) -> dict:
        distance = max(float(self.cumulative_distance), 1.0)
        proposed = max(float(self.cumulative_proposed_tokens), 1.0)
        steps = max(float(self.steps), 1.0)
        return {
            "steps": self.steps,
            "distance": self.cumulative_distance,
            "predicted_advance": self.cumulative_predicted_advance,
            "raw_predicted_advance": self.cumulative_raw_predicted_advance,
            "contextual_predicted_advance": self.cumulative_contextual_predicted_advance,
            "proposed_tokens": self.cumulative_proposed_tokens,
            "discovery_advance": self.cumulative_discovery_advance,
            "energy_spent": self.cumulative_energy_spent,
            "net_return": self.cumulative_net_return,
            "payout_total": self.cumulative_payout,
            "energy_per_advance": self.cumulative_energy_spent / distance,
            # Strict glide: only counts predictive advance beyond trivial 1-token wins.
            "glide_ratio": self.cumulative_contextual_predicted_advance / distance,
            # Legacy glide for debugging/compatibility.
            "token_glide_ratio": self.cumulative_raw_predicted_advance / distance,
            "proposal_efficiency": self.cumulative_raw_predicted_advance / proposed,
            "contextual_efficiency": self.cumulative_contextual_predicted_advance
            / proposed,
            "avg_proposal_len": self.cumulative_proposed_tokens / steps,
            "nontrivial_match_rate": self.cumulative_nontrivial_matches / steps,
            "full_match_rate": self.cumulative_full_matches / steps,
            "discovery_fraction": self.cumulative_discovery_advance / distance,
            "births_total": self.cumulative_births,
            "deaths_total": self.cumulative_deaths,
            "max_symbio_depth": int(self.max_symbio_depth),
            "gap_compression_ratio": float(self.last_gap_compression_ratio),
            "best_gap_compression_ratio": float(
                self.best_gap_compression_ratio
                if math.isfinite(self.best_gap_compression_ratio)
                else self.last_gap_compression_ratio
            ),
            "root_only_fraction": float(self.last_root_only_fraction),
            "nonroot_live_capsules": int(self.last_nonroot_live_capsules),
            "capsule_half_life": float(self.last_capsule_half_life),
            "lineage_persistence_1k": float(self.last_lineage_persistence_1k),
            "chaos_energy_spent": float(self.last_chaos_energy_spent),
            "chaos_avg_substeps": float(self.last_chaos_avg_substeps),
            "last_events": list(self.last_events[-8:]),
        }
-e 

===== run.py =====
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import time
from dataclasses import asdict, replace
from pathlib import Path
from typing import Optional

from config import EmeraConfig
from engine import EmeraEngine


def build_config_from_args(args: argparse.Namespace) -> EmeraConfig:
    cfg = EmeraConfig()
    updates = {}
    for key in [
        "seed",
        "token_space",
        "gpt2_model_name",
        "base_tokens",
        "corpus_file",
        "world_len",
        "world_vocab_size",
        "zipf_alpha",
        "d_latent",
        "gap_dim",
        "gap_len",
        "gap_read_backend",
        "gap_read_batch_size",
        "num_ifs",
        "chaos_substeps_per_round",
        "chaos_min_substeps",
        "chaos_max_substeps",
        "chaos_substep_cost",
        "k_rounds",
        "proposal_lmax",
        "obligatory_proposals",
        "proposal_min_bet",
        "proposal_bet_unit_scale",
        "proposal_bet_max_energy_frac",
        "proposal_bet_floor_frac",
        "proposal_bet_conf_gain",
        "proposal_frontier_contrast",
        "proposal_frontier_fallback",
        "frontier_rescue_energy",
        "frontier_rescue_noise",
        "frontier_rescue_max_per_step",
        "contrastive_enabled",
        "contrastive_correct_reward",
        "contrastive_wrong_penalty",
        "contrastive_wrong_exp",
        "initial_super_tokens",
        "initial_super_energy",
        "ambient_dissipation",
        "metabolic_tax_rate",
        "base_toll",
        "attempt_cost_base",
        "discovery_cost",
        "min_viable_energy",
        "survivor_grace_steps",
        "survivor_relief_active_frac",
        "survivor_relief_reservoir_frac",
        "conserve_total_energy",
        "strict_energy_budget",
        "energy_inflow_per_step",
        "energy_reservoir_init",
        "energy_reservoir_cap",
        "spawn_cost",
        "mint_parent_contrib_frac",
        "capsule_frontier_window",
        "capsule_mint_parent_pool",
        "self_copy_enabled",
        "self_copy_interval",
        "self_copy_cost",
        "self_copy_parent_contrib_frac",
        "self_copy_min_energy",
        "self_copy_min_match_frac",
        "self_copy_min_score",
        "self_copy_max_per_step",
        "mint_interval",
        "mint_delta",
        "ema_alpha",
        "dynamic_laws",
        "law_update_interval",
        "adaptation_rate",
        "adaptation_signal_decay",
        "season_period",
        "season_strength",
        "season_wave_decay",
        "season_revival_spores",
        "season_revival_energy",
        "season_topology_jitter",
        "log_every",
    ]:
        val = getattr(args, key, None)
        if val is not None:
            updates[key] = val
    if updates.get("token_space") == "gpt2":
        if "base_tokens" not in updates:
            updates["base_tokens"] = 50257
        if "world_vocab_size" not in updates:
            updates["world_vocab_size"] = int(updates["base_tokens"])
    cfg = replace(cfg, **updates)
    cfg.validate()
    return cfg


def format_step_line(stats: dict, snapshot: dict) -> str:
    return (
        f"step={stats['step']} active={stats['active_super']} proposers={stats['proposers']} "
        f"rescue={stats.get('frontier_rescue_spawned', 0)} "
        f"rescue_win={stats.get('winner_from_frontier_rescue', 0)} "
        f"win={stats['winner_id']} conf={stats['winner_conf']:.3f} len={stats['proposal_len']} "
        f"super_token_len={stats.get('winner_super_token_len', 0)} "
        f"match={stats['match_len']} adv={stats['advance_len']} "
        f"R={stats['realized_return']:+.3f} payout={stats['jackpot']:.3f} "
        f"costs(a/d/b)={stats.get('attempt_total_actual', stats['attempt_total']):.3f}/"
        f"{stats['discovery_cost']:.3f}/{stats['base_toll']:.3f} "
        f"contrast(b/p)={stats.get('contrastive_bonus', 0.0):.3f}/{stats.get('contrastive_penalty', 0.0):.3f} "
        f"avg_bet={stats.get('attempt_avg', 0.0):.4f} "
        f"births={stats['births']} copy={stats.get('self_copy_births', 0)} deaths={stats['deaths']} "
        f"laws(at={stats['law_attempt_cost_base']:.3f},jk={stats['law_jackpot_base']:.2f},"
        f"sp={stats['law_spawn_cost']:.3f},pa={stats['law_pareto_alpha']:.2f}) "
        f"reservoir={stats.get('reservoir', 0.0):.2f} drift={stats.get('energy_drift', 0.0):+.4f} "
        f"EPA={snapshot['energy_per_advance']:.4f} glide={snapshot['glide_ratio']:.3f} "
        f"tok_glide={snapshot.get('token_glide_ratio', snapshot['glide_ratio']):.3f} "
        f"gap_cr={snapshot.get('gap_compression_ratio', 1.0):.3f} "
        f"depth={int(snapshot.get('max_symbio_depth', 0))} "
        f"root_frac={snapshot.get('root_only_fraction', 1.0):.3f} "
        f"caps_nonroot={int(snapshot.get('nonroot_live_capsules', 0))} "
        f"caps_t1k={snapshot.get('lineage_persistence_1k', 0.0):.3f} "
        f"chaos_sub={snapshot.get('chaos_avg_substeps', 0.0):.1f} "
        f"payout_tot={snapshot.get('payout_total', 0.0):.3f}"
    )


def _safe_text_from_identity(identity_values, max_chars: int, token_space: str) -> str:
    if token_space == "byte_parity":
        raw = bytes(int(b) & 0xFF for b in identity_values)
        text = raw.decode("utf-8", errors="backslashreplace")
    else:
        text = " ".join(str(int(x)) for x in identity_values)
    if max_chars > 0 and len(text) > max_chars:
        text = text[:max_chars] + "..."
    return text.encode("unicode_escape").decode("ascii").replace("'", "\\'")


def _lineage_label(parent_a: int, parent_b: int) -> str:
    if parent_a < 0 and parent_b < 0:
        return "root"
    return f"{parent_a},{parent_b}"


def _token_age(engine: EmeraEngine, token) -> int:
    return max(int(engine.step_idx) - int(getattr(token, "birth_step", 0)), 0)


def _format_feature_line(
    engine: EmeraEngine, label: str, token, metric: str, max_chars: int
) -> str:
    age = _token_age(engine, token)
    rs = float(engine.last_resonance_strength.get(token.token_id, 0.0))
    _, conf, _ = engine._raw_decode_for_token(token, rs)
    text = _safe_text_from_identity(
        token.identity_bytes,
        max_chars=max_chars,
        token_space=engine.cfg.token_space,
    )
    lineage = _lineage_label(int(token.parent_a), int(token.parent_b))
    st_len = int(len(token.identity_bytes))
    metric_part = f"{metric} " if metric else ""
    return (
        f"  feat {label} id={int(token.token_id)} {metric_part}age={age} e={float(token.energy):.3f} "
        f"conf={float(conf):.3f} super_token_len={st_len} txt='{text}' lineage={lineage}"
    )


def format_feature_lines(
    engine: EmeraEngine, stats: dict, k: int, max_chars: int
) -> list[str]:
    if k <= 0 or not engine.super_tokens:
        return []
    tokens = list(engine.super_tokens.values())
    out: list[str] = []

    paid = [t for t in tokens if float(getattr(t, "max_paid_bet", 0.0)) > 0.0]
    if paid:
        best_paid = max(
            paid,
            key=lambda t: (float(t.max_paid_bet), float(t.energy), int(t.token_id)),
        )
        out.append(
            _format_feature_line(
                engine,
                "biggest_paid_bet",
                best_paid,
                f"paid_bet={float(best_paid.max_paid_bet):.4f}",
                max_chars,
            )
        )
    else:
        out.append("  feat biggest_paid_bet none")

    silent_id = int(stats.get("longest_silent_turn_id", -1))
    if silent_id >= 0:
        silent_steps = int(stats.get("longest_silent_turn_steps", 0))
        silent_match = int(stats.get("longest_silent_turn_match_len", 0))
        silent_prop_len = int(stats.get("longest_silent_turn_proposal_len", 0))
        silent_conf = float(stats.get("longest_silent_turn_conf", 0.0))
        silent_score = float(stats.get("longest_silent_turn_score", 0.0))
        silent_won = int(stats.get("longest_silent_turn_won", 0))
        silent_len = int(stats.get("longest_silent_turn_token_len", 0))
        silent_parent_a = int(stats.get("longest_silent_turn_parent_a", -1))
        silent_parent_b = int(stats.get("longest_silent_turn_parent_b", -1))
        silent_identity = list(stats.get("longest_silent_turn_identity_bytes", []))
        silent_text = _safe_text_from_identity(
            silent_identity,
            max_chars=max_chars,
            token_space=engine.cfg.token_space,
        )
        silent_lineage = _lineage_label(silent_parent_a, silent_parent_b)
        out.append(
            f"  feat longest_silent_activated_turn id={silent_id} silent_steps={silent_steps} "
            f"match={silent_match}/{silent_prop_len} conf={silent_conf:.3f} score={silent_score:+.3f} "
            f"won={silent_won} super_token_len={silent_len} txt='{silent_text}' lineage={silent_lineage}"
        )
    else:
        out.append("  feat longest_silent_activated_turn none")

    oldest = max(
        tokens, key=lambda t: (_token_age(engine, t), float(t.energy), -int(t.token_id))
    )
    out.append(
        _format_feature_line(
            engine,
            "oldest",
            oldest,
            "",
            max_chars,
        )
    )
    return out[:k]


class CorpusLineLocator:
    def __init__(self, path: Path):
        self.path = path
        self.data = path.read_bytes()

    def line_at_token_index(self, token_idx: int) -> str:
        if not self.data:
            return ""
        i = int(token_idx) % len(self.data)
        start = self.data.rfind(b"\n", 0, i) + 1
        end = self.data.find(b"\n", i)
        if end < 0:
            end = len(self.data)
        raw = self.data[start:end].rstrip(b"\r")
        return raw.decode("utf-8", errors="replace")


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Emera v2 prototype runner")
    p.add_argument("--steps", type=int, default=5_000)
    p.add_argument("--seed", type=int, default=None)
    p.add_argument(
        "--token-space", type=str, choices=["byte_parity", "gpt2"], default=None
    )
    p.add_argument("--gpt2-model-name", type=str, default=None)
    p.add_argument("--base-tokens", type=int, default=None)
    p.add_argument("--log-every", type=int, default=None)
    p.add_argument("--corpus-file", type=str, default=None)
    p.add_argument("--world-len", type=int, default=None)
    p.add_argument("--world-vocab-size", type=int, default=None)
    p.add_argument("--zipf-alpha", type=float, default=None)
    p.add_argument("--d-latent", type=int, default=None)
    p.add_argument("--gap-dim", type=int, default=None)
    p.add_argument("--gap-len", type=int, default=None)
    p.add_argument(
        "--gap-read-backend", type=str, choices=["auto", "numpy", "jax"], default=None
    )
    p.add_argument("--gap-read-batch-size", type=int, default=None)
    p.add_argument("--num-ifs", type=int, default=None)
    p.add_argument("--chaos-substeps-per-round", type=int, default=None)
    p.add_argument("--chaos-min-substeps", type=int, default=None)
    p.add_argument("--chaos-max-substeps", type=int, default=None)
    p.add_argument("--chaos-substep-cost", type=float, default=None)
    p.add_argument("--k-rounds", type=int, default=None)
    p.add_argument("--proposal-lmax", type=int, default=None)
    p.add_argument(
        "--obligatory-proposals",
        type=lambda s: s.lower() in {"1", "true", "yes", "on"},
        default=None,
    )
    p.add_argument("--proposal-min-bet", type=float, default=None)
    p.add_argument("--proposal-bet-unit-scale", type=float, default=None)
    p.add_argument("--proposal-bet-max-energy-frac", type=float, default=None)
    p.add_argument("--proposal-bet-floor-frac", type=float, default=None)
    p.add_argument("--proposal-bet-conf-gain", type=float, default=None)
    p.add_argument("--proposal-frontier-contrast", type=int, default=None)
    p.add_argument("--proposal-frontier-fallback", type=int, default=None)
    p.add_argument("--frontier-rescue-energy", type=float, default=None)
    p.add_argument("--frontier-rescue-noise", type=float, default=None)
    p.add_argument("--frontier-rescue-max-per-step", type=int, default=None)
    p.add_argument(
        "--contrastive-enabled",
        type=lambda s: s.lower() in {"1", "true", "yes", "on"},
        default=None,
    )
    p.add_argument("--contrastive-correct-reward", type=float, default=None)
    p.add_argument("--contrastive-wrong-penalty", type=float, default=None)
    p.add_argument("--contrastive-wrong-exp", type=float, default=None)
    p.add_argument("--initial-super-tokens", type=int, default=None)
    p.add_argument("--initial-super-energy", type=float, default=None)
    p.add_argument("--ambient-dissipation", type=float, default=None)
    p.add_argument("--metabolic-tax-rate", type=float, default=None)
    p.add_argument("--base-toll", type=float, default=None)
    p.add_argument("--attempt-cost-base", type=float, default=None)
    p.add_argument("--discovery-cost", type=float, default=None)
    p.add_argument("--min-viable-energy", type=float, default=None)
    p.add_argument("--survivor-grace-steps", type=int, default=None)
    p.add_argument("--survivor-relief-active-frac", type=float, default=None)
    p.add_argument("--survivor-relief-reservoir-frac", type=float, default=None)
    p.add_argument(
        "--conserve-total-energy",
        type=lambda s: s.lower() in {"1", "true", "yes", "on"},
        default=None,
    )
    p.add_argument(
        "--strict-energy-budget",
        type=lambda s: s.lower() in {"1", "true", "yes", "on"},
        default=None,
    )
    p.add_argument("--energy-inflow-per-step", type=float, default=None)
    p.add_argument("--energy-reservoir-init", type=float, default=None)
    p.add_argument("--energy-reservoir-cap", type=float, default=None)
    p.add_argument("--spawn-cost", type=float, default=None)
    p.add_argument("--mint-parent-contrib-frac", type=float, default=None)
    p.add_argument("--capsule-frontier-window", type=int, default=None)
    p.add_argument("--capsule-mint-parent-pool", type=int, default=None)
    p.add_argument(
        "--self-copy-enabled",
        type=lambda s: s.lower() in {"1", "true", "yes", "on"},
        default=None,
    )
    p.add_argument("--self-copy-interval", type=int, default=None)
    p.add_argument("--self-copy-cost", type=float, default=None)
    p.add_argument("--self-copy-parent-contrib-frac", type=float, default=None)
    p.add_argument("--self-copy-min-energy", type=float, default=None)
    p.add_argument("--self-copy-min-match-frac", type=float, default=None)
    p.add_argument("--self-copy-min-score", type=float, default=None)
    p.add_argument("--self-copy-max-per-step", type=int, default=None)
    p.add_argument("--mint-interval", type=int, default=None)
    p.add_argument("--mint-delta", type=float, default=None)
    p.add_argument("--ema-alpha", type=float, default=None)
    p.add_argument(
        "--dynamic-laws",
        type=lambda s: s.lower() in {"1", "true", "yes", "on"},
        default=None,
    )
    p.add_argument("--law-update-interval", type=int, default=None)
    p.add_argument("--adaptation-rate", type=float, default=None)
    p.add_argument("--adaptation-signal-decay", type=float, default=None)
    p.add_argument("--season-period", type=int, default=None)
    p.add_argument("--season-strength", type=float, default=None)
    p.add_argument("--season-wave-decay", type=float, default=None)
    p.add_argument("--season-revival-spores", type=int, default=None)
    p.add_argument("--season-revival-energy", type=float, default=None)
    p.add_argument("--season-topology-jitter", type=float, default=None)
    p.add_argument("--log-oldest-k", type=int, default=3)
    p.add_argument("--log-oldest-max-chars", type=int, default=18)
    p.add_argument("--output-json", type=str, default=None)
    return p.parse_args()


def main() -> None:
    args = parse_args()
    if args.steps < 1:
        raise ValueError("--steps must be >= 1")
    if args.log_every is not None and args.log_every < 1:
        raise ValueError("--log-every must be >= 1")
    if args.log_oldest_k is not None and args.log_oldest_k < 0:
        raise ValueError("--log-oldest-k must be >= 0")
    if args.log_oldest_max_chars is not None and args.log_oldest_max_chars < 0:
        raise ValueError("--log-oldest-max-chars must be >= 0")

    cfg = build_config_from_args(args)
    engine = EmeraEngine(cfg)
    locator: Optional[CorpusLineLocator] = None
    if cfg.corpus_file:
        corpus_path = Path(cfg.corpus_file)
        if corpus_path.exists():
            try:
                locator = CorpusLineLocator(corpus_path)
            except OSError:
                locator = None

    t0 = time.time()
    print(
        f"Emera v2 start | seed={cfg.seed} | world_len={engine.world.length} | "
        f"initial_super={len(engine.super_tokens)} | k_rounds={cfg.k_rounds} | "
        f"gap_backend={engine.gap.read_backend} | gap_batch={cfg.gap_read_batch_size}"
    )

    log_every = cfg.log_every
    for _ in range(args.steps):
        result = engine.step()
        if (result.stats["step"] % log_every == 0) or (result.stats["step"] == 1):
            snap = engine.metrics.snapshot()
            print(format_step_line(result.stats, snap))
            if locator is not None:
                line = locator.line_at_token_index(
                    int(result.stats.get("cursor", engine.world.cursor))
                )
                print(f"  text: {line}")
            for line in format_feature_lines(
                engine=engine,
                stats=result.stats,
                k=int(args.log_oldest_k),
                max_chars=int(args.log_oldest_max_chars),
            ):
                print(line)

    snap = engine.metrics.snapshot()
    elapsed = time.time() - t0
    final = {
        "config": asdict(cfg),
        "elapsed_sec": elapsed,
        "active_super": len(engine.super_tokens),
        "cursor": engine.world.cursor,
        "next_token_id": engine.next_token_id,
        "metrics": snap,
    }
    print(
        f"done steps={args.steps} elapsed={elapsed:.2f}s active={final['active_super']} "
        f"EPA={snap['energy_per_advance']:.4f} glide={snap['glide_ratio']:.3f} "
        f"tok_glide={snap.get('token_glide_ratio', snap['glide_ratio']):.3f} "
        f"gap_cr={snap.get('gap_compression_ratio', 1.0):.3f} "
        f"depth={int(snap.get('max_symbio_depth', 0))} "
        f"root_frac={snap.get('root_only_fraction', 1.0):.3f} "
        f"births={snap['births_total']} deaths={snap['deaths_total']}"
    )

    if args.output_json:
        out = Path(args.output_json)
        out.parent.mkdir(parents=True, exist_ok=True)
        out.write_text(json.dumps(final, indent=2, ensure_ascii=True), encoding="utf-8")
        print(f"wrote {out}")


if __name__ == "__main__":
    main()
-e 

===== specs.md =====
# Emera v2 Specification

Version: 0.1  
Date: 2026-02-21  
Status: Implementation spec for first runnable prototype

## 1. Objective

Build a non-loss-based, evolutionary language system that traverses a fixed token stream ("book-world") while minimizing:

`energy_spent / distance_advanced`

Base UTF-8 tokens are the immutable ground truth. Super-tokens emerge by symbiogenesis when cooperation improves traversal economics.

## 2. Design Invariants

1. Strict emergence: no hard-coded roles (no orchestrator/specialist classes).
2. No centralized loss, no attention, no shared token weights.
3. Survival is thermodynamic: useful entities stay alive; useless ones die by energy depletion.
4. Parents remain after minting; children are additive.
5. Keep moving parts minimal for solo-dev iteration speed.

## 3. Practical Constraints

1. Single developer, limited time.
2. Single RTX 4090.
3. Must run fast enough for overnight experiments.
4. Prefer deterministic, replayable runs (fixed seeds).

## 4. Vocabulary and World

### 4.1 Base tokens

Use the same encoding as `emera_fractal.py`:

`token_id = byte + 256 * parity`

- `byte in [0..255]`
- `parity in {0,1}`
- Total base tokens: 512

### 4.2 Book-world

1. The world is a fixed token sequence.
2. Global cursor points to current frontier.
3. Ground truth next tokens are directly available only through discovery (expensive).

## 5. Initialization: Distinct Token Identity

Goal: make all 512 base tokens maximally distinguishable in latent/gap space at step 0.

### 5.1 Immutable identity code

1. Build `H = WalshHadamard(512)`.
2. Assign each token:
   - `id_code[token_id] = H[token_id] / sqrt(512)`  
   (exact orthogonality at init).

### 5.2 Latent projection

1. Choose `d_latent` (default 32; optional 64).
2. Sample fixed random orthonormal `P: (512, d_latent)`.
3. Set:
   - `z[token_id] = normalize(id_code[token_id] @ P)`.

### 5.3 Deterministic per-token geometric seed

From `z[token_id]`, initialize:

1. `attractor0 = z[:2]`
2. `phase0 = atan2(z[1], z[0])`
3. `omega0 = omega_base + epsilon * z[2]`
4. `ifs0` generated deterministically from chunks of `z`

### 5.4 Identity beacon

Every emission includes a small fixed signature:

`emit = semantic_emit + beacon_strength * z[:gap_dim]`

Default `beacon_strength = 0.1`.

This preserves recognizability without role logic.

### 5.5 Energy initialization

All base tokens start with equal base energy:

`E_base_init = constant` (optionally tiny deterministic jitter only for tie-breaks).

## 6. Entities and State

### 6.1 BaseToken (immutable)

1. `token_id`
2. `id_code` / `z`
3. `ifs_base`
4. Optional frequency stats (for rarity estimation)

### 6.2 SuperToken (evolvable, mortal)

1. `token_id` (>= 512)
2. `parent_a`, `parent_b` (or `-1` for none)
3. `energy`
4. `alive` (derived from energy > 0)
5. `inactivity_steps`
6. `attractor`, `phase`, `omega`
7. Genome traits:
   - `ifs`
   - `activation_threshold`
   - `emission_amplitude`
   - `emission_decay`
   - `silence_growth_rate`
   - `resonance_width`
   - `phase_coupling`
   - `velocity_coupling`
   - `proposal_length_bias`

All new children start from same template distribution + inherited blend + small mutation.

### 6.3 Gap field (circular buffer)

Per slot:

1. `point` (vector)
2. `velocity`
3. `phase`
4. `omega`
5. `energy`
6. `emitter_id`
7. `step_idx`, `round_idx`

## 7. Main Dynamics Per Global Step

Let current cursor be `t`.

1. Run `K` synchronized micro-rounds (default `K=6`).
2. In each round, each currently alive super-token:
   - reads gap field,
   - computes resonance,
   - performs one chaos update,
   - optionally emits into gap.
3. Gather prediction proposals from active super-tokens.
4. Select winner by expected net return.
5. Compare proposal to world tokens at cursor.
6. Apply energy ledger updates.
7. Advance cursor by matched/discovered length.
8. Update cooperation statistics from realized return.
9. Kill entities with `energy <= 0`.
10. Run minting check (no caps/cooldowns/age gates).

## 8. Prediction/Discovery Semantics

### 8.1 Proposal

Each active super-token proposes:

1. segment tokens `[y_1..y_L]` (`L <= L_max`)
2. confidence `q`
3. internal match-quality estimate

### 8.2 Evaluation

1. Compute longest prefix match with ground truth at cursor.
2. `match_len = m`
3. `quality` from confidence calibration and geometric coherence.

### 8.3 Cursor movement

1. If `m > 0`, cursor advances by `m`.
2. If mismatch, pay discovery and reveal unmatched ground-truth tokens.

## 9. Energy Ledger

At each step:

1. Base toll: charged for advancing time/travel.
2. Attempt cost: charged to active proposers.
3. Discovery cost: charged per unmatched revealed base token.
4. Silence credit: given to inactive super-tokens as function of inactivity streak.
5. Jackpot reward: applied on successful match.

### 9.1 Silence credit

Use simple accelerating curve:

`silence_credit_i = a_i * log(1 + inactivity_steps_i) + b_i * (exp(c_i * inactivity_steps_i) - 1)`

Parameters are evolvable via `silence_growth_rate`.

### 9.2 Jackpot

`jackpot = rarity_factor * length_factor * quality_factor * silence_multiplier`

where:

1. `rarity_factor` from online token/segment frequency (inverse log-frequency or online IDF),
2. `length_factor` grows with `match_len`,
3. `quality_factor` from confidence/match quality,
4. `silence_multiplier` tied to inactivity streak.

### 9.3 Realized step return

Define:

`R = jackpot - total_attempt_cost - discovery_cost - base_toll`

This is the canonical cooperation signal.

## 10. Cooperation Detection and Minting

Core principle: mint when pair cooperation yields more return together than alone.

### 10.1 Contribution attribution from gap provenance

For each event with return `R`:

1. Compute per-token influence contribution `c_i` from gap slots used in winning proposal (using `emitter_id` and resonance weighting).
2. Normalize contributions: `sum_i c_i = 1`.

### 10.2 Minimal running statistics

Maintain EMAs:

1. `ind_ema[i]`
2. `pair_ema[i,j]`
3. `pair_hits[i,j]`

Updates:

1. `ind_ema[i] <- (1-a) * ind_ema[i] + a * (c_i * R)`
2. `pair_ema[i,j] <- (1-a) * pair_ema[i,j] + a * (min(c_i, c_j) * R)`
3. `synergy[i,j] = pair_ema[i,j] - 0.5 * (ind_ema[i] + ind_ema[j])`

### 10.3 Mint criterion with ablation test

For top positive-synergy pairs, run one deterministic ablation:

1. `R_ab`: both active
2. `R_a`: mute `b`
3. `R_b`: mute `a`

Mint if:

1. `R_ab > max(R_a, R_b) + delta`
2. both parents can pay spawn energy

No extra guardrails:

1. no child cap
2. no cooldown
3. no age threshold

Children that are not useful will die.

### 10.4 Mint mechanics

1. Parents pay spawn cost from energy.
2. Child gets spawn energy and blended genome + mutation.
3. Parents remain alive.
4. Child positive rewards are split:
   - child keeps 80%
   - parent A gets 10%
   - parent B gets 10%
5. Child negative outcomes are paid by child.
6. Death rule: if child energy <= 0, child is removed.

This gives parents resilience through successful descendants while keeping mechanism minimal.

## 11. Death and Compaction

1. Any super-token with `energy <= 0` dies immediately.
2. Remove from active arrays.
3. Gap slots with dead emitter IDs are either:
   - retained as inert decaying history, or
   - zeroed on compaction (implementation choice; start with zeroing for simplicity).

No decomposition-to-parents in v1.

## 12. Minimal Module Layout (`emera/`)

1. `specs.md` (this file)
2. `config.py`
3. `world.py` (stream + rarity stats)
4. `identity.py` (Hadamard identity + projection init)
5. `gap_field.py`
6. `genome.py`
7. `ledger.py`
8. `cooperation.py` (attribution, EMAs, ablation, mint decision)
9. `engine.py` (global loop)
10. `metrics.py`
11. `run.py`

Single-process first; optimize only after behavior exists.

## 13. Default Hyperparameters (4090-friendly)

1. `d_latent = 32`
2. `gap_dim = 16`
3. `gap_len = 128`
4. `num_ifs = 4`
5. `K = 6`
6. `L_max = 4`
7. `base_toll = 0.01`
8. `attempt_cost = 0.1` (tune)
9. `discovery_cost = 0.4` per unmatched token (tune)
10. `spawn_cost = 1.0` total from both parents (tune)
11. `ema_alpha = 0.02`
12. `mint_delta = 0.02`

Start small, then increase complexity.

## 14. Core Metrics

Primary:

1. `energy_per_advance = cumulative_energy_spent / cumulative_distance`

Secondary:

1. `glide_ratio = predicted_advance / total_advance`
2. `discovery_fraction`
3. births/deaths per 1k steps
4. live super-token count
5. activation frequency distribution (expect heavy tail)
6. reward concentration (do few children dominate?)
7. synergy matrix sparsity and persistence

## 15. Prototype Milestones

### M0: Skeleton

1. Implement world, identity, base traversal, ledger without super-tokens.
2. Confirm deterministic replay.

### M1: Signalling + proposals

1. Add super-token chaos rounds and gap field.
2. Add prediction selection and return computation.

### M2: Cooperation + minting

1. Add attribution EMAs, synergy, ablation-based mint.
2. Add child reward split and death.

### M3: Emergence validation

1. Long runs on toy Zipf corpus.
2. Verify decreasing `energy_per_advance`.
3. Verify spontaneous behavioral stratification without role labels.

## 16. Non-Goals (for now)

1. No role taxonomies.
2. No hand-crafted guardrails for child counts/cooldowns/age.
3. No complex multi-agent resource market.
4. No fancy UI before core loop works.

## 17. First Implementation Task

Implement M0+M1 in one runnable loop with logging every N steps, then add M2 minting logic exactly as specified above.
-e 

===== world.py =====
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Iterable

import numpy as np

from config import EmeraConfig


def encode_utf8_parity_tokens(text: str) -> list[int]:
    out: list[int] = []
    parity = 0
    for b in text.encode("utf-8", errors="ignore"):
        out.append(int(b) + 256 * parity)
        parity ^= 1
    return out


_TOKENIZER_CACHE: dict[str, object] = {}


def _get_gpt2_tokenizer(model_name: str):
    tok = _TOKENIZER_CACHE.get(model_name)
    if tok is not None:
        return tok
    try:
        from transformers import AutoTokenizer
    except Exception as exc:  # pragma: no cover
        raise RuntimeError(
            "gpt2 token space requires `transformers` with tokenizer support."
        ) from exc
    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    # We use tokenizer-only mode (no model forward), so long texts are valid.
    if hasattr(tok, "model_max_length"):
        tok.model_max_length = int(10**12)
    _TOKENIZER_CACHE[model_name] = tok
    return tok


def encode_gpt2_tokens(text: str, model_name: str) -> list[int]:
    tok = _get_gpt2_tokenizer(model_name)
    ids = tok.encode(text, add_special_tokens=False)
    return [int(x) for x in ids]


def decode_gpt2_tokens(token_ids: Iterable[int], model_name: str) -> str:
    tok = _get_gpt2_tokenizer(model_name)
    ids = [int(x) for x in token_ids]
    if not ids:
        return ""
    return tok.decode(ids, clean_up_tokenization_spaces=False)


def _build_world_from_corpus(path: Path, target_len: int, cfg: EmeraConfig) -> np.ndarray:
    data = path.read_text(encoding="utf-8", errors="ignore")
    if cfg.token_space == "gpt2":
        tokens = encode_gpt2_tokens(data, cfg.gpt2_model_name)
    else:
        tokens = encode_utf8_parity_tokens(data)
    if not tokens:
        raise ValueError(f"Corpus produced no tokens: {path}")
    arr = np.asarray(tokens, dtype=np.int32)
    if cfg.token_space == "gpt2" and cfg.base_tokens > 0:
        arr = np.clip(arr, 0, cfg.base_tokens - 1).astype(np.int32)
    if arr.size >= target_len:
        return arr[:target_len]
    reps = int(np.ceil(float(target_len) / float(arr.size)))
    return np.tile(arr, reps)[:target_len]


def _build_zipf_world(cfg: EmeraConfig, rng: np.random.Generator) -> np.ndarray:
    # Structured Zipf world: repeated scaffold motifs with variable Zipf content.
    vocab_lim = int(min(cfg.world_vocab_size, cfg.base_tokens))
    top = np.arange(min(24, vocab_lim), dtype=np.int32)
    tail = np.arange(min(24, vocab_lim), vocab_lim, dtype=np.int32)
    if tail.size == 0:
        tail = top.copy()

    # Create a small motif bank where scaffolds recur and content changes.
    motifs: list[np.ndarray] = []
    for _ in range(64):
        f = rng.choice(top, size=4, replace=True)
        c1 = tail[(int(rng.zipf(a=cfg.zipf_alpha)) - 1) % tail.size]
        c2 = tail[(int(rng.zipf(a=cfg.zipf_alpha)) - 1) % tail.size]
        motif = np.array([f[0], f[1], c1, f[2], c2, f[3]], dtype=np.int32)
        motifs.append(motif)

    world = np.zeros((cfg.world_len,), dtype=np.int32)
    pos = 0
    while pos < cfg.world_len:
        motif = motifs[int(rng.integers(0, len(motifs)))]
        burst = int(rng.integers(2, 8))
        for _ in range(burst):
            local = motif.copy()
            # Small content variation preserves structure while keeping predictability.
            if rng.random() < 0.65:
                local[2] = tail[(int(rng.zipf(a=cfg.zipf_alpha)) - 1) % tail.size]
            if rng.random() < 0.65:
                local[4] = tail[(int(rng.zipf(a=cfg.zipf_alpha)) - 1) % tail.size]
            n = min(local.size, cfg.world_len - pos)
            world[pos : pos + n] = local[:n]
            pos += n
            if pos >= cfg.world_len:
                break
    return world


@dataclass
class World:
    tokens: np.ndarray
    cursor: int
    total_distance: int
    counts: np.ndarray

    @classmethod
    def create(cls, cfg: EmeraConfig, rng: np.random.Generator) -> "World":
        if cfg.corpus_file:
            world_tokens = _build_world_from_corpus(Path(cfg.corpus_file), cfg.world_len, cfg)
        else:
            world_tokens = _build_zipf_world(cfg, rng)

        counts = np.bincount(world_tokens, minlength=cfg.base_tokens).astype(np.float64)
        return cls(tokens=world_tokens, cursor=0, total_distance=0, counts=counts)

    @property
    def length(self) -> int:
        return int(self.tokens.size)

    def _indices(self, length: int) -> np.ndarray:
        if length <= 0:
            return np.zeros((0,), dtype=np.int64)
        idx = (self.cursor + np.arange(length, dtype=np.int64)) % self.tokens.size
        return idx

    def peek(self, length: int) -> np.ndarray:
        return self.tokens[self._indices(length)]

    def advance(self, length: int) -> None:
        length = max(0, int(length))
        self.cursor = int((self.cursor + length) % self.tokens.size)
        self.total_distance += length

    def match_prefix_len(self, proposed: Iterable[int]) -> int:
        seq = np.asarray(list(proposed), dtype=np.int32)
        if seq.size == 0:
            return 0
        gt = self.peek(seq.size)
        mismatches = np.where(gt != seq)[0]
        if mismatches.size == 0:
            return int(seq.size)
        return int(mismatches[0])

    def rarity_of_sequence(self, seq: Iterable[int]) -> float:
        arr = np.asarray(list(seq), dtype=np.int32)
        if arr.size == 0:
            return 0.0
        n = float(self.tokens.size)
        denom = np.log(n + 1.0)
        if denom <= 0.0:
            return 0.0
        freqs = self.counts[np.clip(arr, 0, self.counts.size - 1)]
        idf = np.log((n + self.counts.size) / (freqs + 1.0)) / denom
        return float(np.clip(np.mean(idf), 0.0, 2.0))
-e 

